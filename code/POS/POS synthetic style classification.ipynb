{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18262b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from transformers import PreTrainedTokenizer, PreTrainedTokenizerFast, AutoTokenizer\n",
    "from transformers import BertForMaskedLM, BertConfig, PreTrainedModel, AutoModel, AutoModelForSequenceClassification\n",
    "from transformers import DataCollatorForLanguageModeling, DataCollatorWithPadding\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import torch\n",
    "from nltk.parse.corenlp import CoreNLPParser\n",
    "from nltk.tag.hunpos import HunposTagger\n",
    "from nltk.tokenize import word_tokenize\n",
    "import stanza\n",
    "import nltk\n",
    "import numpy as np\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from numerize import numerize\n",
    "import wandb\n",
    "import os \n",
    "import typing\n",
    "import tokenizers\n",
    "from tqdm.auto import trange, tqdm\n",
    "from itertools import cycle\n",
    "import evaluate\n",
    "import datasets\n",
    "from datasets import Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a9d0c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.18.0'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d336e2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d223966c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c21a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = BertConfig.from_json_file('/scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/config.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d777f56d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Didn't find file /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/added_tokens.json. We won't load it.\n",
      "loading file /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/tokenizer.json\n",
      "loading file None\n",
      "loading file /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/special_tokens_map.json\n",
      "loading file /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('/scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/', local_files_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd228be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"/scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 32,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 128,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 128,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 54\n",
      "}\n",
      "\n",
      "loading weights file /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/ were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/ and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained('/scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/', local_files_only=True, num_labels=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a178351",
   "metadata": {},
   "source": [
    "# visualize POS embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b11d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2token = {v:k for k,v in tokenizer.vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517130d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "embs = model.embeddings.word_embeddings.weight.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cb6268",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(2)\n",
    "pca.fit(embs)\n",
    "transformed_embs = pca.transform(embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a86adbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(embs.shape[0]):\n",
    "    plt.scatter(transformed_embs[i,0], transformed_embs[i,1])\n",
    "    plt.annotate(f'{id2token[i]}', (transformed_embs[i,0], transformed_embs[i,1]))\n",
    "# plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bb3e78",
   "metadata": {},
   "source": [
    "# synthetic style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cdf8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gene_sentences(num_sentences, sentence_length, tokens, dists, prob_of_dist=None):\n",
    "    dist_idx = np.arange(0,len(dists), dtype=int).tolist()\n",
    "    if prob_of_dist is None:\n",
    "        prob_of_dist = np.ones(len(dists))/len(dists)\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    for i in range(num_sentences):\n",
    "        i_dist = np.random.choice(dist_idx, p = prob_of_dist)\n",
    "        sent = np.random.choice(common_tokens, size = 15, p = dists[i_dist]).tolist()\n",
    "        sentences.append(' '.join(sent))\n",
    "        labels.append(i_dist)\n",
    "    return sentences, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d528ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_style_dists(num_styles, diff, tokens = ['NN', 'VB', 'DT', 'IN', 'JJ', 'PRP', 'RB',]):\n",
    "    num_tokens = len(tokens)\n",
    "    other_prob = (1-diff)/num_tokens\n",
    "    dominent_prob = other_prob + diff\n",
    "    dists = []\n",
    "    for i in range(num_styles):\n",
    "        dist = other_prob*np.ones(num_tokens)\n",
    "        dist[i] = dominent_prob\n",
    "        dists.append(dist.tolist())\n",
    "    return dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1332a4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_styles = 2\n",
    "# diff = 0.3\n",
    "# common_tokens = ['NN', 'VB', 'DT', 'IN', 'JJ', 'PRP', 'RB',]\n",
    "# dists = get_style_dists(num_styles, diff, tokens=common_tokens)\n",
    "\n",
    "# sentence_length = 15\n",
    "# train_sentences, train_labels = gene_sentences(2000, sentence_length, common_tokens, dists)\n",
    "# test_sentences, test_labels = gene_sentences(200, sentence_length, common_tokens, dists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f100c869",
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_model(model, freeze_bert):\n",
    "    '''\n",
    "    if freeze_bert == True, freeze all layer. \n",
    "    if freeze_bert is a positive integer, freeze the bottom {freeze_bert} attention layers\n",
    "    negative integer should also work\n",
    "    '''\n",
    "    if freeze_bert==True:\n",
    "        for param in model.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "    elif isinstance(freeze_bert, (int, np.int32, np.int64, torch.int32, torch.int64)):\n",
    "        for param in model.bert.embeddings.parameters():\n",
    "            param.requires_grad = False  \n",
    "        for layer in model.bert.encoder.layer[:freeze_bert]: \n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = False  \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699356e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds.logits, eval_preds.labels\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return accuracy_metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3059714f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer, padding = True, return_tensors = 'pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb51328",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nested_to(dic, device):\n",
    "    for k,v in dic.items():\n",
    "        dic[k] = v.to(device)\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaeddcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "# metric = datasets.load_metric('accuracy')\n",
    "\n",
    "# for x in trainer.get_eval_dataloader(test_dataset):\n",
    "#     labels = x['labels']\n",
    "#     x = nested_to(x, device)\n",
    "#     model_predictions = model(**x)\n",
    "#     metric.add_batch(predictions=model_predictions.logits.argmax(axis=-1).cpu().detach().numpy(), references=labels)\n",
    "\n",
    "# final_score = metric.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831f0b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(examples):\n",
    "    labels = examples['labels']\n",
    "    out = tokenizer(examples['text'], padding=True, truncation=True, max_length=128)\n",
    "    out.update({'labels':labels})\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff54424",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb4ad9ab3f1e42d0acccad2bf0f588c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28e7fb1eda1e4627bbeb9588335b69a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a937fd3ee51343dc9f8899f4609556b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"/scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 32,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 128,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 128,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 54\n",
      "}\n",
      "\n",
      "loading weights file /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/ and are newly initialized: ['bert.pooler.dense.weight', 'classifier.weight', 'bert.pooler.dense.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch/data_jz17d/wandb_tmp/wandb/run-20221114_111939-2jtxjj35</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/2jtxjj35\" target=\"_blank\">pos syn style 0</a></strong> to <a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2000\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 128\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 320\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='320' max='320' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [320/320 00:25, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.692400</td>\n",
       "      <td>0.691781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.691700</td>\n",
       "      <td>0.690619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.691000</td>\n",
       "      <td>0.689197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.689800</td>\n",
       "      <td>0.687594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.688300</td>\n",
       "      <td>0.685592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.686300</td>\n",
       "      <td>0.683124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.684200</td>\n",
       "      <td>0.679916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.681600</td>\n",
       "      <td>0.676336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.679100</td>\n",
       "      <td>0.672329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.675800</td>\n",
       "      <td>0.668015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.672700</td>\n",
       "      <td>0.664066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.669900</td>\n",
       "      <td>0.660234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.667700</td>\n",
       "      <td>0.656924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.664600</td>\n",
       "      <td>0.654156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.663700</td>\n",
       "      <td>0.651868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.660800</td>\n",
       "      <td>0.649857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.659300</td>\n",
       "      <td>0.648409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.658900</td>\n",
       "      <td>0.647338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.658000</td>\n",
       "      <td>0.646773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.657500</td>\n",
       "      <td>0.646563</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_0/checkpoint-16\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_0/checkpoint-16/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_0/checkpoint-16/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_0/checkpoint-16/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_0/checkpoint-16/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_0/checkpoint-32\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_0/checkpoint-32/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_0/checkpoint-32/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_0/checkpoint-32/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_0/checkpoint-32/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_0/checkpoint-48\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_0/checkpoint-48/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_0/checkpoint-48/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_0/checkpoint-48/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_0/checkpoint-48/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_0/checkpoint-16] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_0/checkpoint-64\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_0/checkpoint-64/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_0/checkpoint-64/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_0/checkpoint-64/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_0/checkpoint-64/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_0/checkpoint-32] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_0/checkpoint-80\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_0/checkpoint-80/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_0/checkpoint-80/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_0/checkpoint-80/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_0/checkpoint-80/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_0/checkpoint-48] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_0/checkpoint-96\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_0/checkpoint-96/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_0/checkpoint-96/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_0/checkpoint-96/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_0/checkpoint-96/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_0/checkpoint-64] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_0/checkpoint-112\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_0/checkpoint-112/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_0/checkpoint-112/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_0/checkpoint-112/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_0/checkpoint-112/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_0/checkpoint-80] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_0/checkpoint-128\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_0/checkpoint-128/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_0/checkpoint-128/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_0/checkpoint-128/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_0/checkpoint-128/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_0/checkpoint-96] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_0/checkpoint-144\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_0/checkpoint-144/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_0/checkpoint-144/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_0/checkpoint-144/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_0/checkpoint-144/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_0/checkpoint-112] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_0/checkpoint-160\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_0/checkpoint-160/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_0/checkpoint-160/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_0/checkpoint-160/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_0/checkpoint-160/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_0/checkpoint-128] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_0/checkpoint-176\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_0/checkpoint-176/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_0/checkpoint-176/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_0/checkpoint-176/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_0/checkpoint-176/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_0/checkpoint-144] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_0/checkpoint-192\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_0/checkpoint-192/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_0/checkpoint-192/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_0/checkpoint-192/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_0/checkpoint-192/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_0/checkpoint-160] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_0/checkpoint-208\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_0/checkpoint-208/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_0/checkpoint-208/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_0/checkpoint-208/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_0/checkpoint-208/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_0/checkpoint-176] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_0/checkpoint-224\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_0/checkpoint-224/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_0/checkpoint-224/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_0/checkpoint-224/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_0/checkpoint-224/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_0/checkpoint-192] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_0/checkpoint-240\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_0/checkpoint-240/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_0/checkpoint-240/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_0/checkpoint-240/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_0/checkpoint-240/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_0/checkpoint-208] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_0/checkpoint-256\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_0/checkpoint-256/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_0/checkpoint-256/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_0/checkpoint-256/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_0/checkpoint-256/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_0/checkpoint-224] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_0/checkpoint-272\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_0/checkpoint-272/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_0/checkpoint-272/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_0/checkpoint-272/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_0/checkpoint-272/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_0/checkpoint-240] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_0/checkpoint-288\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_0/checkpoint-288/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_0/checkpoint-288/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_0/checkpoint-288/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_0/checkpoint-288/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_0/checkpoint-256] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_0/checkpoint-304\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_0/checkpoint-304/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_0/checkpoint-304/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_0/checkpoint-304/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_0/checkpoint-304/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_0/checkpoint-272] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_0/checkpoint-320\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_0/checkpoint-320/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_0/checkpoint-320/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_0/checkpoint-320/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_0/checkpoint-320/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_0/checkpoint-288] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>███▇▇▇▆▆▅▄▄▃▃▂▂▂▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▆▅▄▃▁▄▄▃▃▃▃▄██▇▃▃▃▄▃</td></tr><tr><td>eval/samples_per_second</td><td>▂▃▄▆█▅▅▆▅▅▅▅▁▁▂▅▆▆▅▆</td></tr><tr><td>eval/steps_per_second</td><td>▂▃▄▆█▅▅▆▅▅▅▅▁▁▂▅▆▆▅▆</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇█████</td></tr><tr><td>train/learning_rate</td><td>██▇▇▇▆▆▅▅▅▄▄▄▃▃▂▂▂▁▁</td></tr><tr><td>train/loss</td><td>███▇▇▇▆▆▅▅▄▃▃▂▂▂▁▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.64656</td></tr><tr><td>eval/runtime</td><td>0.0655</td></tr><tr><td>eval/samples_per_second</td><td>3053.256</td></tr><tr><td>eval/steps_per_second</td><td>30.533</td></tr><tr><td>train/epoch</td><td>20.0</td></tr><tr><td>train/global_step</td><td>320</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.6575</td></tr><tr><td>train/total_flos</td><td>212168160000.0</td></tr><tr><td>train/train_loss</td><td>0.67467</td></tr><tr><td>train/train_runtime</td><td>25.7392</td></tr><tr><td>train/train_samples_per_second</td><td>1554.053</td></tr><tr><td>train/train_steps_per_second</td><td>12.432</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">pos syn style 0</strong>: <a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/2jtxjj35\" target=\"_blank\">https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/2jtxjj35</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/scratch/data_jz17d/wandb_tmp/wandb/run-20221114_111939-2jtxjj35/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bb50fbf8a8b4de095cc782630132371",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d19f3c91996a4ea5a8a24ffb9d43cc9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"/scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 32,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 128,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 128,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 54\n",
      "}\n",
      "\n",
      "loading weights file /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/ and are newly initialized: ['bert.pooler.dense.weight', 'classifier.weight', 'bert.pooler.dense.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch/data_jz17d/wandb_tmp/wandb/run-20221114_112018-1cpxsxjb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/1cpxsxjb\" target=\"_blank\">pos syn style 1</a></strong> to <a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2000\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 128\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 320\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='320' max='320' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [320/320 00:15, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.692900</td>\n",
       "      <td>0.692532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.692700</td>\n",
       "      <td>0.692349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.692600</td>\n",
       "      <td>0.692165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.692500</td>\n",
       "      <td>0.691996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.692600</td>\n",
       "      <td>0.691842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.692300</td>\n",
       "      <td>0.691691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.692300</td>\n",
       "      <td>0.691564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.692100</td>\n",
       "      <td>0.691435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.692100</td>\n",
       "      <td>0.691314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.692000</td>\n",
       "      <td>0.691213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.691900</td>\n",
       "      <td>0.691121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.691900</td>\n",
       "      <td>0.691035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.691800</td>\n",
       "      <td>0.690956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.691700</td>\n",
       "      <td>0.690889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.691700</td>\n",
       "      <td>0.690836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.691700</td>\n",
       "      <td>0.690784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.691800</td>\n",
       "      <td>0.690750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.691600</td>\n",
       "      <td>0.690723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.691700</td>\n",
       "      <td>0.690710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.691700</td>\n",
       "      <td>0.690705</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_1/checkpoint-16\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_1/checkpoint-16/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_1/checkpoint-16/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_1/checkpoint-16/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_1/checkpoint-16/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_1/checkpoint-32\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_1/checkpoint-32/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_1/checkpoint-32/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_1/checkpoint-32/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_1/checkpoint-32/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_1/checkpoint-48\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_1/checkpoint-48/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_1/checkpoint-48/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_1/checkpoint-48/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_1/checkpoint-48/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_1/checkpoint-16] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_1/checkpoint-64\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_1/checkpoint-64/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_1/checkpoint-64/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_1/checkpoint-64/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_1/checkpoint-64/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_1/checkpoint-32] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_1/checkpoint-80\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_1/checkpoint-80/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_1/checkpoint-80/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_1/checkpoint-80/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_1/checkpoint-80/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_1/checkpoint-48] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_1/checkpoint-96\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_1/checkpoint-96/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_1/checkpoint-96/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_1/checkpoint-96/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_1/checkpoint-96/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_1/checkpoint-64] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_1/checkpoint-112\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_1/checkpoint-112/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_1/checkpoint-112/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_1/checkpoint-112/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_1/checkpoint-112/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_1/checkpoint-80] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_1/checkpoint-128\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_1/checkpoint-128/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_1/checkpoint-128/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_1/checkpoint-128/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_1/checkpoint-128/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_1/checkpoint-96] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_1/checkpoint-144\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_1/checkpoint-144/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_1/checkpoint-144/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_1/checkpoint-144/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_1/checkpoint-144/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_1/checkpoint-112] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_1/checkpoint-160\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_1/checkpoint-160/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_1/checkpoint-160/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_1/checkpoint-160/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_1/checkpoint-160/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_1/checkpoint-128] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_1/checkpoint-176\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_1/checkpoint-176/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_1/checkpoint-176/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_1/checkpoint-176/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_1/checkpoint-176/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_1/checkpoint-144] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_1/checkpoint-192\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_1/checkpoint-192/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_1/checkpoint-192/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_1/checkpoint-192/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_1/checkpoint-192/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_1/checkpoint-160] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_1/checkpoint-208\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_1/checkpoint-208/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_1/checkpoint-208/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_1/checkpoint-208/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_1/checkpoint-208/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_1/checkpoint-176] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_1/checkpoint-224\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_1/checkpoint-224/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_1/checkpoint-224/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_1/checkpoint-224/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_1/checkpoint-224/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_1/checkpoint-192] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_1/checkpoint-240\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_1/checkpoint-240/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_1/checkpoint-240/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_1/checkpoint-240/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_1/checkpoint-240/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_1/checkpoint-208] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_1/checkpoint-256\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_1/checkpoint-256/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_1/checkpoint-256/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_1/checkpoint-256/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_1/checkpoint-256/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_1/checkpoint-224] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_1/checkpoint-272\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_1/checkpoint-272/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_1/checkpoint-272/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_1/checkpoint-272/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_1/checkpoint-272/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_1/checkpoint-240] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_1/checkpoint-288\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_1/checkpoint-288/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_1/checkpoint-288/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_1/checkpoint-288/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_1/checkpoint-288/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_1/checkpoint-256] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_1/checkpoint-304\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_1/checkpoint-304/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_1/checkpoint-304/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_1/checkpoint-304/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_1/checkpoint-304/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_1/checkpoint-272] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_1/checkpoint-320\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_1/checkpoint-320/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_1/checkpoint-320/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_1/checkpoint-320/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_1/checkpoint-320/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_1/checkpoint-288] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▇▇▆▅▅▄▄▃▃▃▂▂▂▂▁▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▂█▅▆▅▆▆█▂▆▅▃▁▃▅▇█▅▆▄</td></tr><tr><td>eval/samples_per_second</td><td>▆▁▄▃▄▃▃▁▇▃▄▆█▅▄▂▁▄▃▅</td></tr><tr><td>eval/steps_per_second</td><td>▆▁▄▃▄▃▃▁▇▃▄▆█▅▄▂▁▄▃▅</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇█████</td></tr><tr><td>train/learning_rate</td><td>██▇▇▇▆▆▅▅▅▄▄▄▃▃▂▂▂▁▁</td></tr><tr><td>train/loss</td><td>█▇▆▆▆▅▅▄▄▃▃▃▂▂▂▂▂▁▂▂</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.6907</td></tr><tr><td>eval/runtime</td><td>0.0657</td></tr><tr><td>eval/samples_per_second</td><td>3044.115</td></tr><tr><td>eval/steps_per_second</td><td>30.441</td></tr><tr><td>train/epoch</td><td>20.0</td></tr><tr><td>train/global_step</td><td>320</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.6917</td></tr><tr><td>train/total_flos</td><td>212168160000.0</td></tr><tr><td>train/train_loss</td><td>0.69209</td></tr><tr><td>train/train_runtime</td><td>15.5275</td></tr><tr><td>train/train_samples_per_second</td><td>2576.079</td></tr><tr><td>train/train_steps_per_second</td><td>20.609</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">pos syn style 1</strong>: <a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/1cpxsxjb\" target=\"_blank\">https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/1cpxsxjb</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/scratch/data_jz17d/wandb_tmp/wandb/run-20221114_112018-1cpxsxjb/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0b27b3020f6459186d71e3fc96ca055",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb2f5739ff684aab8e71a0a10c319ae3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"/scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 32,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 128,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 128,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 54\n",
      "}\n",
      "\n",
      "loading weights file /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/ and are newly initialized: ['bert.pooler.dense.weight', 'classifier.weight', 'bert.pooler.dense.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch/data_jz17d/wandb_tmp/wandb/run-20221114_112042-wggkg6qx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/wggkg6qx\" target=\"_blank\">pos syn style 2</a></strong> to <a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2000\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 128\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 320\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='320' max='320' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [320/320 00:20, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.692600</td>\n",
       "      <td>0.691696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.691900</td>\n",
       "      <td>0.690767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.691300</td>\n",
       "      <td>0.689722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.690500</td>\n",
       "      <td>0.688607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.689900</td>\n",
       "      <td>0.687405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.688600</td>\n",
       "      <td>0.686051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.687600</td>\n",
       "      <td>0.684636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.686200</td>\n",
       "      <td>0.682993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.685000</td>\n",
       "      <td>0.681276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.683900</td>\n",
       "      <td>0.679550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.682400</td>\n",
       "      <td>0.677792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.680700</td>\n",
       "      <td>0.676010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.679600</td>\n",
       "      <td>0.674352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.678000</td>\n",
       "      <td>0.672831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.677000</td>\n",
       "      <td>0.671515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.676000</td>\n",
       "      <td>0.670325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.675500</td>\n",
       "      <td>0.669446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.674100</td>\n",
       "      <td>0.668768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.674000</td>\n",
       "      <td>0.668385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.673800</td>\n",
       "      <td>0.668249</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_2/checkpoint-16\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_2/checkpoint-16/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_2/checkpoint-16/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_2/checkpoint-16/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_2/checkpoint-16/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_2/checkpoint-32\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_2/checkpoint-32/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_2/checkpoint-32/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_2/checkpoint-32/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_2/checkpoint-32/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_2/checkpoint-48\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_2/checkpoint-48/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_2/checkpoint-48/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_2/checkpoint-48/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_2/checkpoint-48/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_2/checkpoint-16] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_2/checkpoint-64\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_2/checkpoint-64/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_2/checkpoint-64/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_2/checkpoint-64/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_2/checkpoint-64/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_2/checkpoint-32] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_2/checkpoint-80\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_2/checkpoint-80/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_2/checkpoint-80/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_2/checkpoint-80/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_2/checkpoint-80/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_2/checkpoint-48] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_2/checkpoint-96\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_2/checkpoint-96/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_2/checkpoint-96/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_2/checkpoint-96/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_2/checkpoint-96/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_2/checkpoint-64] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_2/checkpoint-112\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_2/checkpoint-112/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_2/checkpoint-112/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_2/checkpoint-112/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_2/checkpoint-112/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_2/checkpoint-80] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_2/checkpoint-128\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_2/checkpoint-128/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_2/checkpoint-128/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_2/checkpoint-128/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_2/checkpoint-128/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_2/checkpoint-96] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_2/checkpoint-144\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_2/checkpoint-144/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_2/checkpoint-144/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_2/checkpoint-144/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_2/checkpoint-144/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_2/checkpoint-112] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_2/checkpoint-160\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_2/checkpoint-160/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_2/checkpoint-160/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_2/checkpoint-160/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_2/checkpoint-160/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_2/checkpoint-128] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_2/checkpoint-176\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_2/checkpoint-176/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_2/checkpoint-176/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_2/checkpoint-176/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_2/checkpoint-176/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_2/checkpoint-144] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_2/checkpoint-192\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_2/checkpoint-192/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_2/checkpoint-192/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_2/checkpoint-192/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_2/checkpoint-192/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_2/checkpoint-160] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_2/checkpoint-208\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_2/checkpoint-208/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_2/checkpoint-208/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_2/checkpoint-208/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_2/checkpoint-208/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_2/checkpoint-176] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_2/checkpoint-224\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_2/checkpoint-224/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_2/checkpoint-224/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_2/checkpoint-224/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_2/checkpoint-224/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_2/checkpoint-192] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_2/checkpoint-240\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_2/checkpoint-240/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_2/checkpoint-240/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_2/checkpoint-240/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_2/checkpoint-240/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_2/checkpoint-208] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_2/checkpoint-256\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_2/checkpoint-256/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_2/checkpoint-256/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_2/checkpoint-256/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_2/checkpoint-256/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_2/checkpoint-224] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_2/checkpoint-272\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_2/checkpoint-272/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_2/checkpoint-272/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_2/checkpoint-272/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_2/checkpoint-272/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_2/checkpoint-240] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_2/checkpoint-288\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_2/checkpoint-288/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_2/checkpoint-288/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_2/checkpoint-288/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_2/checkpoint-288/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_2/checkpoint-256] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_2/checkpoint-304\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_2/checkpoint-304/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_2/checkpoint-304/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_2/checkpoint-304/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_2/checkpoint-304/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_2/checkpoint-272] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_2/checkpoint-320\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_2/checkpoint-320/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_2/checkpoint-320/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_2/checkpoint-320/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_2/checkpoint-320/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_2/checkpoint-288] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>██▇▇▇▆▆▅▅▄▄▃▃▂▂▂▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>█▁▄▄▄▄▅▄▅▄▂▅▅▄▄▃▃▅▅▃</td></tr><tr><td>eval/samples_per_second</td><td>▁█▄▄▅▅▄▅▄▄▇▄▄▅▅▅▅▄▄▅</td></tr><tr><td>eval/steps_per_second</td><td>▁█▄▄▅▅▄▅▄▄▇▄▄▅▅▅▅▄▄▅</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇█████</td></tr><tr><td>train/learning_rate</td><td>██▇▇▇▆▆▅▅▅▄▄▄▃▃▂▂▂▁▁</td></tr><tr><td>train/loss</td><td>███▇▇▇▆▆▅▅▄▄▃▃▂▂▂▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.66825</td></tr><tr><td>eval/runtime</td><td>0.0645</td></tr><tr><td>eval/samples_per_second</td><td>3100.221</td></tr><tr><td>eval/steps_per_second</td><td>31.002</td></tr><tr><td>train/epoch</td><td>20.0</td></tr><tr><td>train/global_step</td><td>320</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.6738</td></tr><tr><td>train/total_flos</td><td>212168160000.0</td></tr><tr><td>train/train_loss</td><td>0.68294</td></tr><tr><td>train/train_runtime</td><td>20.1362</td></tr><tr><td>train/train_samples_per_second</td><td>1986.477</td></tr><tr><td>train/train_steps_per_second</td><td>15.892</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">pos syn style 2</strong>: <a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/wggkg6qx\" target=\"_blank\">https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/wggkg6qx</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/scratch/data_jz17d/wandb_tmp/wandb/run-20221114_112042-wggkg6qx/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69e7ffe9ab9c4a76985ba039058d6ec9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "949159c7190442cba698d0dc5f4821ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"/scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 32,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 128,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 128,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 54\n",
      "}\n",
      "\n",
      "loading weights file /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/ and are newly initialized: ['bert.pooler.dense.weight', 'classifier.weight', 'bert.pooler.dense.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch/data_jz17d/wandb_tmp/wandb/run-20221114_112110-1rkbb2wg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/1rkbb2wg\" target=\"_blank\">pos syn style 3</a></strong> to <a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2000\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 128\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 320\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='320' max='320' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [320/320 00:17, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.692700</td>\n",
       "      <td>0.691849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.692000</td>\n",
       "      <td>0.691079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.691600</td>\n",
       "      <td>0.690262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.691000</td>\n",
       "      <td>0.689442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.690600</td>\n",
       "      <td>0.688610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.689700</td>\n",
       "      <td>0.687710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.689100</td>\n",
       "      <td>0.686840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.688300</td>\n",
       "      <td>0.685857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.687700</td>\n",
       "      <td>0.684835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.687200</td>\n",
       "      <td>0.683876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.686300</td>\n",
       "      <td>0.682921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.685400</td>\n",
       "      <td>0.681962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.684900</td>\n",
       "      <td>0.681054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.684000</td>\n",
       "      <td>0.680233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.683400</td>\n",
       "      <td>0.679523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.683200</td>\n",
       "      <td>0.678875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.683000</td>\n",
       "      <td>0.678415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.682100</td>\n",
       "      <td>0.678051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.682300</td>\n",
       "      <td>0.677850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.681900</td>\n",
       "      <td>0.677781</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_3/checkpoint-16\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_3/checkpoint-16/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_3/checkpoint-16/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_3/checkpoint-16/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_3/checkpoint-16/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_3/checkpoint-32\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_3/checkpoint-32/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_3/checkpoint-32/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_3/checkpoint-32/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_3/checkpoint-32/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_3/checkpoint-48\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_3/checkpoint-48/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_3/checkpoint-48/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_3/checkpoint-48/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_3/checkpoint-48/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_3/checkpoint-16] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_3/checkpoint-64\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_3/checkpoint-64/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_3/checkpoint-64/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_3/checkpoint-64/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_3/checkpoint-64/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_3/checkpoint-32] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_3/checkpoint-80\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_3/checkpoint-80/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_3/checkpoint-80/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_3/checkpoint-80/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_3/checkpoint-80/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_3/checkpoint-48] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_3/checkpoint-96\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_3/checkpoint-96/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_3/checkpoint-96/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_3/checkpoint-96/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_3/checkpoint-96/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_3/checkpoint-64] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_3/checkpoint-112\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_3/checkpoint-112/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_3/checkpoint-112/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_3/checkpoint-112/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_3/checkpoint-112/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_3/checkpoint-80] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_3/checkpoint-128\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_3/checkpoint-128/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_3/checkpoint-128/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_3/checkpoint-128/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_3/checkpoint-128/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_3/checkpoint-96] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_3/checkpoint-144\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_3/checkpoint-144/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_3/checkpoint-144/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_3/checkpoint-144/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_3/checkpoint-144/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_3/checkpoint-112] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_3/checkpoint-160\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_3/checkpoint-160/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_3/checkpoint-160/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_3/checkpoint-160/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_3/checkpoint-160/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_3/checkpoint-128] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_3/checkpoint-176\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_3/checkpoint-176/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_3/checkpoint-176/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_3/checkpoint-176/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_3/checkpoint-176/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_3/checkpoint-144] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_3/checkpoint-192\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_3/checkpoint-192/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_3/checkpoint-192/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_3/checkpoint-192/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_3/checkpoint-192/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_3/checkpoint-160] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_3/checkpoint-208\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_3/checkpoint-208/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_3/checkpoint-208/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_3/checkpoint-208/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_3/checkpoint-208/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_3/checkpoint-176] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_3/checkpoint-224\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_3/checkpoint-224/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_3/checkpoint-224/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_3/checkpoint-224/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_3/checkpoint-224/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_3/checkpoint-192] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_3/checkpoint-240\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_3/checkpoint-240/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_3/checkpoint-240/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_3/checkpoint-240/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_3/checkpoint-240/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_3/checkpoint-208] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_3/checkpoint-256\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_3/checkpoint-256/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_3/checkpoint-256/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_3/checkpoint-256/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_3/checkpoint-256/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_3/checkpoint-224] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_3/checkpoint-272\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_3/checkpoint-272/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_3/checkpoint-272/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_3/checkpoint-272/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_3/checkpoint-272/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_3/checkpoint-240] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_3/checkpoint-288\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_3/checkpoint-288/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_3/checkpoint-288/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_3/checkpoint-288/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_3/checkpoint-288/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_3/checkpoint-256] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_3/checkpoint-304\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_3/checkpoint-304/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_3/checkpoint-304/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_3/checkpoint-304/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_3/checkpoint-304/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_3/checkpoint-272] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_3/checkpoint-320\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_3/checkpoint-320/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_3/checkpoint-320/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_3/checkpoint-320/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_3/checkpoint-320/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_3/checkpoint-288] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>██▇▇▆▆▆▅▅▄▄▃▃▂▂▂▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▇▆▅▆▆▆█▅▁▅▂▆▇▆▆▆▇▆▆▇</td></tr><tr><td>eval/samples_per_second</td><td>▂▃▄▃▃▃▁▄█▄▇▃▂▃▃▃▂▂▃▂</td></tr><tr><td>eval/steps_per_second</td><td>▂▃▄▃▃▃▁▄█▄▇▃▂▃▃▃▂▂▃▂</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇█████</td></tr><tr><td>train/learning_rate</td><td>██▇▇▇▆▆▅▅▅▄▄▄▃▃▂▂▂▁▁</td></tr><tr><td>train/loss</td><td>██▇▇▇▆▆▅▅▄▄▃▃▂▂▂▂▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.67778</td></tr><tr><td>eval/runtime</td><td>0.0671</td></tr><tr><td>eval/samples_per_second</td><td>2982.404</td></tr><tr><td>eval/steps_per_second</td><td>29.824</td></tr><tr><td>train/epoch</td><td>20.0</td></tr><tr><td>train/global_step</td><td>320</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.6819</td></tr><tr><td>train/total_flos</td><td>212168160000.0</td></tr><tr><td>train/train_loss</td><td>0.68683</td></tr><tr><td>train/train_runtime</td><td>17.5426</td></tr><tr><td>train/train_samples_per_second</td><td>2280.166</td></tr><tr><td>train/train_steps_per_second</td><td>18.241</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">pos syn style 3</strong>: <a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/1rkbb2wg\" target=\"_blank\">https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/1rkbb2wg</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/scratch/data_jz17d/wandb_tmp/wandb/run-20221114_112110-1rkbb2wg/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5adce1a05cde4511b2ed475a6e5dfe3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac1903726fff46618b9dc55556740862",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"/scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 32,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 128,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 128,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 54\n",
      "}\n",
      "\n",
      "loading weights file /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/ and are newly initialized: ['bert.pooler.dense.weight', 'classifier.weight', 'bert.pooler.dense.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch/data_jz17d/wandb_tmp/wandb/run-20221114_112137-2zxs7mqb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/2zxs7mqb\" target=\"_blank\">pos syn style 4</a></strong> to <a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2000\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 128\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 320\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='320' max='320' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [320/320 00:24, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.692200</td>\n",
       "      <td>0.690217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.690300</td>\n",
       "      <td>0.688100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.688300</td>\n",
       "      <td>0.685432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.685500</td>\n",
       "      <td>0.681982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.682000</td>\n",
       "      <td>0.677450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.677300</td>\n",
       "      <td>0.671771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.671600</td>\n",
       "      <td>0.664850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.665300</td>\n",
       "      <td>0.656767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.658700</td>\n",
       "      <td>0.648127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.651700</td>\n",
       "      <td>0.640059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.644200</td>\n",
       "      <td>0.632430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.638200</td>\n",
       "      <td>0.625523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.632300</td>\n",
       "      <td>0.619257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.625900</td>\n",
       "      <td>0.613778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.623300</td>\n",
       "      <td>0.609166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.617400</td>\n",
       "      <td>0.605371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.615200</td>\n",
       "      <td>0.602444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.612700</td>\n",
       "      <td>0.600339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.611800</td>\n",
       "      <td>0.599104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.610300</td>\n",
       "      <td>0.598688</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_4/checkpoint-16\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_4/checkpoint-16/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_4/checkpoint-16/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_4/checkpoint-16/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_4/checkpoint-16/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_4/checkpoint-32\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_4/checkpoint-32/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_4/checkpoint-32/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_4/checkpoint-32/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_4/checkpoint-32/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_4/checkpoint-48\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_4/checkpoint-48/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_4/checkpoint-48/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_4/checkpoint-48/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_4/checkpoint-48/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_4/checkpoint-16] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_4/checkpoint-64\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_4/checkpoint-64/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_4/checkpoint-64/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_4/checkpoint-64/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_4/checkpoint-64/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_4/checkpoint-32] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_4/checkpoint-80\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_4/checkpoint-80/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_4/checkpoint-80/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_4/checkpoint-80/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_4/checkpoint-80/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_4/checkpoint-48] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_4/checkpoint-96\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_4/checkpoint-96/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_4/checkpoint-96/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_4/checkpoint-96/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_4/checkpoint-96/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_4/checkpoint-64] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_4/checkpoint-112\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_4/checkpoint-112/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_4/checkpoint-112/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_4/checkpoint-112/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_4/checkpoint-112/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_4/checkpoint-80] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_4/checkpoint-128\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_4/checkpoint-128/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_4/checkpoint-128/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_4/checkpoint-128/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_4/checkpoint-128/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_4/checkpoint-96] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_4/checkpoint-144\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_4/checkpoint-144/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_4/checkpoint-144/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_4/checkpoint-144/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_4/checkpoint-144/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_4/checkpoint-112] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_4/checkpoint-160\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_4/checkpoint-160/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_4/checkpoint-160/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_4/checkpoint-160/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_4/checkpoint-160/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_4/checkpoint-128] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_4/checkpoint-176\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_4/checkpoint-176/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_4/checkpoint-176/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_4/checkpoint-176/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_4/checkpoint-176/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_4/checkpoint-144] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_4/checkpoint-192\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_4/checkpoint-192/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_4/checkpoint-192/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_4/checkpoint-192/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_4/checkpoint-192/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_4/checkpoint-160] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_4/checkpoint-208\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_4/checkpoint-208/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_4/checkpoint-208/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_4/checkpoint-208/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_4/checkpoint-208/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_4/checkpoint-176] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_4/checkpoint-224\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_4/checkpoint-224/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_4/checkpoint-224/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_4/checkpoint-224/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_4/checkpoint-224/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_4/checkpoint-192] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_4/checkpoint-240\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_4/checkpoint-240/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_4/checkpoint-240/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_4/checkpoint-240/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_4/checkpoint-240/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_4/checkpoint-208] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_4/checkpoint-256\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_4/checkpoint-256/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_4/checkpoint-256/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_4/checkpoint-256/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_4/checkpoint-256/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_4/checkpoint-224] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_4/checkpoint-272\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_4/checkpoint-272/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_4/checkpoint-272/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_4/checkpoint-272/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_4/checkpoint-272/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_4/checkpoint-240] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_4/checkpoint-288\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_4/checkpoint-288/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_4/checkpoint-288/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_4/checkpoint-288/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_4/checkpoint-288/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_4/checkpoint-256] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_4/checkpoint-304\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_4/checkpoint-304/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_4/checkpoint-304/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_4/checkpoint-304/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_4/checkpoint-304/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_4/checkpoint-272] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_4/checkpoint-320\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_4/checkpoint-320/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_4/checkpoint-320/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_4/checkpoint-320/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_4/checkpoint-320/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_4/checkpoint-288] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>███▇▇▇▆▅▅▄▄▃▃▂▂▂▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▁▄▄▄█▂▆▄▃▄▃▅▄▆▄▄▄▄▅▄</td></tr><tr><td>eval/samples_per_second</td><td>█▅▄▄▁▇▃▄▅▅▅▄▅▃▄▄▄▄▄▄</td></tr><tr><td>eval/steps_per_second</td><td>█▅▄▄▁▇▃▄▅▅▅▄▅▃▄▄▄▄▄▄</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇█████</td></tr><tr><td>train/learning_rate</td><td>██▇▇▇▆▆▅▅▅▄▄▄▃▃▂▂▂▁▁</td></tr><tr><td>train/loss</td><td>███▇▇▇▆▆▅▅▄▃▃▂▂▂▁▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.59869</td></tr><tr><td>eval/runtime</td><td>0.0663</td></tr><tr><td>eval/samples_per_second</td><td>3014.73</td></tr><tr><td>eval/steps_per_second</td><td>30.147</td></tr><tr><td>train/epoch</td><td>20.0</td></tr><tr><td>train/global_step</td><td>320</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.6103</td></tr><tr><td>train/total_flos</td><td>212168160000.0</td></tr><tr><td>train/train_loss</td><td>0.64972</td></tr><tr><td>train/train_runtime</td><td>24.6745</td></tr><tr><td>train/train_samples_per_second</td><td>1621.108</td></tr><tr><td>train/train_steps_per_second</td><td>12.969</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">pos syn style 4</strong>: <a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/2zxs7mqb\" target=\"_blank\">https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/2zxs7mqb</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/scratch/data_jz17d/wandb_tmp/wandb/run-20221114_112137-2zxs7mqb/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cf795234ebb4f348909a6a8690efcd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aeda6fb2262e4f26bab2ac06d1cfe26e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"/scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 32,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 128,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 128,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 54\n",
      "}\n",
      "\n",
      "loading weights file /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/ and are newly initialized: ['bert.pooler.dense.weight', 'classifier.weight', 'bert.pooler.dense.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch/data_jz17d/wandb_tmp/wandb/run-20221114_112210-23mub6s4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/23mub6s4\" target=\"_blank\">pos syn style 5</a></strong> to <a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2000\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 128\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 320\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='320' max='320' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [320/320 00:15, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.693000</td>\n",
       "      <td>0.691972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.692600</td>\n",
       "      <td>0.691695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.692500</td>\n",
       "      <td>0.691423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.692300</td>\n",
       "      <td>0.691171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.692100</td>\n",
       "      <td>0.690933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.691800</td>\n",
       "      <td>0.690708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.691700</td>\n",
       "      <td>0.690509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.691400</td>\n",
       "      <td>0.690320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.691300</td>\n",
       "      <td>0.690144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.691100</td>\n",
       "      <td>0.689987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.690900</td>\n",
       "      <td>0.689846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.690800</td>\n",
       "      <td>0.689719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.690700</td>\n",
       "      <td>0.689600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.690600</td>\n",
       "      <td>0.689499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.690500</td>\n",
       "      <td>0.689418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.690500</td>\n",
       "      <td>0.689344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.690500</td>\n",
       "      <td>0.689292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.690300</td>\n",
       "      <td>0.689253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.690500</td>\n",
       "      <td>0.689231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.690400</td>\n",
       "      <td>0.689223</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_5/checkpoint-16\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_5/checkpoint-16/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_5/checkpoint-16/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_5/checkpoint-16/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_5/checkpoint-16/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_5/checkpoint-32\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_5/checkpoint-32/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_5/checkpoint-32/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_5/checkpoint-32/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_5/checkpoint-32/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_5/checkpoint-48\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_5/checkpoint-48/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_5/checkpoint-48/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_5/checkpoint-48/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_5/checkpoint-48/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_5/checkpoint-16] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_5/checkpoint-64\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_5/checkpoint-64/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_5/checkpoint-64/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_5/checkpoint-64/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_5/checkpoint-64/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_5/checkpoint-32] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_5/checkpoint-80\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_5/checkpoint-80/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_5/checkpoint-80/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_5/checkpoint-80/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_5/checkpoint-80/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_5/checkpoint-48] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_5/checkpoint-96\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_5/checkpoint-96/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_5/checkpoint-96/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_5/checkpoint-96/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_5/checkpoint-96/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_5/checkpoint-64] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_5/checkpoint-112\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_5/checkpoint-112/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_5/checkpoint-112/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_5/checkpoint-112/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_5/checkpoint-112/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_5/checkpoint-80] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_5/checkpoint-128\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_5/checkpoint-128/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_5/checkpoint-128/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_5/checkpoint-128/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_5/checkpoint-128/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_5/checkpoint-96] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_5/checkpoint-144\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_5/checkpoint-144/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_5/checkpoint-144/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_5/checkpoint-144/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_5/checkpoint-144/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_5/checkpoint-112] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_5/checkpoint-160\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_5/checkpoint-160/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_5/checkpoint-160/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_5/checkpoint-160/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_5/checkpoint-160/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_5/checkpoint-128] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_5/checkpoint-176\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_5/checkpoint-176/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_5/checkpoint-176/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_5/checkpoint-176/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_5/checkpoint-176/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_5/checkpoint-144] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_5/checkpoint-192\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_5/checkpoint-192/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_5/checkpoint-192/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_5/checkpoint-192/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_5/checkpoint-192/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_5/checkpoint-160] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_5/checkpoint-208\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_5/checkpoint-208/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_5/checkpoint-208/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_5/checkpoint-208/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_5/checkpoint-208/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_5/checkpoint-176] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_5/checkpoint-224\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_5/checkpoint-224/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_5/checkpoint-224/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_5/checkpoint-224/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_5/checkpoint-224/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_5/checkpoint-192] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_5/checkpoint-240\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_5/checkpoint-240/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_5/checkpoint-240/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_5/checkpoint-240/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_5/checkpoint-240/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_5/checkpoint-208] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_5/checkpoint-256\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_5/checkpoint-256/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_5/checkpoint-256/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_5/checkpoint-256/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_5/checkpoint-256/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_5/checkpoint-224] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_5/checkpoint-272\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_5/checkpoint-272/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_5/checkpoint-272/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_5/checkpoint-272/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_5/checkpoint-272/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_5/checkpoint-240] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_5/checkpoint-288\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_5/checkpoint-288/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_5/checkpoint-288/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_5/checkpoint-288/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_5/checkpoint-288/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_5/checkpoint-256] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_5/checkpoint-304\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_5/checkpoint-304/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_5/checkpoint-304/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_5/checkpoint-304/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_5/checkpoint-304/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_5/checkpoint-272] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_5/checkpoint-320\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_5/checkpoint-320/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_5/checkpoint-320/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_5/checkpoint-320/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_5/checkpoint-320/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_5/checkpoint-288] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▇▇▆▅▅▄▄▃▃▃▂▂▂▁▁▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▆▆▆▇█▇▆▅▇▇█▁▆▇▃▆▇▂▆▆</td></tr><tr><td>eval/samples_per_second</td><td>▃▃▃▂▁▂▃▄▂▂▁█▂▂▆▃▂▆▃▃</td></tr><tr><td>eval/steps_per_second</td><td>▃▃▃▂▁▂▃▄▂▂▁█▂▂▆▃▂▆▃▃</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇█████</td></tr><tr><td>train/learning_rate</td><td>██▇▇▇▆▆▅▅▅▄▄▄▃▃▂▂▂▁▁</td></tr><tr><td>train/loss</td><td>█▇▇▆▆▅▅▄▄▃▃▂▂▂▂▂▂▁▂▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.68922</td></tr><tr><td>eval/runtime</td><td>0.0654</td></tr><tr><td>eval/samples_per_second</td><td>3059.749</td></tr><tr><td>eval/steps_per_second</td><td>30.597</td></tr><tr><td>train/epoch</td><td>20.0</td></tr><tr><td>train/global_step</td><td>320</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.6904</td></tr><tr><td>train/total_flos</td><td>212168160000.0</td></tr><tr><td>train/train_loss</td><td>0.69127</td></tr><tr><td>train/train_runtime</td><td>15.4727</td></tr><tr><td>train/train_samples_per_second</td><td>2585.191</td></tr><tr><td>train/train_steps_per_second</td><td>20.682</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">pos syn style 5</strong>: <a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/23mub6s4\" target=\"_blank\">https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/23mub6s4</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/scratch/data_jz17d/wandb_tmp/wandb/run-20221114_112210-23mub6s4/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1b11132508d401ea1a4f77cb4a21f2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af19279a6ee44fc59e0227eb05f6e41a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"/scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 32,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 128,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 128,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 54\n",
      "}\n",
      "\n",
      "loading weights file /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/ and are newly initialized: ['bert.pooler.dense.weight', 'classifier.weight', 'bert.pooler.dense.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch/data_jz17d/wandb_tmp/wandb/run-20221114_112235-1fx35oh1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/1fx35oh1\" target=\"_blank\">pos syn style 6</a></strong> to <a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2000\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 128\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 320\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='320' max='320' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [320/320 00:21, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.692400</td>\n",
       "      <td>0.690623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.690800</td>\n",
       "      <td>0.688988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.689500</td>\n",
       "      <td>0.687119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.687700</td>\n",
       "      <td>0.684950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.685500</td>\n",
       "      <td>0.682356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.682900</td>\n",
       "      <td>0.679358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>0.676004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.676500</td>\n",
       "      <td>0.672169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.673400</td>\n",
       "      <td>0.668085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.669600</td>\n",
       "      <td>0.663773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.665700</td>\n",
       "      <td>0.659354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.661300</td>\n",
       "      <td>0.654993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.657300</td>\n",
       "      <td>0.650809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.653100</td>\n",
       "      <td>0.646948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.650600</td>\n",
       "      <td>0.643593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.647300</td>\n",
       "      <td>0.640759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.646100</td>\n",
       "      <td>0.638509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.642600</td>\n",
       "      <td>0.636860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.643200</td>\n",
       "      <td>0.635889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.641800</td>\n",
       "      <td>0.635557</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_6/checkpoint-16\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_6/checkpoint-16/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_6/checkpoint-16/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_6/checkpoint-16/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_6/checkpoint-16/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_6/checkpoint-32\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_6/checkpoint-32/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_6/checkpoint-32/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_6/checkpoint-32/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_6/checkpoint-32/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_6/checkpoint-48\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_6/checkpoint-48/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_6/checkpoint-48/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_6/checkpoint-48/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_6/checkpoint-48/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_6/checkpoint-16] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_6/checkpoint-64\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_6/checkpoint-64/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_6/checkpoint-64/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_6/checkpoint-64/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_6/checkpoint-64/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_6/checkpoint-32] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_6/checkpoint-80\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_6/checkpoint-80/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_6/checkpoint-80/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_6/checkpoint-80/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_6/checkpoint-80/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_6/checkpoint-48] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_6/checkpoint-96\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_6/checkpoint-96/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_6/checkpoint-96/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_6/checkpoint-96/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_6/checkpoint-96/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_6/checkpoint-64] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_6/checkpoint-112\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_6/checkpoint-112/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_6/checkpoint-112/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_6/checkpoint-112/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_6/checkpoint-112/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_6/checkpoint-80] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_6/checkpoint-128\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_6/checkpoint-128/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_6/checkpoint-128/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_6/checkpoint-128/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_6/checkpoint-128/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_6/checkpoint-96] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_6/checkpoint-144\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_6/checkpoint-144/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_6/checkpoint-144/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_6/checkpoint-144/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_6/checkpoint-144/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_6/checkpoint-112] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_6/checkpoint-160\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_6/checkpoint-160/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_6/checkpoint-160/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_6/checkpoint-160/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_6/checkpoint-160/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_6/checkpoint-128] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_6/checkpoint-176\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_6/checkpoint-176/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_6/checkpoint-176/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_6/checkpoint-176/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_6/checkpoint-176/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_6/checkpoint-144] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_6/checkpoint-192\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_6/checkpoint-192/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_6/checkpoint-192/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_6/checkpoint-192/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_6/checkpoint-192/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_6/checkpoint-160] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_6/checkpoint-208\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_6/checkpoint-208/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_6/checkpoint-208/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_6/checkpoint-208/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_6/checkpoint-208/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_6/checkpoint-176] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_6/checkpoint-224\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_6/checkpoint-224/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_6/checkpoint-224/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_6/checkpoint-224/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_6/checkpoint-224/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_6/checkpoint-192] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_6/checkpoint-240\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_6/checkpoint-240/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_6/checkpoint-240/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_6/checkpoint-240/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_6/checkpoint-240/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_6/checkpoint-208] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_6/checkpoint-256\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_6/checkpoint-256/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_6/checkpoint-256/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_6/checkpoint-256/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_6/checkpoint-256/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_6/checkpoint-224] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_6/checkpoint-272\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_6/checkpoint-272/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_6/checkpoint-272/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_6/checkpoint-272/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_6/checkpoint-272/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_6/checkpoint-240] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_6/checkpoint-288\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_6/checkpoint-288/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_6/checkpoint-288/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_6/checkpoint-288/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_6/checkpoint-288/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_6/checkpoint-256] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_6/checkpoint-304\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_6/checkpoint-304/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_6/checkpoint-304/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_6/checkpoint-304/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_6/checkpoint-304/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_6/checkpoint-272] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_6/checkpoint-320\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_6/checkpoint-320/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_6/checkpoint-320/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_6/checkpoint-320/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_6/checkpoint-320/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_6/checkpoint-288] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>███▇▇▇▆▆▅▅▄▃▃▂▂▂▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▄▄▄▃▃▄▅▄▄▃▄▄▅▂▃▃▄▄█▁</td></tr><tr><td>eval/samples_per_second</td><td>▅▅▅▅▆▄▄▅▅▅▄▄▄▆▆▅▄▅▁█</td></tr><tr><td>eval/steps_per_second</td><td>▅▅▅▅▆▄▄▅▅▅▄▄▄▆▆▅▄▅▁█</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇█████</td></tr><tr><td>train/learning_rate</td><td>██▇▇▇▆▆▅▅▅▄▄▄▃▃▂▂▂▁▁</td></tr><tr><td>train/loss</td><td>███▇▇▇▆▆▅▅▄▄▃▃▂▂▂▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.63556</td></tr><tr><td>eval/runtime</td><td>0.0604</td></tr><tr><td>eval/samples_per_second</td><td>3313.665</td></tr><tr><td>eval/steps_per_second</td><td>33.137</td></tr><tr><td>train/epoch</td><td>20.0</td></tr><tr><td>train/global_step</td><td>320</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.6418</td></tr><tr><td>train/total_flos</td><td>212168160000.0</td></tr><tr><td>train/train_loss</td><td>0.66688</td></tr><tr><td>train/train_runtime</td><td>21.1759</td></tr><tr><td>train/train_samples_per_second</td><td>1888.941</td></tr><tr><td>train/train_steps_per_second</td><td>15.112</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">pos syn style 6</strong>: <a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/1fx35oh1\" target=\"_blank\">https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/1fx35oh1</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/scratch/data_jz17d/wandb_tmp/wandb/run-20221114_112235-1fx35oh1/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28e681e9813e4a1da5722027274d2333",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58da3c3d2f994903916bb3700b9c3c90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"/scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 32,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 128,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 128,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 54\n",
      "}\n",
      "\n",
      "loading weights file /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/ and are newly initialized: ['bert.pooler.dense.weight', 'classifier.weight', 'bert.pooler.dense.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch/data_jz17d/wandb_tmp/wandb/run-20221114_112303-191hgcng</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/191hgcng\" target=\"_blank\">pos syn style 7</a></strong> to <a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2000\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 128\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 320\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='320' max='320' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [320/320 00:17, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.692500</td>\n",
       "      <td>0.690881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.691200</td>\n",
       "      <td>0.689569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.690200</td>\n",
       "      <td>0.688177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.688900</td>\n",
       "      <td>0.686696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.685045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.685900</td>\n",
       "      <td>0.683231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.684300</td>\n",
       "      <td>0.681306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.682300</td>\n",
       "      <td>0.679211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.680800</td>\n",
       "      <td>0.677023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.678800</td>\n",
       "      <td>0.674799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.676700</td>\n",
       "      <td>0.672544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.674400</td>\n",
       "      <td>0.670341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.672400</td>\n",
       "      <td>0.668190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.670300</td>\n",
       "      <td>0.666237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.668600</td>\n",
       "      <td>0.664531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.667500</td>\n",
       "      <td>0.663056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.666900</td>\n",
       "      <td>0.661921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.665100</td>\n",
       "      <td>0.661084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.665400</td>\n",
       "      <td>0.660588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.664600</td>\n",
       "      <td>0.660419</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_7/checkpoint-16\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_7/checkpoint-16/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_7/checkpoint-16/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_7/checkpoint-16/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_7/checkpoint-16/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_7/checkpoint-32\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_7/checkpoint-32/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_7/checkpoint-32/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_7/checkpoint-32/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_7/checkpoint-32/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_7/checkpoint-48\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_7/checkpoint-48/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_7/checkpoint-48/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_7/checkpoint-48/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_7/checkpoint-48/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_7/checkpoint-16] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_7/checkpoint-64\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_7/checkpoint-64/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_7/checkpoint-64/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_7/checkpoint-64/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_7/checkpoint-64/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_7/checkpoint-32] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_7/checkpoint-80\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_7/checkpoint-80/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_7/checkpoint-80/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_7/checkpoint-80/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_7/checkpoint-80/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_7/checkpoint-48] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_7/checkpoint-96\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_7/checkpoint-96/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_7/checkpoint-96/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_7/checkpoint-96/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_7/checkpoint-96/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_7/checkpoint-64] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_7/checkpoint-112\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_7/checkpoint-112/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_7/checkpoint-112/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_7/checkpoint-112/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_7/checkpoint-112/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_7/checkpoint-80] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_7/checkpoint-128\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_7/checkpoint-128/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_7/checkpoint-128/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_7/checkpoint-128/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_7/checkpoint-128/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_7/checkpoint-96] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_7/checkpoint-144\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_7/checkpoint-144/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_7/checkpoint-144/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_7/checkpoint-144/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_7/checkpoint-144/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_7/checkpoint-112] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_7/checkpoint-160\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_7/checkpoint-160/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_7/checkpoint-160/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_7/checkpoint-160/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_7/checkpoint-160/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_7/checkpoint-128] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_7/checkpoint-176\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_7/checkpoint-176/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_7/checkpoint-176/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_7/checkpoint-176/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_7/checkpoint-176/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_7/checkpoint-144] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_7/checkpoint-192\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_7/checkpoint-192/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_7/checkpoint-192/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_7/checkpoint-192/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_7/checkpoint-192/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_7/checkpoint-160] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_7/checkpoint-208\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_7/checkpoint-208/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_7/checkpoint-208/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_7/checkpoint-208/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_7/checkpoint-208/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_7/checkpoint-176] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_7/checkpoint-224\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_7/checkpoint-224/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_7/checkpoint-224/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_7/checkpoint-224/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_7/checkpoint-224/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_7/checkpoint-192] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_7/checkpoint-240\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_7/checkpoint-240/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_7/checkpoint-240/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_7/checkpoint-240/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_7/checkpoint-240/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_7/checkpoint-208] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_7/checkpoint-256\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_7/checkpoint-256/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_7/checkpoint-256/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_7/checkpoint-256/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_7/checkpoint-256/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_7/checkpoint-224] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_7/checkpoint-272\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_7/checkpoint-272/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_7/checkpoint-272/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_7/checkpoint-272/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_7/checkpoint-272/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_7/checkpoint-240] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_7/checkpoint-288\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_7/checkpoint-288/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_7/checkpoint-288/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_7/checkpoint-288/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_7/checkpoint-288/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_7/checkpoint-256] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_7/checkpoint-304\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_7/checkpoint-304/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_7/checkpoint-304/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_7/checkpoint-304/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_7/checkpoint-304/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_7/checkpoint-272] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_7/checkpoint-320\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_7/checkpoint-320/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_7/checkpoint-320/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_7/checkpoint-320/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_7/checkpoint-320/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_7/checkpoint-288] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>██▇▇▇▆▆▅▅▄▄▃▃▂▂▂▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▂▄▄▁▄▅▂▅▅█▄▄▅▅▅▆▆▄▄▂</td></tr><tr><td>eval/samples_per_second</td><td>▇▄▄█▄▄▇▄▃▁▅▄▃▃▄▃▃▄▄▆</td></tr><tr><td>eval/steps_per_second</td><td>▇▄▄█▄▄▇▄▃▁▅▄▃▃▄▃▃▄▄▆</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇█████</td></tr><tr><td>train/learning_rate</td><td>██▇▇▇▆▆▅▅▅▄▄▄▃▃▂▂▂▁▁</td></tr><tr><td>train/loss</td><td>██▇▇▇▆▆▅▅▅▄▃▃▂▂▂▂▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.66042</td></tr><tr><td>eval/runtime</td><td>0.0607</td></tr><tr><td>eval/samples_per_second</td><td>3296.333</td></tr><tr><td>eval/steps_per_second</td><td>32.963</td></tr><tr><td>train/epoch</td><td>20.0</td></tr><tr><td>train/global_step</td><td>320</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.6646</td></tr><tr><td>train/total_flos</td><td>212168160000.0</td></tr><tr><td>train/train_loss</td><td>0.67772</td></tr><tr><td>train/train_runtime</td><td>17.9732</td></tr><tr><td>train/train_samples_per_second</td><td>2225.532</td></tr><tr><td>train/train_steps_per_second</td><td>17.804</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">pos syn style 7</strong>: <a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/191hgcng\" target=\"_blank\">https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/191hgcng</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/scratch/data_jz17d/wandb_tmp/wandb/run-20221114_112303-191hgcng/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "879c20da346147fc8e95d832b8f85414",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66b510608ad646f4b90a89962b585383",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"/scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 32,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 128,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 128,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 54\n",
      "}\n",
      "\n",
      "loading weights file /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/ and are newly initialized: ['bert.pooler.dense.weight', 'classifier.weight', 'bert.pooler.dense.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch/data_jz17d/wandb_tmp/wandb/run-20221114_112329-3otq7r9e</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/3otq7r9e\" target=\"_blank\">pos syn style 8</a></strong> to <a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2000\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 128\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 320\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='320' max='320' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [320/320 00:25, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.692000</td>\n",
       "      <td>0.689479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.689000</td>\n",
       "      <td>0.686237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.685700</td>\n",
       "      <td>0.682076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.681400</td>\n",
       "      <td>0.676584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.675800</td>\n",
       "      <td>0.669458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.668700</td>\n",
       "      <td>0.660724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.660700</td>\n",
       "      <td>0.650573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.651500</td>\n",
       "      <td>0.639577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.642500</td>\n",
       "      <td>0.628592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.633200</td>\n",
       "      <td>0.618174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.623700</td>\n",
       "      <td>0.608459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.615400</td>\n",
       "      <td>0.599372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.607700</td>\n",
       "      <td>0.591277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.600800</td>\n",
       "      <td>0.584300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.595100</td>\n",
       "      <td>0.578339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.589500</td>\n",
       "      <td>0.573379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.585100</td>\n",
       "      <td>0.569685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.582800</td>\n",
       "      <td>0.567040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.580600</td>\n",
       "      <td>0.565444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.579400</td>\n",
       "      <td>0.564905</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_8/checkpoint-16\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_8/checkpoint-16/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_8/checkpoint-16/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_8/checkpoint-16/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_8/checkpoint-16/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_8/checkpoint-32\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_8/checkpoint-32/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_8/checkpoint-32/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_8/checkpoint-32/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_8/checkpoint-32/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_8/checkpoint-48\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_8/checkpoint-48/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_8/checkpoint-48/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_8/checkpoint-48/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_8/checkpoint-48/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_8/checkpoint-16] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_8/checkpoint-64\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_8/checkpoint-64/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_8/checkpoint-64/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_8/checkpoint-64/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_8/checkpoint-64/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_8/checkpoint-32] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_8/checkpoint-80\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_8/checkpoint-80/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_8/checkpoint-80/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_8/checkpoint-80/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_8/checkpoint-80/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_8/checkpoint-48] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_8/checkpoint-96\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_8/checkpoint-96/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_8/checkpoint-96/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_8/checkpoint-96/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_8/checkpoint-96/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_8/checkpoint-64] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_8/checkpoint-112\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_8/checkpoint-112/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_8/checkpoint-112/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_8/checkpoint-112/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_8/checkpoint-112/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_8/checkpoint-80] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_8/checkpoint-128\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_8/checkpoint-128/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_8/checkpoint-128/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_8/checkpoint-128/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_8/checkpoint-128/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_8/checkpoint-96] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_8/checkpoint-144\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_8/checkpoint-144/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_8/checkpoint-144/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_8/checkpoint-144/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_8/checkpoint-144/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_8/checkpoint-112] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_8/checkpoint-160\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_8/checkpoint-160/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_8/checkpoint-160/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_8/checkpoint-160/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_8/checkpoint-160/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_8/checkpoint-128] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_8/checkpoint-176\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_8/checkpoint-176/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_8/checkpoint-176/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_8/checkpoint-176/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_8/checkpoint-176/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_8/checkpoint-144] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_8/checkpoint-192\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_8/checkpoint-192/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_8/checkpoint-192/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_8/checkpoint-192/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_8/checkpoint-192/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_8/checkpoint-160] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_8/checkpoint-208\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_8/checkpoint-208/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_8/checkpoint-208/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_8/checkpoint-208/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_8/checkpoint-208/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_8/checkpoint-176] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_8/checkpoint-224\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_8/checkpoint-224/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_8/checkpoint-224/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_8/checkpoint-224/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_8/checkpoint-224/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_8/checkpoint-192] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_8/checkpoint-240\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_8/checkpoint-240/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_8/checkpoint-240/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_8/checkpoint-240/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_8/checkpoint-240/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_8/checkpoint-208] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_8/checkpoint-256\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_8/checkpoint-256/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_8/checkpoint-256/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_8/checkpoint-256/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_8/checkpoint-256/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_8/checkpoint-224] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_8/checkpoint-272\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_8/checkpoint-272/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_8/checkpoint-272/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_8/checkpoint-272/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_8/checkpoint-272/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_8/checkpoint-240] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_8/checkpoint-288\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_8/checkpoint-288/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_8/checkpoint-288/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_8/checkpoint-288/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_8/checkpoint-288/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_8/checkpoint-256] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_8/checkpoint-304\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_8/checkpoint-304/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_8/checkpoint-304/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_8/checkpoint-304/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_8/checkpoint-304/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_8/checkpoint-272] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_8/checkpoint-320\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_8/checkpoint-320/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_8/checkpoint-320/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_8/checkpoint-320/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_8/checkpoint-320/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_8/checkpoint-288] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>███▇▇▆▆▅▅▄▃▃▂▂▂▁▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▃▅▅▂▃▅█▅▅▆▇▄▆▅▅▄▆▁▇▅</td></tr><tr><td>eval/samples_per_second</td><td>▅▄▄▆▆▄▁▄▄▃▂▄▃▃▃▄▂█▂▄</td></tr><tr><td>eval/steps_per_second</td><td>▅▄▄▆▆▄▁▄▄▃▂▄▃▃▃▄▂█▂▄</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇█████</td></tr><tr><td>train/learning_rate</td><td>██▇▇▇▆▆▅▅▅▄▄▄▃▃▂▂▂▁▁</td></tr><tr><td>train/loss</td><td>███▇▇▇▆▅▅▄▄▃▃▂▂▂▁▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.56491</td></tr><tr><td>eval/runtime</td><td>0.065</td></tr><tr><td>eval/samples_per_second</td><td>3077.327</td></tr><tr><td>eval/steps_per_second</td><td>30.773</td></tr><tr><td>train/epoch</td><td>20.0</td></tr><tr><td>train/global_step</td><td>320</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.5794</td></tr><tr><td>train/total_flos</td><td>212168160000.0</td></tr><tr><td>train/train_loss</td><td>0.63202</td></tr><tr><td>train/train_runtime</td><td>25.433</td></tr><tr><td>train/train_samples_per_second</td><td>1572.76</td></tr><tr><td>train/train_steps_per_second</td><td>12.582</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">pos syn style 8</strong>: <a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/3otq7r9e\" target=\"_blank\">https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/3otq7r9e</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/scratch/data_jz17d/wandb_tmp/wandb/run-20221114_112329-3otq7r9e/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6db8bccf29ef498b801785517af7e5d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22dd07182e974d8f9a4d7b717013f6f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"/scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 32,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 128,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 128,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 54\n",
      "}\n",
      "\n",
      "loading weights file /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/ and are newly initialized: ['bert.pooler.dense.weight', 'classifier.weight', 'bert.pooler.dense.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch/data_jz17d/wandb_tmp/wandb/run-20221114_112403-1v5kltnv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/1v5kltnv\" target=\"_blank\">pos syn style 9</a></strong> to <a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2000\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 128\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 320\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='320' max='320' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [320/320 00:15, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.693100</td>\n",
       "      <td>0.692089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.692600</td>\n",
       "      <td>0.691669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.692300</td>\n",
       "      <td>0.691262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.692000</td>\n",
       "      <td>0.690882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.691600</td>\n",
       "      <td>0.690526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.691300</td>\n",
       "      <td>0.690193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.691000</td>\n",
       "      <td>0.689892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.690700</td>\n",
       "      <td>0.689610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.690600</td>\n",
       "      <td>0.689347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.690300</td>\n",
       "      <td>0.689111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.690000</td>\n",
       "      <td>0.688896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.689900</td>\n",
       "      <td>0.688703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.689600</td>\n",
       "      <td>0.688527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.689600</td>\n",
       "      <td>0.688377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.689300</td>\n",
       "      <td>0.688255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.689200</td>\n",
       "      <td>0.688147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.689200</td>\n",
       "      <td>0.688067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.689000</td>\n",
       "      <td>0.688009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.689200</td>\n",
       "      <td>0.687976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.689200</td>\n",
       "      <td>0.687964</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_9/checkpoint-16\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_9/checkpoint-16/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_9/checkpoint-16/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_9/checkpoint-16/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_9/checkpoint-16/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_9/checkpoint-32\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_9/checkpoint-32/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_9/checkpoint-32/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_9/checkpoint-32/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_9/checkpoint-32/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_9/checkpoint-48\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_9/checkpoint-48/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_9/checkpoint-48/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_9/checkpoint-48/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_9/checkpoint-48/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_9/checkpoint-16] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_9/checkpoint-64\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_9/checkpoint-64/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_9/checkpoint-64/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_9/checkpoint-64/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_9/checkpoint-64/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_9/checkpoint-32] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_9/checkpoint-80\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_9/checkpoint-80/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_9/checkpoint-80/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_9/checkpoint-80/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_9/checkpoint-80/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_9/checkpoint-48] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_9/checkpoint-96\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_9/checkpoint-96/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_9/checkpoint-96/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_9/checkpoint-96/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_9/checkpoint-96/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_9/checkpoint-64] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_9/checkpoint-112\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_9/checkpoint-112/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_9/checkpoint-112/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_9/checkpoint-112/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_9/checkpoint-112/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_9/checkpoint-80] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_9/checkpoint-128\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_9/checkpoint-128/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_9/checkpoint-128/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_9/checkpoint-128/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_9/checkpoint-128/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_9/checkpoint-96] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_9/checkpoint-144\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_9/checkpoint-144/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_9/checkpoint-144/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_9/checkpoint-144/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_9/checkpoint-144/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_9/checkpoint-112] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_9/checkpoint-160\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_9/checkpoint-160/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_9/checkpoint-160/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_9/checkpoint-160/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_9/checkpoint-160/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_9/checkpoint-128] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_9/checkpoint-176\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_9/checkpoint-176/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_9/checkpoint-176/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_9/checkpoint-176/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_9/checkpoint-176/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_9/checkpoint-144] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_9/checkpoint-192\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_9/checkpoint-192/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_9/checkpoint-192/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_9/checkpoint-192/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_9/checkpoint-192/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_9/checkpoint-160] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_9/checkpoint-208\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_9/checkpoint-208/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_9/checkpoint-208/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_9/checkpoint-208/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_9/checkpoint-208/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_9/checkpoint-176] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_9/checkpoint-224\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_9/checkpoint-224/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_9/checkpoint-224/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_9/checkpoint-224/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_9/checkpoint-224/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_9/checkpoint-192] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_9/checkpoint-240\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_9/checkpoint-240/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_9/checkpoint-240/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_9/checkpoint-240/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_9/checkpoint-240/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_9/checkpoint-208] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_9/checkpoint-256\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_9/checkpoint-256/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_9/checkpoint-256/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_9/checkpoint-256/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_9/checkpoint-256/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_9/checkpoint-224] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_9/checkpoint-272\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_9/checkpoint-272/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_9/checkpoint-272/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_9/checkpoint-272/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_9/checkpoint-272/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_9/checkpoint-240] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_9/checkpoint-288\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_9/checkpoint-288/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_9/checkpoint-288/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_9/checkpoint-288/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_9/checkpoint-288/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_9/checkpoint-256] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_9/checkpoint-304\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_9/checkpoint-304/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_9/checkpoint-304/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_9/checkpoint-304/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_9/checkpoint-304/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_9/checkpoint-272] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_9/checkpoint-320\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_9/checkpoint-320/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_9/checkpoint-320/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_9/checkpoint-320/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_9/checkpoint-320/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_9/checkpoint-288] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▇▇▆▅▅▄▄▃▃▃▂▂▂▁▁▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▄▁▃▅▅▅▄▄▅▅▅▁▄▄█▄▄▅▃▃</td></tr><tr><td>eval/samples_per_second</td><td>▅█▅▃▄▄▅▄▄▃▄█▅▄▁▄▄▃▆▅</td></tr><tr><td>eval/steps_per_second</td><td>▅█▅▃▄▄▅▄▄▃▄█▅▄▁▄▄▃▆▅</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇█████</td></tr><tr><td>train/learning_rate</td><td>██▇▇▇▆▆▅▅▅▄▄▄▃▃▂▂▂▁▁</td></tr><tr><td>train/loss</td><td>█▇▇▆▅▅▄▄▄▃▃▃▂▂▂▁▁▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.68796</td></tr><tr><td>eval/runtime</td><td>0.0645</td></tr><tr><td>eval/samples_per_second</td><td>3102.606</td></tr><tr><td>eval/steps_per_second</td><td>31.026</td></tr><tr><td>train/epoch</td><td>20.0</td></tr><tr><td>train/global_step</td><td>320</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.6892</td></tr><tr><td>train/total_flos</td><td>212168160000.0</td></tr><tr><td>train/train_loss</td><td>0.69049</td></tr><tr><td>train/train_runtime</td><td>16.0467</td></tr><tr><td>train/train_samples_per_second</td><td>2492.728</td></tr><tr><td>train/train_steps_per_second</td><td>19.942</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">pos syn style 9</strong>: <a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/1v5kltnv\" target=\"_blank\">https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/1v5kltnv</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/scratch/data_jz17d/wandb_tmp/wandb/run-20221114_112403-1v5kltnv/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d435e4e9356413a99ccb3fd85df8048",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09611948e2a643a4819996b37261b3e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"/scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 32,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 128,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 128,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 54\n",
      "}\n",
      "\n",
      "loading weights file /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/ and are newly initialized: ['bert.pooler.dense.weight', 'classifier.weight', 'bert.pooler.dense.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch/data_jz17d/wandb_tmp/wandb/run-20221114_112427-25dw16o2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/25dw16o2\" target=\"_blank\">pos syn style 10</a></strong> to <a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2000\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 128\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 320\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='320' max='320' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [320/320 00:20, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.692300</td>\n",
       "      <td>0.690096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.689900</td>\n",
       "      <td>0.687622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.687600</td>\n",
       "      <td>0.684771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.684800</td>\n",
       "      <td>0.681445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.681400</td>\n",
       "      <td>0.677440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.677400</td>\n",
       "      <td>0.672731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.673000</td>\n",
       "      <td>0.667401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.667800</td>\n",
       "      <td>0.661415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.662600</td>\n",
       "      <td>0.654939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.656900</td>\n",
       "      <td>0.648232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.650800</td>\n",
       "      <td>0.641578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.645100</td>\n",
       "      <td>0.635030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.639000</td>\n",
       "      <td>0.628899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.634500</td>\n",
       "      <td>0.623304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.629300</td>\n",
       "      <td>0.618428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.625100</td>\n",
       "      <td>0.614361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.622400</td>\n",
       "      <td>0.611166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.619300</td>\n",
       "      <td>0.608848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.618400</td>\n",
       "      <td>0.607450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.616800</td>\n",
       "      <td>0.606974</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_10/checkpoint-16\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_10/checkpoint-16/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_10/checkpoint-16/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_10/checkpoint-16/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_10/checkpoint-16/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_10/checkpoint-32\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_10/checkpoint-32/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_10/checkpoint-32/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_10/checkpoint-32/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_10/checkpoint-32/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_10/checkpoint-48\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_10/checkpoint-48/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_10/checkpoint-48/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_10/checkpoint-48/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_10/checkpoint-48/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_10/checkpoint-16] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_10/checkpoint-64\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_10/checkpoint-64/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_10/checkpoint-64/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_10/checkpoint-64/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_10/checkpoint-64/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_10/checkpoint-32] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_10/checkpoint-80\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_10/checkpoint-80/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_10/checkpoint-80/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_10/checkpoint-80/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_10/checkpoint-80/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_10/checkpoint-48] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_10/checkpoint-96\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_10/checkpoint-96/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_10/checkpoint-96/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_10/checkpoint-96/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_10/checkpoint-96/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_10/checkpoint-64] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_10/checkpoint-112\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_10/checkpoint-112/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_10/checkpoint-112/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_10/checkpoint-112/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_10/checkpoint-112/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_10/checkpoint-80] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_10/checkpoint-128\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_10/checkpoint-128/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_10/checkpoint-128/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_10/checkpoint-128/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_10/checkpoint-128/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_10/checkpoint-96] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_10/checkpoint-144\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_10/checkpoint-144/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_10/checkpoint-144/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_10/checkpoint-144/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_10/checkpoint-144/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_10/checkpoint-112] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_10/checkpoint-160\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_10/checkpoint-160/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_10/checkpoint-160/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_10/checkpoint-160/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_10/checkpoint-160/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_10/checkpoint-128] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_10/checkpoint-176\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_10/checkpoint-176/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_10/checkpoint-176/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_10/checkpoint-176/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_10/checkpoint-176/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_10/checkpoint-144] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_10/checkpoint-192\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_10/checkpoint-192/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_10/checkpoint-192/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_10/checkpoint-192/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_10/checkpoint-192/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_10/checkpoint-160] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_10/checkpoint-208\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_10/checkpoint-208/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_10/checkpoint-208/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_10/checkpoint-208/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_10/checkpoint-208/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_10/checkpoint-176] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_10/checkpoint-224\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_10/checkpoint-224/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_10/checkpoint-224/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_10/checkpoint-224/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_10/checkpoint-224/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_10/checkpoint-192] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_10/checkpoint-240\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_10/checkpoint-240/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_10/checkpoint-240/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_10/checkpoint-240/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_10/checkpoint-240/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_10/checkpoint-208] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_10/checkpoint-256\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_10/checkpoint-256/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_10/checkpoint-256/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_10/checkpoint-256/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_10/checkpoint-256/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_10/checkpoint-224] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_10/checkpoint-272\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_10/checkpoint-272/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_10/checkpoint-272/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_10/checkpoint-272/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_10/checkpoint-272/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_10/checkpoint-240] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_10/checkpoint-288\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_10/checkpoint-288/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_10/checkpoint-288/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_10/checkpoint-288/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_10/checkpoint-288/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_10/checkpoint-256] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_10/checkpoint-304\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_10/checkpoint-304/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_10/checkpoint-304/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_10/checkpoint-304/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_10/checkpoint-304/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_10/checkpoint-272] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_10/checkpoint-320\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_10/checkpoint-320/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_10/checkpoint-320/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_10/checkpoint-320/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_10/checkpoint-320/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_10/checkpoint-288] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>███▇▇▇▆▆▅▄▄▃▃▂▂▂▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▅▅▅▅▅▅▁▅▅▁▆▅▄▅▅█▁▅▅▆</td></tr><tr><td>eval/samples_per_second</td><td>▃▄▃▃▄▃█▃▃█▃▃▄▄▄▁▇▃▃▃</td></tr><tr><td>eval/steps_per_second</td><td>▃▄▃▃▄▃█▃▃█▃▃▄▄▄▁▇▃▃▃</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇█████</td></tr><tr><td>train/learning_rate</td><td>██▇▇▇▆▆▅▅▅▄▄▄▃▃▂▂▂▁▁</td></tr><tr><td>train/loss</td><td>███▇▇▇▆▆▅▅▄▄▃▃▂▂▂▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.60697</td></tr><tr><td>eval/runtime</td><td>0.0669</td></tr><tr><td>eval/samples_per_second</td><td>2990.442</td></tr><tr><td>eval/steps_per_second</td><td>29.904</td></tr><tr><td>train/epoch</td><td>20.0</td></tr><tr><td>train/global_step</td><td>320</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.6168</td></tr><tr><td>train/total_flos</td><td>212168160000.0</td></tr><tr><td>train/train_loss</td><td>0.65373</td></tr><tr><td>train/train_runtime</td><td>20.1455</td></tr><tr><td>train/train_samples_per_second</td><td>1985.556</td></tr><tr><td>train/train_steps_per_second</td><td>15.884</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">pos syn style 10</strong>: <a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/25dw16o2\" target=\"_blank\">https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/25dw16o2</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/scratch/data_jz17d/wandb_tmp/wandb/run-20221114_112427-25dw16o2/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4f14c3e1b1746b1af5df476029c756c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94e125bb126c4451a504bf444dd973eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"/scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 32,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 128,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 128,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 54\n",
      "}\n",
      "\n",
      "loading weights file /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/ and are newly initialized: ['bert.pooler.dense.weight', 'classifier.weight', 'bert.pooler.dense.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch/data_jz17d/wandb_tmp/wandb/run-20221114_112456-2vmymkcs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/2vmymkcs\" target=\"_blank\">pos syn style 11</a></strong> to <a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2000\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 128\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 320\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='302' max='320' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [302/320 00:16 < 00:00, 18.70 it/s, Epoch 18.81/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.692400</td>\n",
       "      <td>0.690490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.690500</td>\n",
       "      <td>0.688514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.688700</td>\n",
       "      <td>0.686414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.686800</td>\n",
       "      <td>0.684150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.684500</td>\n",
       "      <td>0.681623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.682000</td>\n",
       "      <td>0.678841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.679500</td>\n",
       "      <td>0.675833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.676500</td>\n",
       "      <td>0.672581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.673800</td>\n",
       "      <td>0.669132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.670500</td>\n",
       "      <td>0.665600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.666900</td>\n",
       "      <td>0.662055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.663800</td>\n",
       "      <td>0.658564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.660300</td>\n",
       "      <td>0.655200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.658200</td>\n",
       "      <td>0.652163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.654500</td>\n",
       "      <td>0.649479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.652400</td>\n",
       "      <td>0.647178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.651100</td>\n",
       "      <td>0.645383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.649200</td>\n",
       "      <td>0.644087</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_11/checkpoint-16\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_11/checkpoint-16/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_11/checkpoint-16/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_11/checkpoint-16/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_11/checkpoint-16/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_11/checkpoint-32\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_11/checkpoint-32/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_11/checkpoint-32/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_11/checkpoint-32/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_11/checkpoint-32/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_11/checkpoint-48\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_11/checkpoint-48/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_11/checkpoint-48/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_11/checkpoint-48/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_11/checkpoint-48/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_11/checkpoint-16] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_11/checkpoint-64\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_11/checkpoint-64/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_11/checkpoint-64/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_11/checkpoint-64/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_11/checkpoint-64/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_11/checkpoint-32] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_11/checkpoint-80\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_11/checkpoint-80/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_11/checkpoint-80/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_11/checkpoint-80/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_11/checkpoint-80/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_11/checkpoint-48] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_11/checkpoint-96\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_11/checkpoint-96/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_11/checkpoint-96/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_11/checkpoint-96/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_11/checkpoint-96/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_11/checkpoint-64] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_11/checkpoint-112\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_11/checkpoint-112/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_11/checkpoint-112/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_11/checkpoint-112/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_11/checkpoint-112/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_11/checkpoint-80] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_11/checkpoint-128\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_11/checkpoint-128/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_11/checkpoint-128/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_11/checkpoint-128/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_11/checkpoint-128/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_11/checkpoint-96] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_11/checkpoint-144\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_11/checkpoint-144/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_11/checkpoint-144/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_11/checkpoint-144/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_11/checkpoint-144/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_11/checkpoint-112] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_11/checkpoint-160\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_11/checkpoint-160/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_11/checkpoint-160/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_11/checkpoint-160/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_11/checkpoint-160/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_11/checkpoint-128] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_11/checkpoint-176\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_11/checkpoint-176/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_11/checkpoint-176/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_11/checkpoint-176/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_11/checkpoint-176/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_11/checkpoint-144] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_11/checkpoint-192\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_11/checkpoint-192/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_11/checkpoint-192/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_11/checkpoint-192/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_11/checkpoint-192/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_11/checkpoint-160] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_11/checkpoint-208\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_11/checkpoint-208/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_11/checkpoint-208/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_11/checkpoint-208/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_11/checkpoint-208/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_11/checkpoint-176] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_11/checkpoint-224\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_11/checkpoint-224/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_11/checkpoint-224/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_11/checkpoint-224/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_11/checkpoint-224/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_11/checkpoint-192] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_11/checkpoint-240\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_11/checkpoint-240/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_11/checkpoint-240/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_11/checkpoint-240/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_11/checkpoint-240/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_11/checkpoint-208] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_11/checkpoint-256\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_11/checkpoint-256/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_11/checkpoint-256/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_11/checkpoint-256/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_11/checkpoint-256/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_11/checkpoint-224] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_11/checkpoint-272\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_11/checkpoint-272/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_11/checkpoint-272/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_11/checkpoint-272/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_11/checkpoint-272/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_11/checkpoint-240] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_11/checkpoint-288\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_11/checkpoint-288/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_11/checkpoint-288/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_11/checkpoint-288/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_11/checkpoint-288/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_11/checkpoint-256] due to args.save_total_limit\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>██▇▇▇▆▆▅▅▄▄▃▃▂▂▂▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▅▁█▅█▇▇▇█▇▇▇▆▇▇▆▇▆▅█</td></tr><tr><td>eval/samples_per_second</td><td>▄█▁▃▁▂▂▂▁▂▂▂▂▂▂▃▂▃▃▁</td></tr><tr><td>eval/steps_per_second</td><td>▄█▁▃▁▂▂▂▁▂▂▂▂▂▂▃▂▃▃▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇█████</td></tr><tr><td>train/learning_rate</td><td>██▇▇▇▆▆▅▅▅▄▄▄▃▃▂▂▂▁▁</td></tr><tr><td>train/loss</td><td>██▇▇▇▆▆▅▅▅▄▃▃▃▂▂▁▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.64303</td></tr><tr><td>eval/runtime</td><td>0.0672</td></tr><tr><td>eval/samples_per_second</td><td>2975.201</td></tr><tr><td>eval/steps_per_second</td><td>29.752</td></tr><tr><td>train/epoch</td><td>20.0</td></tr><tr><td>train/global_step</td><td>320</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.648</td></tr><tr><td>train/total_flos</td><td>212168160000.0</td></tr><tr><td>train/train_loss</td><td>0.66893</td></tr><tr><td>train/train_runtime</td><td>17.3383</td></tr><tr><td>train/train_samples_per_second</td><td>2307.034</td></tr><tr><td>train/train_steps_per_second</td><td>18.456</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">pos syn style 11</strong>: <a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/2vmymkcs\" target=\"_blank\">https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/2vmymkcs</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/scratch/data_jz17d/wandb_tmp/wandb/run-20221114_112456-2vmymkcs/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c7b7f2c249d43ab9a0f29616a7434a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37b680763a6e4c5dac01b63cb524d6d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"/scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 32,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 128,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 128,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 54\n",
      "}\n",
      "\n",
      "loading weights file /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/ and are newly initialized: ['bert.pooler.dense.weight', 'classifier.weight', 'bert.pooler.dense.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch/data_jz17d/wandb_tmp/wandb/run-20221114_112523-3vtpbgnt</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/3vtpbgnt\" target=\"_blank\">pos syn style 12</a></strong> to <a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2000\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 128\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 320\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='320' max='320' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [320/320 00:24, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.691700</td>\n",
       "      <td>0.688911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.687800</td>\n",
       "      <td>0.684634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.683500</td>\n",
       "      <td>0.679291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.677900</td>\n",
       "      <td>0.672564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.671200</td>\n",
       "      <td>0.664016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.662600</td>\n",
       "      <td>0.653934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.653300</td>\n",
       "      <td>0.642663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.643100</td>\n",
       "      <td>0.630621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.632900</td>\n",
       "      <td>0.619012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.622500</td>\n",
       "      <td>0.608028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.612500</td>\n",
       "      <td>0.597677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.603200</td>\n",
       "      <td>0.588199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.594500</td>\n",
       "      <td>0.579726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.587500</td>\n",
       "      <td>0.572347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.579800</td>\n",
       "      <td>0.566157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.575400</td>\n",
       "      <td>0.561186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.570500</td>\n",
       "      <td>0.557174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.568100</td>\n",
       "      <td>0.554434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.565100</td>\n",
       "      <td>0.552783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.564700</td>\n",
       "      <td>0.552236</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_12/checkpoint-16\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_12/checkpoint-16/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_12/checkpoint-16/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_12/checkpoint-16/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_12/checkpoint-16/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_12/checkpoint-32\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_12/checkpoint-32/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_12/checkpoint-32/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_12/checkpoint-32/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_12/checkpoint-32/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_12/checkpoint-48\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_12/checkpoint-48/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_12/checkpoint-48/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_12/checkpoint-48/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_12/checkpoint-48/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_12/checkpoint-16] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_12/checkpoint-64\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_12/checkpoint-64/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_12/checkpoint-64/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_12/checkpoint-64/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_12/checkpoint-64/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_12/checkpoint-32] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_12/checkpoint-80\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_12/checkpoint-80/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_12/checkpoint-80/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_12/checkpoint-80/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_12/checkpoint-80/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_12/checkpoint-48] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_12/checkpoint-96\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_12/checkpoint-96/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_12/checkpoint-96/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_12/checkpoint-96/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_12/checkpoint-96/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_12/checkpoint-64] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_12/checkpoint-112\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_12/checkpoint-112/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_12/checkpoint-112/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_12/checkpoint-112/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_12/checkpoint-112/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_12/checkpoint-80] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_12/checkpoint-128\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_12/checkpoint-128/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_12/checkpoint-128/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_12/checkpoint-128/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_12/checkpoint-128/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_12/checkpoint-96] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_12/checkpoint-144\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_12/checkpoint-144/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_12/checkpoint-144/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_12/checkpoint-144/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_12/checkpoint-144/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_12/checkpoint-112] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_12/checkpoint-160\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_12/checkpoint-160/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_12/checkpoint-160/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_12/checkpoint-160/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_12/checkpoint-160/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_12/checkpoint-128] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_12/checkpoint-176\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_12/checkpoint-176/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_12/checkpoint-176/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_12/checkpoint-176/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_12/checkpoint-176/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_12/checkpoint-144] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_12/checkpoint-192\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_12/checkpoint-192/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_12/checkpoint-192/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_12/checkpoint-192/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_12/checkpoint-192/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_12/checkpoint-160] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_12/checkpoint-208\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_12/checkpoint-208/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_12/checkpoint-208/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_12/checkpoint-208/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_12/checkpoint-208/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_12/checkpoint-176] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_12/checkpoint-224\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_12/checkpoint-224/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_12/checkpoint-224/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_12/checkpoint-224/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_12/checkpoint-224/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_12/checkpoint-192] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_12/checkpoint-240\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_12/checkpoint-240/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_12/checkpoint-240/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_12/checkpoint-240/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_12/checkpoint-240/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_12/checkpoint-208] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_12/checkpoint-256\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_12/checkpoint-256/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_12/checkpoint-256/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_12/checkpoint-256/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_12/checkpoint-256/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_12/checkpoint-224] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_12/checkpoint-272\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_12/checkpoint-272/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_12/checkpoint-272/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_12/checkpoint-272/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_12/checkpoint-272/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_12/checkpoint-240] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_12/checkpoint-288\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_12/checkpoint-288/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_12/checkpoint-288/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_12/checkpoint-288/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_12/checkpoint-288/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_12/checkpoint-256] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_12/checkpoint-304\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_12/checkpoint-304/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_12/checkpoint-304/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_12/checkpoint-304/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_12/checkpoint-304/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_12/checkpoint-272] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_12/checkpoint-320\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_12/checkpoint-320/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_12/checkpoint-320/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_12/checkpoint-320/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_12/checkpoint-320/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_12/checkpoint-288] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>███▇▇▆▆▅▄▄▃▃▂▂▂▁▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▂▄█▅▂▅▆▅▄▄▆▅▅▆▆▄▅▁▇▅</td></tr><tr><td>eval/samples_per_second</td><td>▇▄▁▃▆▄▂▄▄▄▃▄▄▃▃▄▄█▂▄</td></tr><tr><td>eval/steps_per_second</td><td>▇▄▁▃▆▄▂▄▄▄▃▄▄▃▃▄▄█▂▄</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇█████</td></tr><tr><td>train/learning_rate</td><td>██▇▇▇▆▆▅▅▅▄▄▄▃▃▂▂▂▁▁</td></tr><tr><td>train/loss</td><td>███▇▇▆▆▅▅▄▄▃▃▂▂▂▁▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.55224</td></tr><tr><td>eval/runtime</td><td>0.0658</td></tr><tr><td>eval/samples_per_second</td><td>3039.769</td></tr><tr><td>eval/steps_per_second</td><td>30.398</td></tr><tr><td>train/epoch</td><td>20.0</td></tr><tr><td>train/global_step</td><td>320</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.5647</td></tr><tr><td>train/total_flos</td><td>212168160000.0</td></tr><tr><td>train/train_loss</td><td>0.62239</td></tr><tr><td>train/train_runtime</td><td>24.9955</td></tr><tr><td>train/train_samples_per_second</td><td>1600.286</td></tr><tr><td>train/train_steps_per_second</td><td>12.802</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">pos syn style 12</strong>: <a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/3vtpbgnt\" target=\"_blank\">https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/3vtpbgnt</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/scratch/data_jz17d/wandb_tmp/wandb/run-20221114_112523-3vtpbgnt/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "936e48e31474407aa4e56ff87ba12cae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1aebcb9249aa4b84a94345de1c95404d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"/scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 32,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 128,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 128,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 54\n",
      "}\n",
      "\n",
      "loading weights file /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/ and are newly initialized: ['bert.pooler.dense.weight', 'classifier.weight', 'bert.pooler.dense.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch/data_jz17d/wandb_tmp/wandb/run-20221114_112555-3bke7eip</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/3bke7eip\" target=\"_blank\">pos syn style 13</a></strong> to <a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2000\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 128\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 320\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='320' max='320' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [320/320 00:16, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.693200</td>\n",
       "      <td>0.692392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.692600</td>\n",
       "      <td>0.691846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.692100</td>\n",
       "      <td>0.691320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.691600</td>\n",
       "      <td>0.690828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.691300</td>\n",
       "      <td>0.690364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.690800</td>\n",
       "      <td>0.689933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.690400</td>\n",
       "      <td>0.689537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.690100</td>\n",
       "      <td>0.689171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.689800</td>\n",
       "      <td>0.688832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.689500</td>\n",
       "      <td>0.688526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.689100</td>\n",
       "      <td>0.688249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.689000</td>\n",
       "      <td>0.687997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.688700</td>\n",
       "      <td>0.687774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.688600</td>\n",
       "      <td>0.687579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.688300</td>\n",
       "      <td>0.687418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.688200</td>\n",
       "      <td>0.687279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.688100</td>\n",
       "      <td>0.687176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.687900</td>\n",
       "      <td>0.687101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.688000</td>\n",
       "      <td>0.687057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.688100</td>\n",
       "      <td>0.687042</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_13/checkpoint-16\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_13/checkpoint-16/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_13/checkpoint-16/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_13/checkpoint-16/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_13/checkpoint-16/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_13/checkpoint-32\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_13/checkpoint-32/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_13/checkpoint-32/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_13/checkpoint-32/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_13/checkpoint-32/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_13/checkpoint-48\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_13/checkpoint-48/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_13/checkpoint-48/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_13/checkpoint-48/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_13/checkpoint-48/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_13/checkpoint-16] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_13/checkpoint-64\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_13/checkpoint-64/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_13/checkpoint-64/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_13/checkpoint-64/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_13/checkpoint-64/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_13/checkpoint-32] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_13/checkpoint-80\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_13/checkpoint-80/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_13/checkpoint-80/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_13/checkpoint-80/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_13/checkpoint-80/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_13/checkpoint-48] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_13/checkpoint-96\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_13/checkpoint-96/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_13/checkpoint-96/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_13/checkpoint-96/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_13/checkpoint-96/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_13/checkpoint-64] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_13/checkpoint-112\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_13/checkpoint-112/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_13/checkpoint-112/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_13/checkpoint-112/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_13/checkpoint-112/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_13/checkpoint-80] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_13/checkpoint-128\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_13/checkpoint-128/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_13/checkpoint-128/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_13/checkpoint-128/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_13/checkpoint-128/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_13/checkpoint-96] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_13/checkpoint-144\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_13/checkpoint-144/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_13/checkpoint-144/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_13/checkpoint-144/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_13/checkpoint-144/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_13/checkpoint-112] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_13/checkpoint-160\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_13/checkpoint-160/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_13/checkpoint-160/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_13/checkpoint-160/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_13/checkpoint-160/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_13/checkpoint-128] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_13/checkpoint-176\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_13/checkpoint-176/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_13/checkpoint-176/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_13/checkpoint-176/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_13/checkpoint-176/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_13/checkpoint-144] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_13/checkpoint-192\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_13/checkpoint-192/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_13/checkpoint-192/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_13/checkpoint-192/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_13/checkpoint-192/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_13/checkpoint-160] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_13/checkpoint-208\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_13/checkpoint-208/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_13/checkpoint-208/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_13/checkpoint-208/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_13/checkpoint-208/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_13/checkpoint-176] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_13/checkpoint-224\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_13/checkpoint-224/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_13/checkpoint-224/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_13/checkpoint-224/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_13/checkpoint-224/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_13/checkpoint-192] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_13/checkpoint-240\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_13/checkpoint-240/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_13/checkpoint-240/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_13/checkpoint-240/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_13/checkpoint-240/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_13/checkpoint-208] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_13/checkpoint-256\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_13/checkpoint-256/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_13/checkpoint-256/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_13/checkpoint-256/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_13/checkpoint-256/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_13/checkpoint-224] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_13/checkpoint-272\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_13/checkpoint-272/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_13/checkpoint-272/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_13/checkpoint-272/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_13/checkpoint-272/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_13/checkpoint-240] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_13/checkpoint-288\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_13/checkpoint-288/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_13/checkpoint-288/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_13/checkpoint-288/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_13/checkpoint-288/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_13/checkpoint-256] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_13/checkpoint-304\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_13/checkpoint-304/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_13/checkpoint-304/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_13/checkpoint-304/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_13/checkpoint-304/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_13/checkpoint-272] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_13/checkpoint-320\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_13/checkpoint-320/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_13/checkpoint-320/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_13/checkpoint-320/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_13/checkpoint-320/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_13/checkpoint-288] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▇▇▆▅▅▄▄▃▃▃▂▂▂▁▁▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▅▅▄▄▆▆▅▅▅▄▅▂█▁▁▅▅▅▁▄</td></tr><tr><td>eval/samples_per_second</td><td>▄▄▅▅▃▃▄▃▄▄▄▇▁██▄▄▄█▄</td></tr><tr><td>eval/steps_per_second</td><td>▄▄▅▅▃▃▄▃▄▄▄▇▁██▄▄▄█▄</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇█████</td></tr><tr><td>train/learning_rate</td><td>██▇▇▇▆▆▅▅▅▄▄▄▃▃▂▂▂▁▁</td></tr><tr><td>train/loss</td><td>█▇▇▆▅▅▄▄▄▃▃▂▂▂▂▁▁▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.68704</td></tr><tr><td>eval/runtime</td><td>0.0652</td></tr><tr><td>eval/samples_per_second</td><td>3068.793</td></tr><tr><td>eval/steps_per_second</td><td>30.688</td></tr><tr><td>train/epoch</td><td>20.0</td></tr><tr><td>train/global_step</td><td>320</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.6881</td></tr><tr><td>train/total_flos</td><td>212168160000.0</td></tr><tr><td>train/train_loss</td><td>0.68977</td></tr><tr><td>train/train_runtime</td><td>16.2254</td></tr><tr><td>train/train_samples_per_second</td><td>2465.277</td></tr><tr><td>train/train_steps_per_second</td><td>19.722</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">pos syn style 13</strong>: <a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/3bke7eip\" target=\"_blank\">https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/3bke7eip</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/scratch/data_jz17d/wandb_tmp/wandb/run-20221114_112555-3bke7eip/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52ce522fcaad4dd4852f9804ade24326",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c881f7548927461388582c23f7319b9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"/scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 32,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 128,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 128,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 54\n",
      "}\n",
      "\n",
      "loading weights file /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/ and are newly initialized: ['bert.pooler.dense.weight', 'classifier.weight', 'bert.pooler.dense.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch/data_jz17d/wandb_tmp/wandb/run-20221114_112621-30i39vc5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/30i39vc5\" target=\"_blank\">pos syn style 14</a></strong> to <a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2000\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 128\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 320\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='320' max='320' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [320/320 00:19, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.692100</td>\n",
       "      <td>0.689766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.689000</td>\n",
       "      <td>0.686538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.686000</td>\n",
       "      <td>0.682857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.682200</td>\n",
       "      <td>0.678597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.678100</td>\n",
       "      <td>0.673559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.673000</td>\n",
       "      <td>0.667761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.667300</td>\n",
       "      <td>0.661230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.661400</td>\n",
       "      <td>0.654160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.654600</td>\n",
       "      <td>0.646626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.647900</td>\n",
       "      <td>0.638902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.640700</td>\n",
       "      <td>0.631328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.634200</td>\n",
       "      <td>0.623984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.627100</td>\n",
       "      <td>0.617166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.621600</td>\n",
       "      <td>0.611018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.615200</td>\n",
       "      <td>0.605610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.611600</td>\n",
       "      <td>0.601145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.607900</td>\n",
       "      <td>0.597636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.604200</td>\n",
       "      <td>0.595102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.603400</td>\n",
       "      <td>0.593599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.601900</td>\n",
       "      <td>0.593080</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_14/checkpoint-16\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_14/checkpoint-16/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_14/checkpoint-16/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_14/checkpoint-16/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_14/checkpoint-16/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_14/checkpoint-32\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_14/checkpoint-32/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_14/checkpoint-32/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_14/checkpoint-32/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_14/checkpoint-32/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_14/checkpoint-48\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_14/checkpoint-48/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_14/checkpoint-48/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_14/checkpoint-48/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_14/checkpoint-48/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_14/checkpoint-16] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_14/checkpoint-64\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_14/checkpoint-64/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_14/checkpoint-64/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_14/checkpoint-64/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_14/checkpoint-64/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_14/checkpoint-32] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_14/checkpoint-80\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_14/checkpoint-80/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_14/checkpoint-80/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_14/checkpoint-80/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_14/checkpoint-80/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_14/checkpoint-48] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_14/checkpoint-96\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_14/checkpoint-96/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_14/checkpoint-96/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_14/checkpoint-96/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_14/checkpoint-96/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_14/checkpoint-64] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_14/checkpoint-112\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_14/checkpoint-112/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_14/checkpoint-112/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_14/checkpoint-112/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_14/checkpoint-112/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_14/checkpoint-80] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_14/checkpoint-128\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_14/checkpoint-128/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_14/checkpoint-128/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_14/checkpoint-128/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_14/checkpoint-128/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_14/checkpoint-96] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_14/checkpoint-144\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_14/checkpoint-144/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_14/checkpoint-144/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_14/checkpoint-144/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_14/checkpoint-144/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_14/checkpoint-112] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_14/checkpoint-160\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_14/checkpoint-160/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_14/checkpoint-160/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_14/checkpoint-160/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_14/checkpoint-160/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_14/checkpoint-128] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_14/checkpoint-176\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_14/checkpoint-176/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_14/checkpoint-176/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_14/checkpoint-176/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_14/checkpoint-176/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_14/checkpoint-144] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_14/checkpoint-192\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_14/checkpoint-192/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_14/checkpoint-192/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_14/checkpoint-192/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_14/checkpoint-192/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_14/checkpoint-160] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_14/checkpoint-208\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_14/checkpoint-208/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_14/checkpoint-208/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_14/checkpoint-208/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_14/checkpoint-208/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_14/checkpoint-176] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_14/checkpoint-224\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_14/checkpoint-224/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_14/checkpoint-224/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_14/checkpoint-224/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_14/checkpoint-224/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_14/checkpoint-192] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_14/checkpoint-240\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_14/checkpoint-240/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_14/checkpoint-240/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_14/checkpoint-240/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_14/checkpoint-240/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_14/checkpoint-208] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_14/checkpoint-256\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_14/checkpoint-256/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_14/checkpoint-256/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_14/checkpoint-256/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_14/checkpoint-256/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_14/checkpoint-224] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_14/checkpoint-272\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_14/checkpoint-272/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_14/checkpoint-272/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_14/checkpoint-272/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_14/checkpoint-272/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_14/checkpoint-240] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_14/checkpoint-288\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_14/checkpoint-288/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_14/checkpoint-288/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_14/checkpoint-288/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_14/checkpoint-288/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_14/checkpoint-256] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_14/checkpoint-304\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_14/checkpoint-304/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_14/checkpoint-304/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_14/checkpoint-304/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_14/checkpoint-304/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_14/checkpoint-272] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_14/checkpoint-320\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_14/checkpoint-320/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_14/checkpoint-320/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_14/checkpoint-320/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_14/checkpoint-320/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_14/checkpoint-288] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>██▇▇▇▆▆▅▅▄▄▃▃▂▂▂▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▆▆▆▅█▅▅▆▃█▁▄▆▇▆▅▆▆▇▇</td></tr><tr><td>eval/samples_per_second</td><td>▃▃▃▃▁▄▃▃▅▁█▅▃▂▃▃▃▃▂▂</td></tr><tr><td>eval/steps_per_second</td><td>▃▃▃▃▁▄▃▃▅▁█▅▃▂▃▃▃▃▂▂</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇█████</td></tr><tr><td>train/learning_rate</td><td>██▇▇▇▆▆▅▅▅▄▄▄▃▃▂▂▂▁▁</td></tr><tr><td>train/loss</td><td>███▇▇▇▆▆▅▅▄▄▃▃▂▂▁▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.59308</td></tr><tr><td>eval/runtime</td><td>0.0676</td></tr><tr><td>eval/samples_per_second</td><td>2960.114</td></tr><tr><td>eval/steps_per_second</td><td>29.601</td></tr><tr><td>train/epoch</td><td>20.0</td></tr><tr><td>train/global_step</td><td>320</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.6019</td></tr><tr><td>train/total_flos</td><td>212168160000.0</td></tr><tr><td>train/train_loss</td><td>0.64496</td></tr><tr><td>train/train_runtime</td><td>19.9472</td></tr><tr><td>train/train_samples_per_second</td><td>2005.29</td></tr><tr><td>train/train_steps_per_second</td><td>16.042</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">pos syn style 14</strong>: <a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/30i39vc5\" target=\"_blank\">https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/30i39vc5</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/scratch/data_jz17d/wandb_tmp/wandb/run-20221114_112621-30i39vc5/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30a45529f0eb46449e65507e400b82b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61cac8d1a1df4989a1e01760c40aef72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"/scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 32,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 128,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 128,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 54\n",
      "}\n",
      "\n",
      "loading weights file /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/ and are newly initialized: ['bert.pooler.dense.weight', 'classifier.weight', 'bert.pooler.dense.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch/data_jz17d/wandb_tmp/wandb/run-20221114_112650-lt8s9djm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/lt8s9djm\" target=\"_blank\">pos syn style 15</a></strong> to <a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2000\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 128\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 320\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='320' max='320' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [320/320 00:17, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.692300</td>\n",
       "      <td>0.690286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.689800</td>\n",
       "      <td>0.687695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.687400</td>\n",
       "      <td>0.684954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.684600</td>\n",
       "      <td>0.682012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.681900</td>\n",
       "      <td>0.678741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.678600</td>\n",
       "      <td>0.675167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.675100</td>\n",
       "      <td>0.671259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.671600</td>\n",
       "      <td>0.667084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.667700</td>\n",
       "      <td>0.662694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.663500</td>\n",
       "      <td>0.658195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.659100</td>\n",
       "      <td>0.653708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.655200</td>\n",
       "      <td>0.649308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.650900</td>\n",
       "      <td>0.645133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.647500</td>\n",
       "      <td>0.641319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.643100</td>\n",
       "      <td>0.637963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.640900</td>\n",
       "      <td>0.635105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.638800</td>\n",
       "      <td>0.632885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.636100</td>\n",
       "      <td>0.631271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.636100</td>\n",
       "      <td>0.630303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.635200</td>\n",
       "      <td>0.629970</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_15/checkpoint-16\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_15/checkpoint-16/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_15/checkpoint-16/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_15/checkpoint-16/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_15/checkpoint-16/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_15/checkpoint-32\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_15/checkpoint-32/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_15/checkpoint-32/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_15/checkpoint-32/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_15/checkpoint-32/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_15/checkpoint-48\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_15/checkpoint-48/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_15/checkpoint-48/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_15/checkpoint-48/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_15/checkpoint-48/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_15/checkpoint-16] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_15/checkpoint-64\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_15/checkpoint-64/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_15/checkpoint-64/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_15/checkpoint-64/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_15/checkpoint-64/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_15/checkpoint-32] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_15/checkpoint-80\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_15/checkpoint-80/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_15/checkpoint-80/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_15/checkpoint-80/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_15/checkpoint-80/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_15/checkpoint-48] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_15/checkpoint-96\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_15/checkpoint-96/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_15/checkpoint-96/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_15/checkpoint-96/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_15/checkpoint-96/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_15/checkpoint-64] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_15/checkpoint-112\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_15/checkpoint-112/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_15/checkpoint-112/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_15/checkpoint-112/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_15/checkpoint-112/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_15/checkpoint-80] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_15/checkpoint-128\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_15/checkpoint-128/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_15/checkpoint-128/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_15/checkpoint-128/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_15/checkpoint-128/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_15/checkpoint-96] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_15/checkpoint-144\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_15/checkpoint-144/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_15/checkpoint-144/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_15/checkpoint-144/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_15/checkpoint-144/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_15/checkpoint-112] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_15/checkpoint-160\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_15/checkpoint-160/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_15/checkpoint-160/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_15/checkpoint-160/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_15/checkpoint-160/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_15/checkpoint-128] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_15/checkpoint-176\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_15/checkpoint-176/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_15/checkpoint-176/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_15/checkpoint-176/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_15/checkpoint-176/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_15/checkpoint-144] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_15/checkpoint-192\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_15/checkpoint-192/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_15/checkpoint-192/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_15/checkpoint-192/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_15/checkpoint-192/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_15/checkpoint-160] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_15/checkpoint-208\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_15/checkpoint-208/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_15/checkpoint-208/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_15/checkpoint-208/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_15/checkpoint-208/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_15/checkpoint-176] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_15/checkpoint-224\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_15/checkpoint-224/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_15/checkpoint-224/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_15/checkpoint-224/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_15/checkpoint-224/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_15/checkpoint-192] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_15/checkpoint-240\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_15/checkpoint-240/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_15/checkpoint-240/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_15/checkpoint-240/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_15/checkpoint-240/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_15/checkpoint-208] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_15/checkpoint-256\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_15/checkpoint-256/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_15/checkpoint-256/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_15/checkpoint-256/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_15/checkpoint-256/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_15/checkpoint-224] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_15/checkpoint-272\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_15/checkpoint-272/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_15/checkpoint-272/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_15/checkpoint-272/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_15/checkpoint-272/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_15/checkpoint-240] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_15/checkpoint-288\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_15/checkpoint-288/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_15/checkpoint-288/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_15/checkpoint-288/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_15/checkpoint-288/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_15/checkpoint-256] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_15/checkpoint-304\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_15/checkpoint-304/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_15/checkpoint-304/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_15/checkpoint-304/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_15/checkpoint-304/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_15/checkpoint-272] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_15/checkpoint-320\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_15/checkpoint-320/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_15/checkpoint-320/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_15/checkpoint-320/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_15/checkpoint-320/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_15/checkpoint-288] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>██▇▇▇▆▆▅▅▄▄▃▃▂▂▂▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>█▃▆▇▆▃▆▄▆▅▁▆▇▇▂▅▁▆▇▆</td></tr><tr><td>eval/samples_per_second</td><td>▁▆▃▂▃▆▂▅▂▄▇▂▂▂▆▄█▃▂▂</td></tr><tr><td>eval/steps_per_second</td><td>▁▆▃▂▃▆▂▅▂▄▇▂▂▂▆▄█▃▂▂</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇█████</td></tr><tr><td>train/learning_rate</td><td>██▇▇▇▆▆▅▅▅▄▄▄▃▃▂▂▂▁▁</td></tr><tr><td>train/loss</td><td>██▇▇▇▆▆▅▅▄▄▃▃▃▂▂▁▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.62997</td></tr><tr><td>eval/runtime</td><td>0.0669</td></tr><tr><td>eval/samples_per_second</td><td>2990.378</td></tr><tr><td>eval/steps_per_second</td><td>29.904</td></tr><tr><td>train/epoch</td><td>20.0</td></tr><tr><td>train/global_step</td><td>320</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.6352</td></tr><tr><td>train/total_flos</td><td>212168160000.0</td></tr><tr><td>train/train_loss</td><td>0.66176</td></tr><tr><td>train/train_runtime</td><td>17.5642</td></tr><tr><td>train/train_samples_per_second</td><td>2277.359</td></tr><tr><td>train/train_steps_per_second</td><td>18.219</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">pos syn style 15</strong>: <a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/lt8s9djm\" target=\"_blank\">https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/lt8s9djm</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/scratch/data_jz17d/wandb_tmp/wandb/run-20221114_112650-lt8s9djm/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aedd96e8a3f54726ab2f87405d093102",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a41e49121d2b4df2b93e3de759da7cc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"/scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 32,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 128,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 128,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 54\n",
      "}\n",
      "\n",
      "loading weights file /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/ and are newly initialized: ['bert.pooler.dense.weight', 'classifier.weight', 'bert.pooler.dense.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch/data_jz17d/wandb_tmp/wandb/run-20221114_112717-1umpfaq8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/1umpfaq8\" target=\"_blank\">pos syn style 16</a></strong> to <a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2000\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 128\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 320\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='320' max='320' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [320/320 00:25, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.692600</td>\n",
       "      <td>0.691461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.691600</td>\n",
       "      <td>0.690318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.690900</td>\n",
       "      <td>0.688963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.689800</td>\n",
       "      <td>0.687388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.688600</td>\n",
       "      <td>0.685591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.686800</td>\n",
       "      <td>0.683455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.684800</td>\n",
       "      <td>0.681077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.682500</td>\n",
       "      <td>0.678181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.680300</td>\n",
       "      <td>0.674973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.677900</td>\n",
       "      <td>0.671364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.674800</td>\n",
       "      <td>0.667489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.672200</td>\n",
       "      <td>0.663265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.669800</td>\n",
       "      <td>0.659411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.666800</td>\n",
       "      <td>0.656022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.665700</td>\n",
       "      <td>0.653509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.663500</td>\n",
       "      <td>0.651301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.661500</td>\n",
       "      <td>0.649676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.661000</td>\n",
       "      <td>0.648539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.660200</td>\n",
       "      <td>0.647895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.659600</td>\n",
       "      <td>0.647669</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_16/checkpoint-16\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_16/checkpoint-16/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_16/checkpoint-16/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_16/checkpoint-16/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_16/checkpoint-16/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_16/checkpoint-32\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_16/checkpoint-32/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_16/checkpoint-32/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_16/checkpoint-32/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_16/checkpoint-32/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_16/checkpoint-48\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_16/checkpoint-48/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_16/checkpoint-48/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_16/checkpoint-48/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_16/checkpoint-48/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_16/checkpoint-16] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_16/checkpoint-64\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_16/checkpoint-64/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_16/checkpoint-64/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_16/checkpoint-64/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_16/checkpoint-64/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_16/checkpoint-32] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_16/checkpoint-80\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_16/checkpoint-80/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_16/checkpoint-80/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_16/checkpoint-80/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_16/checkpoint-80/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_16/checkpoint-48] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_16/checkpoint-96\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_16/checkpoint-96/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_16/checkpoint-96/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_16/checkpoint-96/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_16/checkpoint-96/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_16/checkpoint-64] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_16/checkpoint-112\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_16/checkpoint-112/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_16/checkpoint-112/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_16/checkpoint-112/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_16/checkpoint-112/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_16/checkpoint-80] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_16/checkpoint-128\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_16/checkpoint-128/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_16/checkpoint-128/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_16/checkpoint-128/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_16/checkpoint-128/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_16/checkpoint-96] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_16/checkpoint-144\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_16/checkpoint-144/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_16/checkpoint-144/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_16/checkpoint-144/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_16/checkpoint-144/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_16/checkpoint-112] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_16/checkpoint-160\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_16/checkpoint-160/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_16/checkpoint-160/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_16/checkpoint-160/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_16/checkpoint-160/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_16/checkpoint-128] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_16/checkpoint-176\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_16/checkpoint-176/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_16/checkpoint-176/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_16/checkpoint-176/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_16/checkpoint-176/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_16/checkpoint-144] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_16/checkpoint-192\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_16/checkpoint-192/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_16/checkpoint-192/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_16/checkpoint-192/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_16/checkpoint-192/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_16/checkpoint-160] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_16/checkpoint-208\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_16/checkpoint-208/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_16/checkpoint-208/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_16/checkpoint-208/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_16/checkpoint-208/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_16/checkpoint-176] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_16/checkpoint-224\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_16/checkpoint-224/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_16/checkpoint-224/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_16/checkpoint-224/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_16/checkpoint-224/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_16/checkpoint-192] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_16/checkpoint-240\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_16/checkpoint-240/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_16/checkpoint-240/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_16/checkpoint-240/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_16/checkpoint-240/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_16/checkpoint-208] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_16/checkpoint-256\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_16/checkpoint-256/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_16/checkpoint-256/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_16/checkpoint-256/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_16/checkpoint-256/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_16/checkpoint-224] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_16/checkpoint-272\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_16/checkpoint-272/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_16/checkpoint-272/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_16/checkpoint-272/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_16/checkpoint-272/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_16/checkpoint-240] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_16/checkpoint-288\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_16/checkpoint-288/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_16/checkpoint-288/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_16/checkpoint-288/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_16/checkpoint-288/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_16/checkpoint-256] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_16/checkpoint-304\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_16/checkpoint-304/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_16/checkpoint-304/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_16/checkpoint-304/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_16/checkpoint-304/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_16/checkpoint-272] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_16/checkpoint-320\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_16/checkpoint-320/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_16/checkpoint-320/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_16/checkpoint-320/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_16/checkpoint-320/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_16/checkpoint-288] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>███▇▇▇▆▆▅▅▄▃▃▂▂▂▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▄▄▁▂▃▄▄▂▄▄▄▄▃▅█▄▄▃▄▄</td></tr><tr><td>eval/samples_per_second</td><td>▅▄█▇▅▄▄▇▅▅▄▅▅▄▁▅▄▅▄▅</td></tr><tr><td>eval/steps_per_second</td><td>▅▄█▇▅▄▄▇▅▅▄▅▅▄▁▅▄▅▄▅</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇█████</td></tr><tr><td>train/learning_rate</td><td>██▇▇▇▆▆▅▅▅▄▄▄▃▃▂▂▂▁▁</td></tr><tr><td>train/loss</td><td>███▇▇▇▆▆▅▅▄▄▃▃▂▂▁▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.64767</td></tr><tr><td>eval/runtime</td><td>0.0654</td></tr><tr><td>eval/samples_per_second</td><td>3059.202</td></tr><tr><td>eval/steps_per_second</td><td>30.592</td></tr><tr><td>train/epoch</td><td>20.0</td></tr><tr><td>train/global_step</td><td>320</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.6596</td></tr><tr><td>train/total_flos</td><td>212168160000.0</td></tr><tr><td>train/train_loss</td><td>0.67605</td></tr><tr><td>train/train_runtime</td><td>25.4174</td></tr><tr><td>train/train_samples_per_second</td><td>1573.725</td></tr><tr><td>train/train_steps_per_second</td><td>12.59</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">pos syn style 16</strong>: <a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/1umpfaq8\" target=\"_blank\">https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/1umpfaq8</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/scratch/data_jz17d/wandb_tmp/wandb/run-20221114_112717-1umpfaq8/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1228789c12946d8b5dc4522453fd9fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90c028d789aa40c78c0c35c3205fb7c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"/scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 32,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 128,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 128,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 54\n",
      "}\n",
      "\n",
      "loading weights file /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/ and are newly initialized: ['bert.pooler.dense.weight', 'classifier.weight', 'bert.pooler.dense.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch/data_jz17d/wandb_tmp/wandb/run-20221114_112752-q5dexi8c</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/q5dexi8c\" target=\"_blank\">pos syn style 17</a></strong> to <a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2000\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 128\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 320\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='320' max='320' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [320/320 00:15, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.692900</td>\n",
       "      <td>0.692532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.692700</td>\n",
       "      <td>0.692349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.692600</td>\n",
       "      <td>0.692165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.692500</td>\n",
       "      <td>0.691996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.692600</td>\n",
       "      <td>0.691842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.692300</td>\n",
       "      <td>0.691691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.692300</td>\n",
       "      <td>0.691564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.692100</td>\n",
       "      <td>0.691435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.692100</td>\n",
       "      <td>0.691314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.692000</td>\n",
       "      <td>0.691213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.691900</td>\n",
       "      <td>0.691121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.691900</td>\n",
       "      <td>0.691035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.691800</td>\n",
       "      <td>0.690956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.691700</td>\n",
       "      <td>0.690889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.691700</td>\n",
       "      <td>0.690836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.691700</td>\n",
       "      <td>0.690784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.691800</td>\n",
       "      <td>0.690750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.691600</td>\n",
       "      <td>0.690723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.691700</td>\n",
       "      <td>0.690710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.691700</td>\n",
       "      <td>0.690705</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_17/checkpoint-16\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_17/checkpoint-16/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_17/checkpoint-16/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_17/checkpoint-16/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_17/checkpoint-16/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_17/checkpoint-32\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_17/checkpoint-32/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_17/checkpoint-32/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_17/checkpoint-32/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_17/checkpoint-32/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_17/checkpoint-48\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_17/checkpoint-48/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_17/checkpoint-48/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_17/checkpoint-48/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_17/checkpoint-48/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_17/checkpoint-16] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_17/checkpoint-64\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_17/checkpoint-64/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_17/checkpoint-64/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_17/checkpoint-64/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_17/checkpoint-64/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_17/checkpoint-32] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_17/checkpoint-80\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_17/checkpoint-80/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_17/checkpoint-80/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_17/checkpoint-80/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_17/checkpoint-80/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_17/checkpoint-48] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_17/checkpoint-96\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_17/checkpoint-96/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_17/checkpoint-96/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_17/checkpoint-96/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_17/checkpoint-96/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_17/checkpoint-64] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_17/checkpoint-112\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_17/checkpoint-112/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_17/checkpoint-112/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_17/checkpoint-112/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_17/checkpoint-112/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_17/checkpoint-80] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_17/checkpoint-128\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_17/checkpoint-128/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_17/checkpoint-128/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_17/checkpoint-128/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_17/checkpoint-128/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_17/checkpoint-96] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_17/checkpoint-144\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_17/checkpoint-144/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_17/checkpoint-144/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_17/checkpoint-144/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_17/checkpoint-144/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_17/checkpoint-112] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_17/checkpoint-160\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_17/checkpoint-160/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_17/checkpoint-160/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_17/checkpoint-160/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_17/checkpoint-160/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_17/checkpoint-128] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_17/checkpoint-176\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_17/checkpoint-176/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_17/checkpoint-176/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_17/checkpoint-176/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_17/checkpoint-176/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_17/checkpoint-144] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_17/checkpoint-192\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_17/checkpoint-192/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_17/checkpoint-192/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_17/checkpoint-192/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_17/checkpoint-192/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_17/checkpoint-160] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_17/checkpoint-208\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_17/checkpoint-208/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_17/checkpoint-208/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_17/checkpoint-208/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_17/checkpoint-208/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_17/checkpoint-176] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_17/checkpoint-224\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_17/checkpoint-224/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_17/checkpoint-224/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_17/checkpoint-224/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_17/checkpoint-224/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_17/checkpoint-192] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_17/checkpoint-240\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_17/checkpoint-240/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_17/checkpoint-240/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_17/checkpoint-240/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_17/checkpoint-240/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_17/checkpoint-208] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_17/checkpoint-256\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_17/checkpoint-256/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_17/checkpoint-256/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_17/checkpoint-256/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_17/checkpoint-256/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_17/checkpoint-224] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_17/checkpoint-272\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_17/checkpoint-272/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_17/checkpoint-272/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_17/checkpoint-272/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_17/checkpoint-272/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_17/checkpoint-240] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_17/checkpoint-288\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_17/checkpoint-288/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_17/checkpoint-288/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_17/checkpoint-288/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_17/checkpoint-288/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_17/checkpoint-256] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_17/checkpoint-304\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_17/checkpoint-304/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_17/checkpoint-304/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_17/checkpoint-304/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_17/checkpoint-304/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_17/checkpoint-272] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_17/checkpoint-320\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_17/checkpoint-320/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_17/checkpoint-320/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_17/checkpoint-320/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_17/checkpoint-320/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_17/checkpoint-288] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▇▇▆▅▅▄▄▃▃▃▂▂▂▂▁▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▆▅▅▇▆▇▆▆▇▅██▁▁▇▇▅▃▁▂</td></tr><tr><td>eval/samples_per_second</td><td>▃▄▄▂▂▂▃▂▂▃▁▁██▂▂▄▆█▇</td></tr><tr><td>eval/steps_per_second</td><td>▃▄▄▂▂▂▃▂▂▃▁▁██▂▂▄▆█▇</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇█████</td></tr><tr><td>train/learning_rate</td><td>██▇▇▇▆▆▅▅▅▄▄▄▃▃▂▂▂▁▁</td></tr><tr><td>train/loss</td><td>█▇▆▆▆▅▅▄▄▃▃▃▂▂▂▂▂▁▂▂</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.6907</td></tr><tr><td>eval/runtime</td><td>0.0598</td></tr><tr><td>eval/samples_per_second</td><td>3343.62</td></tr><tr><td>eval/steps_per_second</td><td>33.436</td></tr><tr><td>train/epoch</td><td>20.0</td></tr><tr><td>train/global_step</td><td>320</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.6917</td></tr><tr><td>train/total_flos</td><td>212168160000.0</td></tr><tr><td>train/train_loss</td><td>0.69209</td></tr><tr><td>train/train_runtime</td><td>15.3475</td></tr><tr><td>train/train_samples_per_second</td><td>2606.281</td></tr><tr><td>train/train_steps_per_second</td><td>20.85</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">pos syn style 17</strong>: <a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/q5dexi8c\" target=\"_blank\">https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/q5dexi8c</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/scratch/data_jz17d/wandb_tmp/wandb/run-20221114_112752-q5dexi8c/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90f80e80a3cb4b89b7ad85da43a1a1f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d895a19e0bb64d0e85ee1dfc71c48fd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"/scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 32,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 128,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 128,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 54\n",
      "}\n",
      "\n",
      "loading weights file /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/ and are newly initialized: ['bert.pooler.dense.weight', 'classifier.weight', 'bert.pooler.dense.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch/data_jz17d/wandb_tmp/wandb/run-20221114_112817-j7d85wfr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/j7d85wfr\" target=\"_blank\">pos syn style 18</a></strong> to <a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2000\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 128\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 320\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='320' max='320' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [320/320 00:19, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.692600</td>\n",
       "      <td>0.691696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.691900</td>\n",
       "      <td>0.690767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.691300</td>\n",
       "      <td>0.689722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.690500</td>\n",
       "      <td>0.688607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.689900</td>\n",
       "      <td>0.687405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.688600</td>\n",
       "      <td>0.686051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.687600</td>\n",
       "      <td>0.684636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.686200</td>\n",
       "      <td>0.682993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.685000</td>\n",
       "      <td>0.681276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.683900</td>\n",
       "      <td>0.679550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.682400</td>\n",
       "      <td>0.677792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.680700</td>\n",
       "      <td>0.676010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.679600</td>\n",
       "      <td>0.674352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.678000</td>\n",
       "      <td>0.672831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.677000</td>\n",
       "      <td>0.671515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.676000</td>\n",
       "      <td>0.670325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.675500</td>\n",
       "      <td>0.669446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.674100</td>\n",
       "      <td>0.668768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.674000</td>\n",
       "      <td>0.668385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.673800</td>\n",
       "      <td>0.668249</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_18/checkpoint-16\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_18/checkpoint-16/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_18/checkpoint-16/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_18/checkpoint-16/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_18/checkpoint-16/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_18/checkpoint-32\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_18/checkpoint-32/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_18/checkpoint-32/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_18/checkpoint-32/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_18/checkpoint-32/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_18/checkpoint-48\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_18/checkpoint-48/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_18/checkpoint-48/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_18/checkpoint-48/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_18/checkpoint-48/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_18/checkpoint-16] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_18/checkpoint-64\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_18/checkpoint-64/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_18/checkpoint-64/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_18/checkpoint-64/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_18/checkpoint-64/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_18/checkpoint-32] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_18/checkpoint-80\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_18/checkpoint-80/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_18/checkpoint-80/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_18/checkpoint-80/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_18/checkpoint-80/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_18/checkpoint-48] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_18/checkpoint-96\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_18/checkpoint-96/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_18/checkpoint-96/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_18/checkpoint-96/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_18/checkpoint-96/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_18/checkpoint-64] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_18/checkpoint-112\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_18/checkpoint-112/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_18/checkpoint-112/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_18/checkpoint-112/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_18/checkpoint-112/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_18/checkpoint-80] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_18/checkpoint-128\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_18/checkpoint-128/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_18/checkpoint-128/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_18/checkpoint-128/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_18/checkpoint-128/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_18/checkpoint-96] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_18/checkpoint-144\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_18/checkpoint-144/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_18/checkpoint-144/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_18/checkpoint-144/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_18/checkpoint-144/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_18/checkpoint-112] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_18/checkpoint-160\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_18/checkpoint-160/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_18/checkpoint-160/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_18/checkpoint-160/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_18/checkpoint-160/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_18/checkpoint-128] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_18/checkpoint-176\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_18/checkpoint-176/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_18/checkpoint-176/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_18/checkpoint-176/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_18/checkpoint-176/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_18/checkpoint-144] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_18/checkpoint-192\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_18/checkpoint-192/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_18/checkpoint-192/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_18/checkpoint-192/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_18/checkpoint-192/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_18/checkpoint-160] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_18/checkpoint-208\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_18/checkpoint-208/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_18/checkpoint-208/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_18/checkpoint-208/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_18/checkpoint-208/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_18/checkpoint-176] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_18/checkpoint-224\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_18/checkpoint-224/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_18/checkpoint-224/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_18/checkpoint-224/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_18/checkpoint-224/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_18/checkpoint-192] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_18/checkpoint-240\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_18/checkpoint-240/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_18/checkpoint-240/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_18/checkpoint-240/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_18/checkpoint-240/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_18/checkpoint-208] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_18/checkpoint-256\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_18/checkpoint-256/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_18/checkpoint-256/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_18/checkpoint-256/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_18/checkpoint-256/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_18/checkpoint-224] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_18/checkpoint-272\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_18/checkpoint-272/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_18/checkpoint-272/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_18/checkpoint-272/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_18/checkpoint-272/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_18/checkpoint-240] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_18/checkpoint-288\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_18/checkpoint-288/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_18/checkpoint-288/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_18/checkpoint-288/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_18/checkpoint-288/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_18/checkpoint-256] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_18/checkpoint-304\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_18/checkpoint-304/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_18/checkpoint-304/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_18/checkpoint-304/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_18/checkpoint-304/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_18/checkpoint-272] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_18/checkpoint-320\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_18/checkpoint-320/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_18/checkpoint-320/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_18/checkpoint-320/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_18/checkpoint-320/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_18/checkpoint-288] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>██▇▇▇▆▆▅▅▄▄▃▃▂▂▂▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▆▆▅▆▆▆█▆▇▆▆▆██▆▆▁▆█▆</td></tr><tr><td>eval/samples_per_second</td><td>▂▃▃▃▃▃▁▃▂▃▂▂▁▁▂▃█▃▁▃</td></tr><tr><td>eval/steps_per_second</td><td>▂▃▃▃▃▃▁▃▂▃▂▂▁▁▂▃█▃▁▃</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇█████</td></tr><tr><td>train/learning_rate</td><td>██▇▇▇▆▆▅▅▅▄▄▄▃▃▂▂▂▁▁</td></tr><tr><td>train/loss</td><td>███▇▇▇▆▆▅▅▄▄▃▃▂▂▂▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.66825</td></tr><tr><td>eval/runtime</td><td>0.066</td></tr><tr><td>eval/samples_per_second</td><td>3032.045</td></tr><tr><td>eval/steps_per_second</td><td>30.32</td></tr><tr><td>train/epoch</td><td>20.0</td></tr><tr><td>train/global_step</td><td>320</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.6738</td></tr><tr><td>train/total_flos</td><td>212168160000.0</td></tr><tr><td>train/train_loss</td><td>0.68294</td></tr><tr><td>train/train_runtime</td><td>19.9542</td></tr><tr><td>train/train_samples_per_second</td><td>2004.591</td></tr><tr><td>train/train_steps_per_second</td><td>16.037</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">pos syn style 18</strong>: <a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/j7d85wfr\" target=\"_blank\">https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/j7d85wfr</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/scratch/data_jz17d/wandb_tmp/wandb/run-20221114_112817-j7d85wfr/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c381124a07a0401bbbebaaeef9c8d3fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bb42c4163cb4a82875c63ffb5bce72c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"/scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 32,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 128,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 128,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 54\n",
      "}\n",
      "\n",
      "loading weights file /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/ and are newly initialized: ['bert.pooler.dense.weight', 'classifier.weight', 'bert.pooler.dense.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch/data_jz17d/wandb_tmp/wandb/run-20221114_112846-duvawf97</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/duvawf97\" target=\"_blank\">pos syn style 19</a></strong> to <a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2000\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 128\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 320\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='320' max='320' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [320/320 00:18, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.692700</td>\n",
       "      <td>0.691849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.692000</td>\n",
       "      <td>0.691079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.691600</td>\n",
       "      <td>0.690262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.691000</td>\n",
       "      <td>0.689442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.690600</td>\n",
       "      <td>0.688610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.689700</td>\n",
       "      <td>0.687710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.689100</td>\n",
       "      <td>0.686840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.688300</td>\n",
       "      <td>0.685857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.687700</td>\n",
       "      <td>0.684835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.687200</td>\n",
       "      <td>0.683876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.686300</td>\n",
       "      <td>0.682921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.685400</td>\n",
       "      <td>0.681962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.684900</td>\n",
       "      <td>0.681054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.684000</td>\n",
       "      <td>0.680233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.683400</td>\n",
       "      <td>0.679523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.683200</td>\n",
       "      <td>0.678875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.683000</td>\n",
       "      <td>0.678415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.682100</td>\n",
       "      <td>0.678051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.682300</td>\n",
       "      <td>0.677850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.681900</td>\n",
       "      <td>0.677781</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_19/checkpoint-16\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_19/checkpoint-16/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_19/checkpoint-16/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_19/checkpoint-16/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_19/checkpoint-16/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_19/checkpoint-32\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_19/checkpoint-32/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_19/checkpoint-32/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_19/checkpoint-32/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_19/checkpoint-32/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_19/checkpoint-48\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_19/checkpoint-48/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_19/checkpoint-48/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_19/checkpoint-48/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_19/checkpoint-48/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_19/checkpoint-16] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_19/checkpoint-64\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_19/checkpoint-64/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_19/checkpoint-64/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_19/checkpoint-64/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_19/checkpoint-64/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_19/checkpoint-32] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_19/checkpoint-80\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_19/checkpoint-80/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_19/checkpoint-80/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_19/checkpoint-80/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_19/checkpoint-80/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_19/checkpoint-48] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_19/checkpoint-96\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_19/checkpoint-96/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_19/checkpoint-96/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_19/checkpoint-96/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_19/checkpoint-96/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_19/checkpoint-64] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_19/checkpoint-112\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_19/checkpoint-112/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_19/checkpoint-112/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_19/checkpoint-112/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_19/checkpoint-112/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_19/checkpoint-80] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_19/checkpoint-128\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_19/checkpoint-128/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_19/checkpoint-128/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_19/checkpoint-128/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_19/checkpoint-128/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_19/checkpoint-96] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_19/checkpoint-144\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_19/checkpoint-144/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_19/checkpoint-144/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_19/checkpoint-144/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_19/checkpoint-144/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_19/checkpoint-112] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_19/checkpoint-160\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_19/checkpoint-160/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_19/checkpoint-160/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_19/checkpoint-160/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_19/checkpoint-160/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_19/checkpoint-128] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_19/checkpoint-176\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_19/checkpoint-176/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_19/checkpoint-176/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_19/checkpoint-176/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_19/checkpoint-176/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_19/checkpoint-144] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_19/checkpoint-192\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_19/checkpoint-192/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_19/checkpoint-192/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_19/checkpoint-192/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_19/checkpoint-192/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_19/checkpoint-160] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_19/checkpoint-208\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_19/checkpoint-208/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_19/checkpoint-208/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_19/checkpoint-208/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_19/checkpoint-208/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_19/checkpoint-176] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_19/checkpoint-224\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_19/checkpoint-224/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_19/checkpoint-224/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_19/checkpoint-224/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_19/checkpoint-224/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_19/checkpoint-192] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_19/checkpoint-240\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_19/checkpoint-240/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_19/checkpoint-240/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_19/checkpoint-240/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_19/checkpoint-240/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_19/checkpoint-208] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_19/checkpoint-256\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_19/checkpoint-256/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_19/checkpoint-256/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_19/checkpoint-256/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_19/checkpoint-256/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_19/checkpoint-224] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_19/checkpoint-272\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_19/checkpoint-272/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_19/checkpoint-272/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_19/checkpoint-272/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_19/checkpoint-272/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_19/checkpoint-240] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_19/checkpoint-288\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_19/checkpoint-288/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_19/checkpoint-288/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_19/checkpoint-288/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_19/checkpoint-288/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_19/checkpoint-256] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_19/checkpoint-304\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_19/checkpoint-304/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_19/checkpoint-304/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_19/checkpoint-304/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_19/checkpoint-304/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_19/checkpoint-272] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_19/checkpoint-320\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_19/checkpoint-320/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_19/checkpoint-320/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_19/checkpoint-320/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_19/checkpoint-320/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_19/checkpoint-288] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>██▇▇▆▆▆▅▅▄▄▃▃▂▂▂▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▁▃▄▅▆█▅▆▄▁▅▆▆▆▅▇▄▅▂█</td></tr><tr><td>eval/samples_per_second</td><td>█▆▅▄▃▁▃▂▅▇▄▃▃▃▄▂▅▄▇▁</td></tr><tr><td>eval/steps_per_second</td><td>█▆▅▄▃▁▃▂▅▇▄▃▃▃▄▂▅▄▇▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇█████</td></tr><tr><td>train/learning_rate</td><td>██▇▇▇▆▆▅▅▅▄▄▄▃▃▂▂▂▁▁</td></tr><tr><td>train/loss</td><td>██▇▇▇▆▆▅▅▄▄▃▃▂▂▂▂▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.67778</td></tr><tr><td>eval/runtime</td><td>0.0683</td></tr><tr><td>eval/samples_per_second</td><td>2929.239</td></tr><tr><td>eval/steps_per_second</td><td>29.292</td></tr><tr><td>train/epoch</td><td>20.0</td></tr><tr><td>train/global_step</td><td>320</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.6819</td></tr><tr><td>train/total_flos</td><td>212168160000.0</td></tr><tr><td>train/train_loss</td><td>0.68683</td></tr><tr><td>train/train_runtime</td><td>18.6827</td></tr><tr><td>train/train_samples_per_second</td><td>2141.022</td></tr><tr><td>train/train_steps_per_second</td><td>17.128</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">pos syn style 19</strong>: <a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/duvawf97\" target=\"_blank\">https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/duvawf97</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/scratch/data_jz17d/wandb_tmp/wandb/run-20221114_112846-duvawf97/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b1e0f74f1294b64b3f3172f1e1b7f44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2ce3b91da6b4123b18ed5fd01b89d25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"/scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 32,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 128,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 128,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 54\n",
      "}\n",
      "\n",
      "loading weights file /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/ and are newly initialized: ['bert.pooler.dense.weight', 'classifier.weight', 'bert.pooler.dense.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch/data_jz17d/wandb_tmp/wandb/run-20221114_112913-3tcjzyr8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/3tcjzyr8\" target=\"_blank\">pos syn style 20</a></strong> to <a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2000\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 128\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 320\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='320' max='320' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [320/320 00:24, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.692200</td>\n",
       "      <td>0.690217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.690300</td>\n",
       "      <td>0.688100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.688300</td>\n",
       "      <td>0.685432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.685500</td>\n",
       "      <td>0.681982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.682000</td>\n",
       "      <td>0.677450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.677300</td>\n",
       "      <td>0.671771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.671600</td>\n",
       "      <td>0.664850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.665300</td>\n",
       "      <td>0.656767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.658700</td>\n",
       "      <td>0.648127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.651700</td>\n",
       "      <td>0.640059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.644200</td>\n",
       "      <td>0.632430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.638200</td>\n",
       "      <td>0.625523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.632300</td>\n",
       "      <td>0.619257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.625900</td>\n",
       "      <td>0.613778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.623300</td>\n",
       "      <td>0.609166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.617400</td>\n",
       "      <td>0.605371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.615200</td>\n",
       "      <td>0.602444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.612700</td>\n",
       "      <td>0.600339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.611800</td>\n",
       "      <td>0.599104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.610300</td>\n",
       "      <td>0.598688</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_20/checkpoint-16\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_20/checkpoint-16/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_20/checkpoint-16/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_20/checkpoint-16/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_20/checkpoint-16/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_20/checkpoint-32\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_20/checkpoint-32/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_20/checkpoint-32/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_20/checkpoint-32/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_20/checkpoint-32/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_20/checkpoint-48\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_20/checkpoint-48/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_20/checkpoint-48/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_20/checkpoint-48/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_20/checkpoint-48/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_20/checkpoint-16] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_20/checkpoint-64\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_20/checkpoint-64/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_20/checkpoint-64/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_20/checkpoint-64/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_20/checkpoint-64/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_20/checkpoint-32] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_20/checkpoint-80\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_20/checkpoint-80/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_20/checkpoint-80/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_20/checkpoint-80/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_20/checkpoint-80/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_20/checkpoint-48] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_20/checkpoint-96\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_20/checkpoint-96/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_20/checkpoint-96/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_20/checkpoint-96/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_20/checkpoint-96/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_20/checkpoint-64] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_20/checkpoint-112\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_20/checkpoint-112/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_20/checkpoint-112/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_20/checkpoint-112/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_20/checkpoint-112/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_20/checkpoint-80] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_20/checkpoint-128\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_20/checkpoint-128/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_20/checkpoint-128/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_20/checkpoint-128/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_20/checkpoint-128/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_20/checkpoint-96] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_20/checkpoint-144\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_20/checkpoint-144/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_20/checkpoint-144/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_20/checkpoint-144/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_20/checkpoint-144/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_20/checkpoint-112] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_20/checkpoint-160\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_20/checkpoint-160/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_20/checkpoint-160/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_20/checkpoint-160/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_20/checkpoint-160/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_20/checkpoint-128] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_20/checkpoint-176\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_20/checkpoint-176/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_20/checkpoint-176/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_20/checkpoint-176/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_20/checkpoint-176/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_20/checkpoint-144] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_20/checkpoint-192\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_20/checkpoint-192/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_20/checkpoint-192/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_20/checkpoint-192/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_20/checkpoint-192/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_20/checkpoint-160] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_20/checkpoint-208\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_20/checkpoint-208/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_20/checkpoint-208/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_20/checkpoint-208/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_20/checkpoint-208/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_20/checkpoint-176] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_20/checkpoint-224\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_20/checkpoint-224/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_20/checkpoint-224/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_20/checkpoint-224/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_20/checkpoint-224/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_20/checkpoint-192] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_20/checkpoint-240\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_20/checkpoint-240/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_20/checkpoint-240/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_20/checkpoint-240/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_20/checkpoint-240/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_20/checkpoint-208] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_20/checkpoint-256\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_20/checkpoint-256/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_20/checkpoint-256/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_20/checkpoint-256/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_20/checkpoint-256/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_20/checkpoint-224] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_20/checkpoint-272\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_20/checkpoint-272/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_20/checkpoint-272/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_20/checkpoint-272/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_20/checkpoint-272/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_20/checkpoint-240] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_20/checkpoint-288\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_20/checkpoint-288/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_20/checkpoint-288/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_20/checkpoint-288/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_20/checkpoint-288/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_20/checkpoint-256] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_20/checkpoint-304\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_20/checkpoint-304/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_20/checkpoint-304/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_20/checkpoint-304/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_20/checkpoint-304/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_20/checkpoint-272] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_20/checkpoint-320\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_20/checkpoint-320/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_20/checkpoint-320/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_20/checkpoint-320/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_20/checkpoint-320/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_20/checkpoint-288] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>███▇▇▇▆▅▅▄▄▃▃▂▂▂▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▅▄▄▄▂▂█▄▄▄▅▄▄▃▅▅▄▆▆▁</td></tr><tr><td>eval/samples_per_second</td><td>▄▄▄▄▇▇▁▄▅▄▄▅▅▅▃▃▅▃▃█</td></tr><tr><td>eval/steps_per_second</td><td>▄▄▄▄▇▇▁▄▅▄▄▅▅▅▃▃▅▃▃█</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇█████</td></tr><tr><td>train/learning_rate</td><td>██▇▇▇▆▆▅▅▅▄▄▄▃▃▂▂▂▁▁</td></tr><tr><td>train/loss</td><td>███▇▇▇▆▆▅▅▄▃▃▂▂▂▁▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.59869</td></tr><tr><td>eval/runtime</td><td>0.0577</td></tr><tr><td>eval/samples_per_second</td><td>3464.463</td></tr><tr><td>eval/steps_per_second</td><td>34.645</td></tr><tr><td>train/epoch</td><td>20.0</td></tr><tr><td>train/global_step</td><td>320</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.6103</td></tr><tr><td>train/total_flos</td><td>212168160000.0</td></tr><tr><td>train/train_loss</td><td>0.64972</td></tr><tr><td>train/train_runtime</td><td>24.6123</td></tr><tr><td>train/train_samples_per_second</td><td>1625.206</td></tr><tr><td>train/train_steps_per_second</td><td>13.002</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">pos syn style 20</strong>: <a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/3tcjzyr8\" target=\"_blank\">https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/3tcjzyr8</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/scratch/data_jz17d/wandb_tmp/wandb/run-20221114_112913-3tcjzyr8/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eece825efcd48db8d0ca2043c9b8261",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61f083c09b3548c28adec44bb4f4266a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"/scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 32,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 128,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 128,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 54\n",
      "}\n",
      "\n",
      "loading weights file /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/ and are newly initialized: ['bert.pooler.dense.weight', 'classifier.weight', 'bert.pooler.dense.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch/data_jz17d/wandb_tmp/wandb/run-20221114_112946-1suhkcij</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/1suhkcij\" target=\"_blank\">pos syn style 21</a></strong> to <a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2000\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 128\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 320\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='320' max='320' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [320/320 00:15, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.693000</td>\n",
       "      <td>0.691972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.692600</td>\n",
       "      <td>0.691695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.692500</td>\n",
       "      <td>0.691423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.692300</td>\n",
       "      <td>0.691171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.692100</td>\n",
       "      <td>0.690933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.691800</td>\n",
       "      <td>0.690708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.691700</td>\n",
       "      <td>0.690509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.691400</td>\n",
       "      <td>0.690320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.691300</td>\n",
       "      <td>0.690144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.691100</td>\n",
       "      <td>0.689987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.690900</td>\n",
       "      <td>0.689846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.690800</td>\n",
       "      <td>0.689719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.690700</td>\n",
       "      <td>0.689600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.690600</td>\n",
       "      <td>0.689499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.690500</td>\n",
       "      <td>0.689418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.690500</td>\n",
       "      <td>0.689344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.690500</td>\n",
       "      <td>0.689292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.690300</td>\n",
       "      <td>0.689253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.690500</td>\n",
       "      <td>0.689231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.690400</td>\n",
       "      <td>0.689223</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_21/checkpoint-16\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_21/checkpoint-16/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_21/checkpoint-16/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_21/checkpoint-16/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_21/checkpoint-16/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_21/checkpoint-32\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_21/checkpoint-32/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_21/checkpoint-32/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_21/checkpoint-32/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_21/checkpoint-32/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_21/checkpoint-48\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_21/checkpoint-48/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_21/checkpoint-48/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_21/checkpoint-48/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_21/checkpoint-48/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_21/checkpoint-16] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_21/checkpoint-64\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_21/checkpoint-64/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_21/checkpoint-64/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_21/checkpoint-64/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_21/checkpoint-64/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_21/checkpoint-32] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_21/checkpoint-80\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_21/checkpoint-80/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_21/checkpoint-80/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_21/checkpoint-80/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_21/checkpoint-80/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_21/checkpoint-48] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_21/checkpoint-96\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_21/checkpoint-96/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_21/checkpoint-96/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_21/checkpoint-96/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_21/checkpoint-96/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_21/checkpoint-64] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_21/checkpoint-112\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_21/checkpoint-112/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_21/checkpoint-112/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_21/checkpoint-112/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_21/checkpoint-112/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_21/checkpoint-80] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_21/checkpoint-128\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_21/checkpoint-128/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_21/checkpoint-128/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_21/checkpoint-128/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_21/checkpoint-128/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_21/checkpoint-96] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_21/checkpoint-144\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_21/checkpoint-144/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_21/checkpoint-144/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_21/checkpoint-144/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_21/checkpoint-144/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_21/checkpoint-112] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_21/checkpoint-160\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_21/checkpoint-160/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_21/checkpoint-160/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_21/checkpoint-160/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_21/checkpoint-160/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_21/checkpoint-128] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_21/checkpoint-176\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_21/checkpoint-176/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_21/checkpoint-176/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_21/checkpoint-176/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_21/checkpoint-176/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_21/checkpoint-144] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_21/checkpoint-192\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_21/checkpoint-192/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_21/checkpoint-192/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_21/checkpoint-192/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_21/checkpoint-192/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_21/checkpoint-160] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_21/checkpoint-208\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_21/checkpoint-208/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_21/checkpoint-208/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_21/checkpoint-208/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_21/checkpoint-208/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_21/checkpoint-176] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_21/checkpoint-224\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_21/checkpoint-224/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_21/checkpoint-224/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_21/checkpoint-224/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_21/checkpoint-224/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_21/checkpoint-192] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_21/checkpoint-240\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_21/checkpoint-240/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_21/checkpoint-240/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_21/checkpoint-240/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_21/checkpoint-240/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_21/checkpoint-208] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_21/checkpoint-256\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_21/checkpoint-256/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_21/checkpoint-256/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_21/checkpoint-256/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_21/checkpoint-256/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_21/checkpoint-224] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_21/checkpoint-272\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_21/checkpoint-272/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_21/checkpoint-272/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_21/checkpoint-272/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_21/checkpoint-272/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_21/checkpoint-240] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_21/checkpoint-288\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_21/checkpoint-288/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_21/checkpoint-288/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_21/checkpoint-288/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_21/checkpoint-288/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_21/checkpoint-256] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_21/checkpoint-304\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_21/checkpoint-304/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_21/checkpoint-304/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_21/checkpoint-304/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_21/checkpoint-304/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_21/checkpoint-272] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_21/checkpoint-320\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_21/checkpoint-320/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_21/checkpoint-320/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_21/checkpoint-320/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_21/checkpoint-320/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_21/checkpoint-288] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▇▇▆▅▅▄▄▃▃▃▂▂▂▁▁▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▁▃▄▁▃▄▄▄▄█▁▃▄▇▃▁▄▃▃▃</td></tr><tr><td>eval/samples_per_second</td><td>█▆▅█▆▅▅▅▄▁█▆▅▂▆█▅▅▅▆</td></tr><tr><td>eval/steps_per_second</td><td>█▆▅█▆▅▅▅▄▁█▅▅▂▆█▅▅▅▆</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇█████</td></tr><tr><td>train/learning_rate</td><td>██▇▇▇▆▆▅▅▅▄▄▄▃▃▂▂▂▁▁</td></tr><tr><td>train/loss</td><td>█▇▇▆▆▅▅▄▄▃▃▂▂▂▂▂▂▁▂▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.68922</td></tr><tr><td>eval/runtime</td><td>0.0637</td></tr><tr><td>eval/samples_per_second</td><td>3137.57</td></tr><tr><td>eval/steps_per_second</td><td>31.376</td></tr><tr><td>train/epoch</td><td>20.0</td></tr><tr><td>train/global_step</td><td>320</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.6904</td></tr><tr><td>train/total_flos</td><td>212168160000.0</td></tr><tr><td>train/train_loss</td><td>0.69127</td></tr><tr><td>train/train_runtime</td><td>15.9607</td></tr><tr><td>train/train_samples_per_second</td><td>2506.149</td></tr><tr><td>train/train_steps_per_second</td><td>20.049</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">pos syn style 21</strong>: <a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/1suhkcij\" target=\"_blank\">https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/1suhkcij</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/scratch/data_jz17d/wandb_tmp/wandb/run-20221114_112946-1suhkcij/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4458cb628644f3184cd6eba7e9ce0fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d00f8c568224858badd7c9452076e25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"/scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 32,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 128,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 128,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 54\n",
      "}\n",
      "\n",
      "loading weights file /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/ and are newly initialized: ['bert.pooler.dense.weight', 'classifier.weight', 'bert.pooler.dense.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch/data_jz17d/wandb_tmp/wandb/run-20221114_113011-7ype130o</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/7ype130o\" target=\"_blank\">pos syn style 22</a></strong> to <a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2000\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 128\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 320\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='320' max='320' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [320/320 00:20, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.692400</td>\n",
       "      <td>0.690623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.690800</td>\n",
       "      <td>0.688988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.689500</td>\n",
       "      <td>0.687119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.687700</td>\n",
       "      <td>0.684950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.685500</td>\n",
       "      <td>0.682356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.682900</td>\n",
       "      <td>0.679358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>0.676004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.676500</td>\n",
       "      <td>0.672169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.673400</td>\n",
       "      <td>0.668085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.669600</td>\n",
       "      <td>0.663773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.665700</td>\n",
       "      <td>0.659354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.661300</td>\n",
       "      <td>0.654993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.657300</td>\n",
       "      <td>0.650809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.653100</td>\n",
       "      <td>0.646948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.650600</td>\n",
       "      <td>0.643593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.647300</td>\n",
       "      <td>0.640759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.646100</td>\n",
       "      <td>0.638509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.642600</td>\n",
       "      <td>0.636860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.643200</td>\n",
       "      <td>0.635889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.641800</td>\n",
       "      <td>0.635557</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_22/checkpoint-16\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_22/checkpoint-16/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_22/checkpoint-16/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_22/checkpoint-16/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_22/checkpoint-16/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_22/checkpoint-32\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_22/checkpoint-32/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_22/checkpoint-32/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_22/checkpoint-32/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_22/checkpoint-32/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_22/checkpoint-48\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_22/checkpoint-48/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_22/checkpoint-48/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_22/checkpoint-48/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_22/checkpoint-48/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_22/checkpoint-16] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_22/checkpoint-64\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_22/checkpoint-64/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_22/checkpoint-64/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_22/checkpoint-64/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_22/checkpoint-64/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_22/checkpoint-32] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_22/checkpoint-80\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_22/checkpoint-80/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_22/checkpoint-80/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_22/checkpoint-80/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_22/checkpoint-80/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_22/checkpoint-48] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_22/checkpoint-96\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_22/checkpoint-96/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_22/checkpoint-96/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_22/checkpoint-96/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_22/checkpoint-96/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_22/checkpoint-64] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_22/checkpoint-112\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_22/checkpoint-112/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_22/checkpoint-112/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_22/checkpoint-112/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_22/checkpoint-112/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_22/checkpoint-80] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_22/checkpoint-128\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_22/checkpoint-128/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_22/checkpoint-128/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_22/checkpoint-128/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_22/checkpoint-128/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_22/checkpoint-96] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_22/checkpoint-144\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_22/checkpoint-144/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_22/checkpoint-144/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_22/checkpoint-144/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_22/checkpoint-144/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_22/checkpoint-112] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_22/checkpoint-160\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_22/checkpoint-160/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_22/checkpoint-160/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_22/checkpoint-160/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_22/checkpoint-160/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_22/checkpoint-128] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_22/checkpoint-176\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_22/checkpoint-176/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_22/checkpoint-176/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_22/checkpoint-176/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_22/checkpoint-176/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_22/checkpoint-144] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_22/checkpoint-192\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_22/checkpoint-192/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_22/checkpoint-192/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_22/checkpoint-192/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_22/checkpoint-192/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_22/checkpoint-160] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_22/checkpoint-208\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_22/checkpoint-208/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_22/checkpoint-208/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_22/checkpoint-208/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_22/checkpoint-208/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_22/checkpoint-176] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_22/checkpoint-224\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_22/checkpoint-224/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_22/checkpoint-224/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_22/checkpoint-224/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_22/checkpoint-224/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_22/checkpoint-192] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_22/checkpoint-240\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_22/checkpoint-240/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_22/checkpoint-240/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_22/checkpoint-240/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_22/checkpoint-240/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_22/checkpoint-208] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_22/checkpoint-256\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_22/checkpoint-256/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_22/checkpoint-256/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_22/checkpoint-256/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_22/checkpoint-256/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_22/checkpoint-224] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_22/checkpoint-272\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_22/checkpoint-272/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_22/checkpoint-272/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_22/checkpoint-272/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_22/checkpoint-272/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_22/checkpoint-240] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_22/checkpoint-288\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_22/checkpoint-288/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_22/checkpoint-288/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_22/checkpoint-288/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_22/checkpoint-288/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_22/checkpoint-256] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_22/checkpoint-304\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_22/checkpoint-304/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_22/checkpoint-304/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_22/checkpoint-304/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_22/checkpoint-304/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_22/checkpoint-272] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_22/checkpoint-320\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_22/checkpoint-320/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_22/checkpoint-320/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_22/checkpoint-320/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_22/checkpoint-320/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_22/checkpoint-288] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>███▇▇▇▆▆▅▅▄▃▃▂▂▂▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▅▆▂▃▅▅▁▁▂▅▅█▅▂▆▄▆▅▅▆</td></tr><tr><td>eval/samples_per_second</td><td>▃▃▇▅▃▄██▇▃▄▁▃▇▃▅▃▃▄▃</td></tr><tr><td>eval/steps_per_second</td><td>▃▃▇▅▃▄██▇▃▄▁▃▇▃▅▃▃▄▃</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇█████</td></tr><tr><td>train/learning_rate</td><td>██▇▇▇▆▆▅▅▅▄▄▄▃▃▂▂▂▁▁</td></tr><tr><td>train/loss</td><td>███▇▇▇▆▆▅▅▄▄▃▃▂▂▂▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.63556</td></tr><tr><td>eval/runtime</td><td>0.0681</td></tr><tr><td>eval/samples_per_second</td><td>2938.289</td></tr><tr><td>eval/steps_per_second</td><td>29.383</td></tr><tr><td>train/epoch</td><td>20.0</td></tr><tr><td>train/global_step</td><td>320</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.6418</td></tr><tr><td>train/total_flos</td><td>212168160000.0</td></tr><tr><td>train/train_loss</td><td>0.66688</td></tr><tr><td>train/train_runtime</td><td>20.2919</td></tr><tr><td>train/train_samples_per_second</td><td>1971.229</td></tr><tr><td>train/train_steps_per_second</td><td>15.77</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">pos syn style 22</strong>: <a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/7ype130o\" target=\"_blank\">https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/7ype130o</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/scratch/data_jz17d/wandb_tmp/wandb/run-20221114_113011-7ype130o/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b45493cc8d10433cbe6d9b8ebd178f8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32ddcf3cf6d84562a3c3032f489ec269",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"/scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 32,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 128,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 128,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 54\n",
      "}\n",
      "\n",
      "loading weights file /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/ and are newly initialized: ['bert.pooler.dense.weight', 'classifier.weight', 'bert.pooler.dense.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch/data_jz17d/wandb_tmp/wandb/run-20221114_113040-3fzlfs8j</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/3fzlfs8j\" target=\"_blank\">pos syn style 23</a></strong> to <a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2000\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 128\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 320\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='320' max='320' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [320/320 00:17, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.692500</td>\n",
       "      <td>0.690881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.691200</td>\n",
       "      <td>0.689569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.690200</td>\n",
       "      <td>0.688177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.688900</td>\n",
       "      <td>0.686696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.685045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.685900</td>\n",
       "      <td>0.683231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.684300</td>\n",
       "      <td>0.681306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.682300</td>\n",
       "      <td>0.679211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.680800</td>\n",
       "      <td>0.677023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.678800</td>\n",
       "      <td>0.674799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.676700</td>\n",
       "      <td>0.672544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.674400</td>\n",
       "      <td>0.670341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.672400</td>\n",
       "      <td>0.668190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.670300</td>\n",
       "      <td>0.666237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.668600</td>\n",
       "      <td>0.664531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.667500</td>\n",
       "      <td>0.663056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.666900</td>\n",
       "      <td>0.661921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.665100</td>\n",
       "      <td>0.661084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.665400</td>\n",
       "      <td>0.660588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.664600</td>\n",
       "      <td>0.660419</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_23/checkpoint-16\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_23/checkpoint-16/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_23/checkpoint-16/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_23/checkpoint-16/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_23/checkpoint-16/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_23/checkpoint-32\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_23/checkpoint-32/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_23/checkpoint-32/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_23/checkpoint-32/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_23/checkpoint-32/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_23/checkpoint-48\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_23/checkpoint-48/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_23/checkpoint-48/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_23/checkpoint-48/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_23/checkpoint-48/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_23/checkpoint-16] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_23/checkpoint-64\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_23/checkpoint-64/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_23/checkpoint-64/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_23/checkpoint-64/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_23/checkpoint-64/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_23/checkpoint-32] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_23/checkpoint-80\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_23/checkpoint-80/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_23/checkpoint-80/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_23/checkpoint-80/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_23/checkpoint-80/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_23/checkpoint-48] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_23/checkpoint-96\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_23/checkpoint-96/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_23/checkpoint-96/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_23/checkpoint-96/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_23/checkpoint-96/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_23/checkpoint-64] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_23/checkpoint-112\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_23/checkpoint-112/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_23/checkpoint-112/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_23/checkpoint-112/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_23/checkpoint-112/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_23/checkpoint-80] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_23/checkpoint-128\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_23/checkpoint-128/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_23/checkpoint-128/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_23/checkpoint-128/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_23/checkpoint-128/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_23/checkpoint-96] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_23/checkpoint-144\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_23/checkpoint-144/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_23/checkpoint-144/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_23/checkpoint-144/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_23/checkpoint-144/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_23/checkpoint-112] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_23/checkpoint-160\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_23/checkpoint-160/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_23/checkpoint-160/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_23/checkpoint-160/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_23/checkpoint-160/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_23/checkpoint-128] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_23/checkpoint-176\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_23/checkpoint-176/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_23/checkpoint-176/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_23/checkpoint-176/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_23/checkpoint-176/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_23/checkpoint-144] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_23/checkpoint-192\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_23/checkpoint-192/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_23/checkpoint-192/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_23/checkpoint-192/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_23/checkpoint-192/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_23/checkpoint-160] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_23/checkpoint-208\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_23/checkpoint-208/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_23/checkpoint-208/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_23/checkpoint-208/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_23/checkpoint-208/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_23/checkpoint-176] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_23/checkpoint-224\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_23/checkpoint-224/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_23/checkpoint-224/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_23/checkpoint-224/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_23/checkpoint-224/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_23/checkpoint-192] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_23/checkpoint-240\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_23/checkpoint-240/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_23/checkpoint-240/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_23/checkpoint-240/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_23/checkpoint-240/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_23/checkpoint-208] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_23/checkpoint-256\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_23/checkpoint-256/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_23/checkpoint-256/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_23/checkpoint-256/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_23/checkpoint-256/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_23/checkpoint-224] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_23/checkpoint-272\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_23/checkpoint-272/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_23/checkpoint-272/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_23/checkpoint-272/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_23/checkpoint-272/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_23/checkpoint-240] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_23/checkpoint-288\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_23/checkpoint-288/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_23/checkpoint-288/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_23/checkpoint-288/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_23/checkpoint-288/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_23/checkpoint-256] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_23/checkpoint-304\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_23/checkpoint-304/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_23/checkpoint-304/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_23/checkpoint-304/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_23/checkpoint-304/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_23/checkpoint-272] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_23/checkpoint-320\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_23/checkpoint-320/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_23/checkpoint-320/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_23/checkpoint-320/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_23/checkpoint-320/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_23/checkpoint-288] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>██▇▇▇▆▆▅▅▄▄▃▃▂▂▂▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▆▅█▆▆▇▇▇▇▆▆▆█▂▁▂▅▇▁▆</td></tr><tr><td>eval/samples_per_second</td><td>▃▄▁▃▃▂▂▂▂▃▃▂▁▆█▇▃▂█▃</td></tr><tr><td>eval/steps_per_second</td><td>▃▄▁▃▃▂▂▂▂▃▃▂▁▆█▇▃▂█▃</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇█████</td></tr><tr><td>train/learning_rate</td><td>██▇▇▇▆▆▅▅▅▄▄▄▃▃▂▂▂▁▁</td></tr><tr><td>train/loss</td><td>██▇▇▇▆▆▅▅▅▄▃▃▂▂▂▂▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.66042</td></tr><tr><td>eval/runtime</td><td>0.0648</td></tr><tr><td>eval/samples_per_second</td><td>3085.976</td></tr><tr><td>eval/steps_per_second</td><td>30.86</td></tr><tr><td>train/epoch</td><td>20.0</td></tr><tr><td>train/global_step</td><td>320</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.6646</td></tr><tr><td>train/total_flos</td><td>212168160000.0</td></tr><tr><td>train/train_loss</td><td>0.67772</td></tr><tr><td>train/train_runtime</td><td>17.6998</td></tr><tr><td>train/train_samples_per_second</td><td>2259.911</td></tr><tr><td>train/train_steps_per_second</td><td>18.079</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">pos syn style 23</strong>: <a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/3fzlfs8j\" target=\"_blank\">https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/3fzlfs8j</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/scratch/data_jz17d/wandb_tmp/wandb/run-20221114_113040-3fzlfs8j/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c795bf64b8b94d4bbaed3a64f2c2b6ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2bfb53b863f4a0e946edaefba03a685",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"/scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 32,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 128,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 128,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 54\n",
      "}\n",
      "\n",
      "loading weights file /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/ and are newly initialized: ['bert.pooler.dense.weight', 'classifier.weight', 'bert.pooler.dense.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch/data_jz17d/wandb_tmp/wandb/run-20221114_113118-12jyvq33</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/12jyvq33\" target=\"_blank\">pos syn style 24</a></strong> to <a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2000\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 128\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 320\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='320' max='320' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [320/320 00:24, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.692000</td>\n",
       "      <td>0.689479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.689000</td>\n",
       "      <td>0.686237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.685700</td>\n",
       "      <td>0.682076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.681400</td>\n",
       "      <td>0.676584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.675800</td>\n",
       "      <td>0.669458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.668700</td>\n",
       "      <td>0.660724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.660700</td>\n",
       "      <td>0.650573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.651500</td>\n",
       "      <td>0.639577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.642500</td>\n",
       "      <td>0.628592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.633200</td>\n",
       "      <td>0.618174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.623700</td>\n",
       "      <td>0.608459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.615400</td>\n",
       "      <td>0.599372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.607700</td>\n",
       "      <td>0.591277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.600800</td>\n",
       "      <td>0.584300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.595100</td>\n",
       "      <td>0.578339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.589500</td>\n",
       "      <td>0.573379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.585100</td>\n",
       "      <td>0.569685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.582800</td>\n",
       "      <td>0.567040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.580600</td>\n",
       "      <td>0.565444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.579400</td>\n",
       "      <td>0.564905</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_24/checkpoint-16\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_24/checkpoint-16/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_24/checkpoint-16/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_24/checkpoint-16/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_24/checkpoint-16/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_24/checkpoint-32\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_24/checkpoint-32/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_24/checkpoint-32/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_24/checkpoint-32/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_24/checkpoint-32/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_24/checkpoint-48\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_24/checkpoint-48/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_24/checkpoint-48/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_24/checkpoint-48/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_24/checkpoint-48/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_24/checkpoint-16] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_24/checkpoint-64\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_24/checkpoint-64/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_24/checkpoint-64/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_24/checkpoint-64/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_24/checkpoint-64/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_24/checkpoint-32] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_24/checkpoint-80\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_24/checkpoint-80/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_24/checkpoint-80/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_24/checkpoint-80/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_24/checkpoint-80/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_24/checkpoint-48] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_24/checkpoint-96\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_24/checkpoint-96/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_24/checkpoint-96/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_24/checkpoint-96/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_24/checkpoint-96/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_24/checkpoint-64] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_24/checkpoint-112\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_24/checkpoint-112/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_24/checkpoint-112/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_24/checkpoint-112/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_24/checkpoint-112/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_24/checkpoint-80] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_24/checkpoint-128\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_24/checkpoint-128/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_24/checkpoint-128/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_24/checkpoint-128/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_24/checkpoint-128/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_24/checkpoint-96] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_24/checkpoint-144\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_24/checkpoint-144/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_24/checkpoint-144/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_24/checkpoint-144/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_24/checkpoint-144/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_24/checkpoint-112] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_24/checkpoint-160\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_24/checkpoint-160/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_24/checkpoint-160/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_24/checkpoint-160/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_24/checkpoint-160/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_24/checkpoint-128] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_24/checkpoint-176\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_24/checkpoint-176/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_24/checkpoint-176/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_24/checkpoint-176/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_24/checkpoint-176/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_24/checkpoint-144] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_24/checkpoint-192\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_24/checkpoint-192/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_24/checkpoint-192/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_24/checkpoint-192/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_24/checkpoint-192/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_24/checkpoint-160] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_24/checkpoint-208\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_24/checkpoint-208/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_24/checkpoint-208/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_24/checkpoint-208/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_24/checkpoint-208/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_24/checkpoint-176] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_24/checkpoint-224\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_24/checkpoint-224/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_24/checkpoint-224/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_24/checkpoint-224/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_24/checkpoint-224/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_24/checkpoint-192] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_24/checkpoint-240\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_24/checkpoint-240/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_24/checkpoint-240/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_24/checkpoint-240/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_24/checkpoint-240/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_24/checkpoint-208] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_24/checkpoint-256\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_24/checkpoint-256/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_24/checkpoint-256/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_24/checkpoint-256/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_24/checkpoint-256/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_24/checkpoint-224] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_24/checkpoint-272\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_24/checkpoint-272/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_24/checkpoint-272/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_24/checkpoint-272/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_24/checkpoint-272/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_24/checkpoint-240] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_24/checkpoint-288\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_24/checkpoint-288/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_24/checkpoint-288/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_24/checkpoint-288/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_24/checkpoint-288/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_24/checkpoint-256] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_24/checkpoint-304\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_24/checkpoint-304/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_24/checkpoint-304/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_24/checkpoint-304/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_24/checkpoint-304/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_24/checkpoint-272] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_24/checkpoint-320\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_24/checkpoint-320/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_24/checkpoint-320/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_24/checkpoint-320/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_24/checkpoint-320/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_24/checkpoint-288] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>███▇▇▆▆▅▅▄▃▃▂▂▂▁▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▆▇▇▂▆▆▆▅▁▇█▆▅▆▇▆▅▅█▆</td></tr><tr><td>eval/samples_per_second</td><td>▃▂▂▇▃▃▃▃█▂▁▂▄▂▂▃▄▄▁▂</td></tr><tr><td>eval/steps_per_second</td><td>▃▂▂▇▃▃▃▃█▂▁▂▄▂▂▃▄▄▁▂</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇█████</td></tr><tr><td>train/learning_rate</td><td>██▇▇▇▆▆▅▅▅▄▄▄▃▃▂▂▂▁▁</td></tr><tr><td>train/loss</td><td>███▇▇▇▆▅▅▄▄▃▃▂▂▂▁▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.56491</td></tr><tr><td>eval/runtime</td><td>0.0662</td></tr><tr><td>eval/samples_per_second</td><td>3023.118</td></tr><tr><td>eval/steps_per_second</td><td>30.231</td></tr><tr><td>train/epoch</td><td>20.0</td></tr><tr><td>train/global_step</td><td>320</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.5794</td></tr><tr><td>train/total_flos</td><td>212168160000.0</td></tr><tr><td>train/train_loss</td><td>0.63202</td></tr><tr><td>train/train_runtime</td><td>24.6274</td></tr><tr><td>train/train_samples_per_second</td><td>1624.205</td></tr><tr><td>train/train_steps_per_second</td><td>12.994</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">pos syn style 24</strong>: <a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/12jyvq33\" target=\"_blank\">https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/12jyvq33</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/scratch/data_jz17d/wandb_tmp/wandb/run-20221114_113118-12jyvq33/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01af84bfd54048ec8c317f55639214be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "861285315ca140068c9a3d990ba44656",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"/scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 32,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 128,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 128,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 54\n",
      "}\n",
      "\n",
      "loading weights file /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/ and are newly initialized: ['bert.pooler.dense.weight', 'classifier.weight', 'bert.pooler.dense.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch/data_jz17d/wandb_tmp/wandb/run-20221114_113151-wt1m5z4x</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/wt1m5z4x\" target=\"_blank\">pos syn style 25</a></strong> to <a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2000\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 128\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 320\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='320' max='320' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [320/320 00:15, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.693100</td>\n",
       "      <td>0.692089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.692600</td>\n",
       "      <td>0.691669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.692300</td>\n",
       "      <td>0.691262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.692000</td>\n",
       "      <td>0.690882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.691600</td>\n",
       "      <td>0.690526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.691300</td>\n",
       "      <td>0.690193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.691000</td>\n",
       "      <td>0.689892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.690700</td>\n",
       "      <td>0.689610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.690600</td>\n",
       "      <td>0.689347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.690300</td>\n",
       "      <td>0.689111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.690000</td>\n",
       "      <td>0.688896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.689900</td>\n",
       "      <td>0.688703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.689600</td>\n",
       "      <td>0.688527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.689600</td>\n",
       "      <td>0.688377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.689300</td>\n",
       "      <td>0.688255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.689200</td>\n",
       "      <td>0.688147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.689200</td>\n",
       "      <td>0.688067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.689000</td>\n",
       "      <td>0.688009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.689200</td>\n",
       "      <td>0.687976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.689200</td>\n",
       "      <td>0.687964</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_25/checkpoint-16\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_25/checkpoint-16/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_25/checkpoint-16/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_25/checkpoint-16/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_25/checkpoint-16/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_25/checkpoint-32\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_25/checkpoint-32/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_25/checkpoint-32/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_25/checkpoint-32/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_25/checkpoint-32/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_25/checkpoint-48\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_25/checkpoint-48/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_25/checkpoint-48/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_25/checkpoint-48/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_25/checkpoint-48/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_25/checkpoint-16] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_25/checkpoint-64\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_25/checkpoint-64/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_25/checkpoint-64/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_25/checkpoint-64/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_25/checkpoint-64/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_25/checkpoint-32] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_25/checkpoint-80\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_25/checkpoint-80/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_25/checkpoint-80/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_25/checkpoint-80/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_25/checkpoint-80/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_25/checkpoint-48] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_25/checkpoint-96\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_25/checkpoint-96/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_25/checkpoint-96/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_25/checkpoint-96/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_25/checkpoint-96/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_25/checkpoint-64] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_25/checkpoint-112\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_25/checkpoint-112/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_25/checkpoint-112/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_25/checkpoint-112/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_25/checkpoint-112/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_25/checkpoint-80] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_25/checkpoint-128\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_25/checkpoint-128/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_25/checkpoint-128/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_25/checkpoint-128/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_25/checkpoint-128/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_25/checkpoint-96] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_25/checkpoint-144\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_25/checkpoint-144/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_25/checkpoint-144/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_25/checkpoint-144/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_25/checkpoint-144/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_25/checkpoint-112] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_25/checkpoint-160\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_25/checkpoint-160/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_25/checkpoint-160/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_25/checkpoint-160/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_25/checkpoint-160/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_25/checkpoint-128] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_25/checkpoint-176\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_25/checkpoint-176/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_25/checkpoint-176/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_25/checkpoint-176/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_25/checkpoint-176/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_25/checkpoint-144] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_25/checkpoint-192\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_25/checkpoint-192/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_25/checkpoint-192/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_25/checkpoint-192/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_25/checkpoint-192/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_25/checkpoint-160] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_25/checkpoint-208\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_25/checkpoint-208/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_25/checkpoint-208/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_25/checkpoint-208/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_25/checkpoint-208/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_25/checkpoint-176] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_25/checkpoint-224\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_25/checkpoint-224/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_25/checkpoint-224/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_25/checkpoint-224/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_25/checkpoint-224/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_25/checkpoint-192] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_25/checkpoint-240\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_25/checkpoint-240/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_25/checkpoint-240/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_25/checkpoint-240/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_25/checkpoint-240/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_25/checkpoint-208] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_25/checkpoint-256\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_25/checkpoint-256/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_25/checkpoint-256/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_25/checkpoint-256/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_25/checkpoint-256/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_25/checkpoint-224] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_25/checkpoint-272\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_25/checkpoint-272/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_25/checkpoint-272/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_25/checkpoint-272/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_25/checkpoint-272/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_25/checkpoint-240] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_25/checkpoint-288\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_25/checkpoint-288/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_25/checkpoint-288/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_25/checkpoint-288/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_25/checkpoint-288/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_25/checkpoint-256] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_25/checkpoint-304\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_25/checkpoint-304/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_25/checkpoint-304/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_25/checkpoint-304/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_25/checkpoint-304/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_25/checkpoint-272] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_25/checkpoint-320\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_25/checkpoint-320/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_25/checkpoint-320/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_25/checkpoint-320/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_25/checkpoint-320/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_25/checkpoint-288] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▇▇▆▅▅▄▄▃▃▃▂▂▂▁▁▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▃▄█▄▅▄▄▅▄▃▄▄▄▄▃▁▃▅▁▃</td></tr><tr><td>eval/samples_per_second</td><td>▅▄▁▅▄▅▅▄▅▅▄▄▄▅▆█▅▄█▆</td></tr><tr><td>eval/steps_per_second</td><td>▅▄▁▅▄▅▅▄▅▅▄▄▄▅▆█▅▄█▆</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇█████</td></tr><tr><td>train/learning_rate</td><td>██▇▇▇▆▆▅▅▅▄▄▄▃▃▂▂▂▁▁</td></tr><tr><td>train/loss</td><td>█▇▇▆▅▅▄▄▄▃▃▃▂▂▂▁▁▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.68796</td></tr><tr><td>eval/runtime</td><td>0.0639</td></tr><tr><td>eval/samples_per_second</td><td>3130.697</td></tr><tr><td>eval/steps_per_second</td><td>31.307</td></tr><tr><td>train/epoch</td><td>20.0</td></tr><tr><td>train/global_step</td><td>320</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.6892</td></tr><tr><td>train/total_flos</td><td>212168160000.0</td></tr><tr><td>train/train_loss</td><td>0.69049</td></tr><tr><td>train/train_runtime</td><td>15.873</td></tr><tr><td>train/train_samples_per_second</td><td>2520.01</td></tr><tr><td>train/train_steps_per_second</td><td>20.16</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">pos syn style 25</strong>: <a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/wt1m5z4x\" target=\"_blank\">https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/wt1m5z4x</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/scratch/data_jz17d/wandb_tmp/wandb/run-20221114_113151-wt1m5z4x/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "058416cf7dd14fc5a4cc78f012eeb69f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "322520c34a914a9a84f59ba1e2d8d0a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"/scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 32,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 128,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 128,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 54\n",
      "}\n",
      "\n",
      "loading weights file /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/ and are newly initialized: ['bert.pooler.dense.weight', 'classifier.weight', 'bert.pooler.dense.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch/data_jz17d/wandb_tmp/wandb/run-20221114_113217-3qho6yx2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/3qho6yx2\" target=\"_blank\">pos syn style 26</a></strong> to <a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2000\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 128\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 320\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='320' max='320' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [320/320 00:19, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.692300</td>\n",
       "      <td>0.690096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.689900</td>\n",
       "      <td>0.687622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.687600</td>\n",
       "      <td>0.684771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.684800</td>\n",
       "      <td>0.681445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.681400</td>\n",
       "      <td>0.677440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.677400</td>\n",
       "      <td>0.672731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.673000</td>\n",
       "      <td>0.667401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.667800</td>\n",
       "      <td>0.661415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.662600</td>\n",
       "      <td>0.654939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.656900</td>\n",
       "      <td>0.648232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.650800</td>\n",
       "      <td>0.641578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.645100</td>\n",
       "      <td>0.635030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.639000</td>\n",
       "      <td>0.628899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.634500</td>\n",
       "      <td>0.623304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.629300</td>\n",
       "      <td>0.618428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.625100</td>\n",
       "      <td>0.614361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.622400</td>\n",
       "      <td>0.611166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.619300</td>\n",
       "      <td>0.608848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.618400</td>\n",
       "      <td>0.607450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.616800</td>\n",
       "      <td>0.606974</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_26/checkpoint-16\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_26/checkpoint-16/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_26/checkpoint-16/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_26/checkpoint-16/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_26/checkpoint-16/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_26/checkpoint-32\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_26/checkpoint-32/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_26/checkpoint-32/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_26/checkpoint-32/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_26/checkpoint-32/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_26/checkpoint-48\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_26/checkpoint-48/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_26/checkpoint-48/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_26/checkpoint-48/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_26/checkpoint-48/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_26/checkpoint-16] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_26/checkpoint-64\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_26/checkpoint-64/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_26/checkpoint-64/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_26/checkpoint-64/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_26/checkpoint-64/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_26/checkpoint-32] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_26/checkpoint-80\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_26/checkpoint-80/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_26/checkpoint-80/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_26/checkpoint-80/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_26/checkpoint-80/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_26/checkpoint-48] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_26/checkpoint-96\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_26/checkpoint-96/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_26/checkpoint-96/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_26/checkpoint-96/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_26/checkpoint-96/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_26/checkpoint-64] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_26/checkpoint-112\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_26/checkpoint-112/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_26/checkpoint-112/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_26/checkpoint-112/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_26/checkpoint-112/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_26/checkpoint-80] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_26/checkpoint-128\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_26/checkpoint-128/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_26/checkpoint-128/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_26/checkpoint-128/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_26/checkpoint-128/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_26/checkpoint-96] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_26/checkpoint-144\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_26/checkpoint-144/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_26/checkpoint-144/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_26/checkpoint-144/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_26/checkpoint-144/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_26/checkpoint-112] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_26/checkpoint-160\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_26/checkpoint-160/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_26/checkpoint-160/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_26/checkpoint-160/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_26/checkpoint-160/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_26/checkpoint-128] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_26/checkpoint-176\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_26/checkpoint-176/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_26/checkpoint-176/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_26/checkpoint-176/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_26/checkpoint-176/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_26/checkpoint-144] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_26/checkpoint-192\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_26/checkpoint-192/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_26/checkpoint-192/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_26/checkpoint-192/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_26/checkpoint-192/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_26/checkpoint-160] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_26/checkpoint-208\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_26/checkpoint-208/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_26/checkpoint-208/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_26/checkpoint-208/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_26/checkpoint-208/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_26/checkpoint-176] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_26/checkpoint-224\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_26/checkpoint-224/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_26/checkpoint-224/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_26/checkpoint-224/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_26/checkpoint-224/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_26/checkpoint-192] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_26/checkpoint-240\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_26/checkpoint-240/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_26/checkpoint-240/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_26/checkpoint-240/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_26/checkpoint-240/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_26/checkpoint-208] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_26/checkpoint-256\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_26/checkpoint-256/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_26/checkpoint-256/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_26/checkpoint-256/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_26/checkpoint-256/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_26/checkpoint-224] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_26/checkpoint-272\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_26/checkpoint-272/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_26/checkpoint-272/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_26/checkpoint-272/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_26/checkpoint-272/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_26/checkpoint-240] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_26/checkpoint-288\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_26/checkpoint-288/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_26/checkpoint-288/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_26/checkpoint-288/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_26/checkpoint-288/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_26/checkpoint-256] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_26/checkpoint-304\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_26/checkpoint-304/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_26/checkpoint-304/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_26/checkpoint-304/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_26/checkpoint-304/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_26/checkpoint-272] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_26/checkpoint-320\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_26/checkpoint-320/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_26/checkpoint-320/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_26/checkpoint-320/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_26/checkpoint-320/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_26/checkpoint-288] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>███▇▇▇▆▆▅▄▄▃▃▂▂▂▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▇▆▆▅▅▁▅▆▆▆▁▆▆▅█▅▆▂▅▅</td></tr><tr><td>eval/samples_per_second</td><td>▂▃▃▃▃█▃▃▃▃▇▂▃▃▁▄▃▇▄▃</td></tr><tr><td>eval/steps_per_second</td><td>▂▃▃▃▃█▃▃▃▃▇▂▃▃▁▄▃▇▄▃</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇█████</td></tr><tr><td>train/learning_rate</td><td>██▇▇▇▆▆▅▅▅▄▄▄▃▃▂▂▂▁▁</td></tr><tr><td>train/loss</td><td>███▇▇▇▆▆▅▅▄▄▃▃▂▂▂▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.60697</td></tr><tr><td>eval/runtime</td><td>0.0657</td></tr><tr><td>eval/samples_per_second</td><td>3043.652</td></tr><tr><td>eval/steps_per_second</td><td>30.437</td></tr><tr><td>train/epoch</td><td>20.0</td></tr><tr><td>train/global_step</td><td>320</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.6168</td></tr><tr><td>train/total_flos</td><td>212168160000.0</td></tr><tr><td>train/train_loss</td><td>0.65373</td></tr><tr><td>train/train_runtime</td><td>19.8748</td></tr><tr><td>train/train_samples_per_second</td><td>2012.594</td></tr><tr><td>train/train_steps_per_second</td><td>16.101</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">pos syn style 26</strong>: <a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/3qho6yx2\" target=\"_blank\">https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/3qho6yx2</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/scratch/data_jz17d/wandb_tmp/wandb/run-20221114_113217-3qho6yx2/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb26d39ec6604e63b3cad785c6120fb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf63ed3dbd4c4b6c80bce6cdf82b651e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"/scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 32,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 128,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 128,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 54\n",
      "}\n",
      "\n",
      "loading weights file /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/ and are newly initialized: ['bert.pooler.dense.weight', 'classifier.weight', 'bert.pooler.dense.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch/data_jz17d/wandb_tmp/wandb/run-20221114_113245-l05hv5s5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/l05hv5s5\" target=\"_blank\">pos syn style 27</a></strong> to <a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2000\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 128\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 320\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='320' max='320' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [320/320 00:17, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.692400</td>\n",
       "      <td>0.690490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.690500</td>\n",
       "      <td>0.688514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.688700</td>\n",
       "      <td>0.686414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.686800</td>\n",
       "      <td>0.684150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.684500</td>\n",
       "      <td>0.681623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.682000</td>\n",
       "      <td>0.678841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.679500</td>\n",
       "      <td>0.675833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.676500</td>\n",
       "      <td>0.672581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.673800</td>\n",
       "      <td>0.669132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.670500</td>\n",
       "      <td>0.665600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.666900</td>\n",
       "      <td>0.662055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.663800</td>\n",
       "      <td>0.658564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.660300</td>\n",
       "      <td>0.655200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.658200</td>\n",
       "      <td>0.652163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.654500</td>\n",
       "      <td>0.649479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.652400</td>\n",
       "      <td>0.647178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.651100</td>\n",
       "      <td>0.645383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.649200</td>\n",
       "      <td>0.644087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.649100</td>\n",
       "      <td>0.643298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.648000</td>\n",
       "      <td>0.643028</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_27/checkpoint-16\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_27/checkpoint-16/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_27/checkpoint-16/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_27/checkpoint-16/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_27/checkpoint-16/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_27/checkpoint-32\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_27/checkpoint-32/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_27/checkpoint-32/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_27/checkpoint-32/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_27/checkpoint-32/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_27/checkpoint-48\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_27/checkpoint-48/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_27/checkpoint-48/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_27/checkpoint-48/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_27/checkpoint-48/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_27/checkpoint-16] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_27/checkpoint-64\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_27/checkpoint-64/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_27/checkpoint-64/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_27/checkpoint-64/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_27/checkpoint-64/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_27/checkpoint-32] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_27/checkpoint-80\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_27/checkpoint-80/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_27/checkpoint-80/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_27/checkpoint-80/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_27/checkpoint-80/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_27/checkpoint-48] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_27/checkpoint-96\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_27/checkpoint-96/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_27/checkpoint-96/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_27/checkpoint-96/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_27/checkpoint-96/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_27/checkpoint-64] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_27/checkpoint-112\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_27/checkpoint-112/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_27/checkpoint-112/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_27/checkpoint-112/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_27/checkpoint-112/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_27/checkpoint-80] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_27/checkpoint-128\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_27/checkpoint-128/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_27/checkpoint-128/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_27/checkpoint-128/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_27/checkpoint-128/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_27/checkpoint-96] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_27/checkpoint-144\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_27/checkpoint-144/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_27/checkpoint-144/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_27/checkpoint-144/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_27/checkpoint-144/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_27/checkpoint-112] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_27/checkpoint-160\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_27/checkpoint-160/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_27/checkpoint-160/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_27/checkpoint-160/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_27/checkpoint-160/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_27/checkpoint-128] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_27/checkpoint-176\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_27/checkpoint-176/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_27/checkpoint-176/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_27/checkpoint-176/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_27/checkpoint-176/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_27/checkpoint-144] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_27/checkpoint-192\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_27/checkpoint-192/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_27/checkpoint-192/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_27/checkpoint-192/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_27/checkpoint-192/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_27/checkpoint-160] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_27/checkpoint-208\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_27/checkpoint-208/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_27/checkpoint-208/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_27/checkpoint-208/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_27/checkpoint-208/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_27/checkpoint-176] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_27/checkpoint-224\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_27/checkpoint-224/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_27/checkpoint-224/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_27/checkpoint-224/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_27/checkpoint-224/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_27/checkpoint-192] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_27/checkpoint-240\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_27/checkpoint-240/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_27/checkpoint-240/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_27/checkpoint-240/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_27/checkpoint-240/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_27/checkpoint-208] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_27/checkpoint-256\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_27/checkpoint-256/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_27/checkpoint-256/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_27/checkpoint-256/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_27/checkpoint-256/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_27/checkpoint-224] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_27/checkpoint-272\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_27/checkpoint-272/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_27/checkpoint-272/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_27/checkpoint-272/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_27/checkpoint-272/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_27/checkpoint-240] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_27/checkpoint-288\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_27/checkpoint-288/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_27/checkpoint-288/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_27/checkpoint-288/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_27/checkpoint-288/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_27/checkpoint-256] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_27/checkpoint-304\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_27/checkpoint-304/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_27/checkpoint-304/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_27/checkpoint-304/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_27/checkpoint-304/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_27/checkpoint-272] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_27/checkpoint-320\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_27/checkpoint-320/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_27/checkpoint-320/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_27/checkpoint-320/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_27/checkpoint-320/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_27/checkpoint-288] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>██▇▇▇▆▆▅▅▄▄▃▃▂▂▂▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▅▂▂▄▄▄▅▄▄▁▃▅▅█▅▅▅▄▅▅</td></tr><tr><td>eval/samples_per_second</td><td>▄▇▆▄▅▄▃▄▅█▅▃▄▁▄▄▄▄▄▄</td></tr><tr><td>eval/steps_per_second</td><td>▄▇▆▄▅▄▃▄▅█▅▃▄▁▄▄▄▄▄▄</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇█████</td></tr><tr><td>train/learning_rate</td><td>██▇▇▇▆▆▅▅▅▄▄▄▃▃▂▂▂▁▁</td></tr><tr><td>train/loss</td><td>██▇▇▇▆▆▅▅▅▄▃▃▃▂▂▁▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.64303</td></tr><tr><td>eval/runtime</td><td>0.0661</td></tr><tr><td>eval/samples_per_second</td><td>3025.364</td></tr><tr><td>eval/steps_per_second</td><td>30.254</td></tr><tr><td>train/epoch</td><td>20.0</td></tr><tr><td>train/global_step</td><td>320</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.648</td></tr><tr><td>train/total_flos</td><td>212168160000.0</td></tr><tr><td>train/train_loss</td><td>0.66893</td></tr><tr><td>train/train_runtime</td><td>17.7211</td></tr><tr><td>train/train_samples_per_second</td><td>2257.195</td></tr><tr><td>train/train_steps_per_second</td><td>18.058</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">pos syn style 27</strong>: <a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/l05hv5s5\" target=\"_blank\">https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/l05hv5s5</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/scratch/data_jz17d/wandb_tmp/wandb/run-20221114_113245-l05hv5s5/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "528ec787305b4bffac25ad3a48f6004b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "420d88e5aa2c4200a7667598414d5e37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"/scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 32,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 128,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 128,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 54\n",
      "}\n",
      "\n",
      "loading weights file /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/ and are newly initialized: ['bert.pooler.dense.weight', 'classifier.weight', 'bert.pooler.dense.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch/data_jz17d/wandb_tmp/wandb/run-20221114_113312-1tpemw6r</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/1tpemw6r\" target=\"_blank\">pos syn style 28</a></strong> to <a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2000\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 128\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 320\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='320' max='320' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [320/320 00:24, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.691700</td>\n",
       "      <td>0.688911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.687800</td>\n",
       "      <td>0.684634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.683500</td>\n",
       "      <td>0.679291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.677900</td>\n",
       "      <td>0.672564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.671200</td>\n",
       "      <td>0.664016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.662600</td>\n",
       "      <td>0.653934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.653300</td>\n",
       "      <td>0.642663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.643100</td>\n",
       "      <td>0.630621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.632900</td>\n",
       "      <td>0.619012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.622500</td>\n",
       "      <td>0.608028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.612500</td>\n",
       "      <td>0.597677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.603200</td>\n",
       "      <td>0.588199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.594500</td>\n",
       "      <td>0.579726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.587500</td>\n",
       "      <td>0.572347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.579800</td>\n",
       "      <td>0.566157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.575400</td>\n",
       "      <td>0.561186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.570500</td>\n",
       "      <td>0.557174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.568100</td>\n",
       "      <td>0.554434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.565100</td>\n",
       "      <td>0.552783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.564700</td>\n",
       "      <td>0.552236</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_28/checkpoint-16\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_28/checkpoint-16/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_28/checkpoint-16/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_28/checkpoint-16/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_28/checkpoint-16/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_28/checkpoint-32\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_28/checkpoint-32/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_28/checkpoint-32/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_28/checkpoint-32/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_28/checkpoint-32/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_28/checkpoint-48\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_28/checkpoint-48/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_28/checkpoint-48/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_28/checkpoint-48/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_28/checkpoint-48/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_28/checkpoint-16] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_28/checkpoint-64\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_28/checkpoint-64/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_28/checkpoint-64/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_28/checkpoint-64/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_28/checkpoint-64/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_28/checkpoint-32] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_28/checkpoint-80\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_28/checkpoint-80/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_28/checkpoint-80/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_28/checkpoint-80/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_28/checkpoint-80/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_28/checkpoint-48] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_28/checkpoint-96\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_28/checkpoint-96/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_28/checkpoint-96/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_28/checkpoint-96/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_28/checkpoint-96/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_28/checkpoint-64] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_28/checkpoint-112\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_28/checkpoint-112/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_28/checkpoint-112/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_28/checkpoint-112/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_28/checkpoint-112/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_28/checkpoint-80] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_28/checkpoint-128\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_28/checkpoint-128/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_28/checkpoint-128/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_28/checkpoint-128/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_28/checkpoint-128/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_28/checkpoint-96] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_28/checkpoint-144\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_28/checkpoint-144/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_28/checkpoint-144/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_28/checkpoint-144/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_28/checkpoint-144/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_28/checkpoint-112] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_28/checkpoint-160\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_28/checkpoint-160/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_28/checkpoint-160/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_28/checkpoint-160/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_28/checkpoint-160/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_28/checkpoint-128] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_28/checkpoint-176\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_28/checkpoint-176/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_28/checkpoint-176/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_28/checkpoint-176/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_28/checkpoint-176/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_28/checkpoint-144] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_28/checkpoint-192\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_28/checkpoint-192/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_28/checkpoint-192/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_28/checkpoint-192/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_28/checkpoint-192/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_28/checkpoint-160] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_28/checkpoint-208\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_28/checkpoint-208/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_28/checkpoint-208/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_28/checkpoint-208/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_28/checkpoint-208/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_28/checkpoint-176] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_28/checkpoint-224\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_28/checkpoint-224/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_28/checkpoint-224/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_28/checkpoint-224/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_28/checkpoint-224/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_28/checkpoint-192] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_28/checkpoint-240\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_28/checkpoint-240/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_28/checkpoint-240/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_28/checkpoint-240/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_28/checkpoint-240/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_28/checkpoint-208] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_28/checkpoint-256\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_28/checkpoint-256/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_28/checkpoint-256/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_28/checkpoint-256/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_28/checkpoint-256/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_28/checkpoint-224] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_28/checkpoint-272\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_28/checkpoint-272/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_28/checkpoint-272/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_28/checkpoint-272/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_28/checkpoint-272/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_28/checkpoint-240] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_28/checkpoint-288\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_28/checkpoint-288/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_28/checkpoint-288/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_28/checkpoint-288/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_28/checkpoint-288/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_28/checkpoint-256] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_28/checkpoint-304\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_28/checkpoint-304/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_28/checkpoint-304/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_28/checkpoint-304/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_28/checkpoint-304/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_28/checkpoint-272] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_28/checkpoint-320\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_28/checkpoint-320/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_28/checkpoint-320/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_28/checkpoint-320/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_28/checkpoint-320/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_28/checkpoint-288] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>███▇▇▆▆▅▄▄▃▃▂▂▂▁▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▄▅▅▆▆▅▇▃▁▁▅▆█▅▆▅▅▄▆▆</td></tr><tr><td>eval/samples_per_second</td><td>▄▄▄▃▃▄▂▆██▃▃▁▄▃▃▃▄▃▃</td></tr><tr><td>eval/steps_per_second</td><td>▄▄▄▃▃▄▂▆██▃▃▁▄▃▃▃▄▃▃</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇█████</td></tr><tr><td>train/learning_rate</td><td>██▇▇▇▆▆▅▅▅▄▄▄▃▃▂▂▂▁▁</td></tr><tr><td>train/loss</td><td>███▇▇▆▆▅▅▄▄▃▃▂▂▂▁▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.55224</td></tr><tr><td>eval/runtime</td><td>0.0674</td></tr><tr><td>eval/samples_per_second</td><td>2966.657</td></tr><tr><td>eval/steps_per_second</td><td>29.667</td></tr><tr><td>train/epoch</td><td>20.0</td></tr><tr><td>train/global_step</td><td>320</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.5647</td></tr><tr><td>train/total_flos</td><td>212168160000.0</td></tr><tr><td>train/train_loss</td><td>0.62239</td></tr><tr><td>train/train_runtime</td><td>25.0921</td></tr><tr><td>train/train_samples_per_second</td><td>1594.126</td></tr><tr><td>train/train_steps_per_second</td><td>12.753</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">pos syn style 28</strong>: <a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/1tpemw6r\" target=\"_blank\">https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/1tpemw6r</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/scratch/data_jz17d/wandb_tmp/wandb/run-20221114_113312-1tpemw6r/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02f1cd7a95c34eae845018243a035a04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8532c5eed63b4dc593ab1218926629f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"/scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 32,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 128,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 128,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 54\n",
      "}\n",
      "\n",
      "loading weights file /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/ and are newly initialized: ['bert.pooler.dense.weight', 'classifier.weight', 'bert.pooler.dense.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch/data_jz17d/wandb_tmp/wandb/run-20221114_113345-1xwguogz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/1xwguogz\" target=\"_blank\">pos syn style 29</a></strong> to <a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2000\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 128\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 320\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='320' max='320' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [320/320 00:16, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.693200</td>\n",
       "      <td>0.692392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.692600</td>\n",
       "      <td>0.691846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.692100</td>\n",
       "      <td>0.691320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.691600</td>\n",
       "      <td>0.690828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.691300</td>\n",
       "      <td>0.690364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.690800</td>\n",
       "      <td>0.689933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.690400</td>\n",
       "      <td>0.689537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.690100</td>\n",
       "      <td>0.689171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.689800</td>\n",
       "      <td>0.688832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.689500</td>\n",
       "      <td>0.688526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.689100</td>\n",
       "      <td>0.688249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.689000</td>\n",
       "      <td>0.687997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.688700</td>\n",
       "      <td>0.687774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.688600</td>\n",
       "      <td>0.687579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.688300</td>\n",
       "      <td>0.687418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.688200</td>\n",
       "      <td>0.687279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.688100</td>\n",
       "      <td>0.687176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.687900</td>\n",
       "      <td>0.687101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.688000</td>\n",
       "      <td>0.687057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.688100</td>\n",
       "      <td>0.687042</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_29/checkpoint-16\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_29/checkpoint-16/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_29/checkpoint-16/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_29/checkpoint-16/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_29/checkpoint-16/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_29/checkpoint-32\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_29/checkpoint-32/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_29/checkpoint-32/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_29/checkpoint-32/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_29/checkpoint-32/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_29/checkpoint-48\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_29/checkpoint-48/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_29/checkpoint-48/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_29/checkpoint-48/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_29/checkpoint-48/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_29/checkpoint-16] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_29/checkpoint-64\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_29/checkpoint-64/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_29/checkpoint-64/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_29/checkpoint-64/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_29/checkpoint-64/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_29/checkpoint-32] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_29/checkpoint-80\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_29/checkpoint-80/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_29/checkpoint-80/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_29/checkpoint-80/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_29/checkpoint-80/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_29/checkpoint-48] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_29/checkpoint-96\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_29/checkpoint-96/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_29/checkpoint-96/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_29/checkpoint-96/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_29/checkpoint-96/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_29/checkpoint-64] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_29/checkpoint-112\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_29/checkpoint-112/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_29/checkpoint-112/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_29/checkpoint-112/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_29/checkpoint-112/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_29/checkpoint-80] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_29/checkpoint-128\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_29/checkpoint-128/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_29/checkpoint-128/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_29/checkpoint-128/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_29/checkpoint-128/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_29/checkpoint-96] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_29/checkpoint-144\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_29/checkpoint-144/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_29/checkpoint-144/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_29/checkpoint-144/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_29/checkpoint-144/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_29/checkpoint-112] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_29/checkpoint-160\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_29/checkpoint-160/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_29/checkpoint-160/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_29/checkpoint-160/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_29/checkpoint-160/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_29/checkpoint-128] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_29/checkpoint-176\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_29/checkpoint-176/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_29/checkpoint-176/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_29/checkpoint-176/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_29/checkpoint-176/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_29/checkpoint-144] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_29/checkpoint-192\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_29/checkpoint-192/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_29/checkpoint-192/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_29/checkpoint-192/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_29/checkpoint-192/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_29/checkpoint-160] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_29/checkpoint-208\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_29/checkpoint-208/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_29/checkpoint-208/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_29/checkpoint-208/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_29/checkpoint-208/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_29/checkpoint-176] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_29/checkpoint-224\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_29/checkpoint-224/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_29/checkpoint-224/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_29/checkpoint-224/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_29/checkpoint-224/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_29/checkpoint-192] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_29/checkpoint-240\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_29/checkpoint-240/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_29/checkpoint-240/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_29/checkpoint-240/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_29/checkpoint-240/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_29/checkpoint-208] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_29/checkpoint-256\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_29/checkpoint-256/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_29/checkpoint-256/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_29/checkpoint-256/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_29/checkpoint-256/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_29/checkpoint-224] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_29/checkpoint-272\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_29/checkpoint-272/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_29/checkpoint-272/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_29/checkpoint-272/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_29/checkpoint-272/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_29/checkpoint-240] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_29/checkpoint-288\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_29/checkpoint-288/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_29/checkpoint-288/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_29/checkpoint-288/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_29/checkpoint-288/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_29/checkpoint-256] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_29/checkpoint-304\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_29/checkpoint-304/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_29/checkpoint-304/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_29/checkpoint-304/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_29/checkpoint-304/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_29/checkpoint-272] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_29/checkpoint-320\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_29/checkpoint-320/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_29/checkpoint-320/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_29/checkpoint-320/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_29/checkpoint-320/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_29/checkpoint-288] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▇▇▆▅▅▄▄▃▃▃▂▂▂▁▁▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▂▇█▇▇▇▇██▆▇▇▆▆▆▆▅▁▆▆</td></tr><tr><td>eval/samples_per_second</td><td>▇▂▁▂▂▂▂▁▁▃▂▂▃▃▃▂▄█▃▃</td></tr><tr><td>eval/steps_per_second</td><td>▇▂▁▂▂▂▂▁▁▃▂▂▃▃▃▂▄█▃▃</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇█████</td></tr><tr><td>train/learning_rate</td><td>██▇▇▇▆▆▅▅▅▄▄▄▃▃▂▂▂▁▁</td></tr><tr><td>train/loss</td><td>█▇▇▆▅▅▄▄▄▃▃▂▂▂▂▁▁▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.68704</td></tr><tr><td>eval/runtime</td><td>0.0656</td></tr><tr><td>eval/samples_per_second</td><td>3047.344</td></tr><tr><td>eval/steps_per_second</td><td>30.473</td></tr><tr><td>train/epoch</td><td>20.0</td></tr><tr><td>train/global_step</td><td>320</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.6881</td></tr><tr><td>train/total_flos</td><td>212168160000.0</td></tr><tr><td>train/train_loss</td><td>0.68977</td></tr><tr><td>train/train_runtime</td><td>15.9828</td></tr><tr><td>train/train_samples_per_second</td><td>2502.683</td></tr><tr><td>train/train_steps_per_second</td><td>20.021</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">pos syn style 29</strong>: <a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/1xwguogz\" target=\"_blank\">https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/1xwguogz</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/scratch/data_jz17d/wandb_tmp/wandb/run-20221114_113345-1xwguogz/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ba12b3f711244ae911581981dea5de1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "150d723f20e84d79bf703b78b0e8b40c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"/scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 32,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 128,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 128,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 54\n",
      "}\n",
      "\n",
      "loading weights file /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/ and are newly initialized: ['bert.pooler.dense.weight', 'classifier.weight', 'bert.pooler.dense.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch/data_jz17d/wandb_tmp/wandb/run-20221114_113410-3fyi41or</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/3fyi41or\" target=\"_blank\">pos syn style 30</a></strong> to <a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2000\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 128\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 320\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='320' max='320' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [320/320 00:20, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.692100</td>\n",
       "      <td>0.689766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.689000</td>\n",
       "      <td>0.686538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.686000</td>\n",
       "      <td>0.682857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.682200</td>\n",
       "      <td>0.678597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.678100</td>\n",
       "      <td>0.673559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.673000</td>\n",
       "      <td>0.667761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.667300</td>\n",
       "      <td>0.661230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.661400</td>\n",
       "      <td>0.654160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.654600</td>\n",
       "      <td>0.646626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.647900</td>\n",
       "      <td>0.638902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.640700</td>\n",
       "      <td>0.631328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.634200</td>\n",
       "      <td>0.623984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.627100</td>\n",
       "      <td>0.617166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.621600</td>\n",
       "      <td>0.611018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.615200</td>\n",
       "      <td>0.605610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.611600</td>\n",
       "      <td>0.601145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.607900</td>\n",
       "      <td>0.597636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.604200</td>\n",
       "      <td>0.595102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.603400</td>\n",
       "      <td>0.593599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.601900</td>\n",
       "      <td>0.593080</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_30/checkpoint-16\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_30/checkpoint-16/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_30/checkpoint-16/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_30/checkpoint-16/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_30/checkpoint-16/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_30/checkpoint-32\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_30/checkpoint-32/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_30/checkpoint-32/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_30/checkpoint-32/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_30/checkpoint-32/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_30/checkpoint-48\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_30/checkpoint-48/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_30/checkpoint-48/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_30/checkpoint-48/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_30/checkpoint-48/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_30/checkpoint-16] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_30/checkpoint-64\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_30/checkpoint-64/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_30/checkpoint-64/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_30/checkpoint-64/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_30/checkpoint-64/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_30/checkpoint-32] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_30/checkpoint-80\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_30/checkpoint-80/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_30/checkpoint-80/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_30/checkpoint-80/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_30/checkpoint-80/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_30/checkpoint-48] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_30/checkpoint-96\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_30/checkpoint-96/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_30/checkpoint-96/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_30/checkpoint-96/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_30/checkpoint-96/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_30/checkpoint-64] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_30/checkpoint-112\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_30/checkpoint-112/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_30/checkpoint-112/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_30/checkpoint-112/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_30/checkpoint-112/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_30/checkpoint-80] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_30/checkpoint-128\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_30/checkpoint-128/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_30/checkpoint-128/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_30/checkpoint-128/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_30/checkpoint-128/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_30/checkpoint-96] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_30/checkpoint-144\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_30/checkpoint-144/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_30/checkpoint-144/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_30/checkpoint-144/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_30/checkpoint-144/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_30/checkpoint-112] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_30/checkpoint-160\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_30/checkpoint-160/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_30/checkpoint-160/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_30/checkpoint-160/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_30/checkpoint-160/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_30/checkpoint-128] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_30/checkpoint-176\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_30/checkpoint-176/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_30/checkpoint-176/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_30/checkpoint-176/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_30/checkpoint-176/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_30/checkpoint-144] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_30/checkpoint-192\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_30/checkpoint-192/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_30/checkpoint-192/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_30/checkpoint-192/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_30/checkpoint-192/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_30/checkpoint-160] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_30/checkpoint-208\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_30/checkpoint-208/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_30/checkpoint-208/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_30/checkpoint-208/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_30/checkpoint-208/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_30/checkpoint-176] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_30/checkpoint-224\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_30/checkpoint-224/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_30/checkpoint-224/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_30/checkpoint-224/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_30/checkpoint-224/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_30/checkpoint-192] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_30/checkpoint-240\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_30/checkpoint-240/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_30/checkpoint-240/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_30/checkpoint-240/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_30/checkpoint-240/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_30/checkpoint-208] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_30/checkpoint-256\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_30/checkpoint-256/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_30/checkpoint-256/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_30/checkpoint-256/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_30/checkpoint-256/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_30/checkpoint-224] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_30/checkpoint-272\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_30/checkpoint-272/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_30/checkpoint-272/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_30/checkpoint-272/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_30/checkpoint-272/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_30/checkpoint-240] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_30/checkpoint-288\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_30/checkpoint-288/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_30/checkpoint-288/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_30/checkpoint-288/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_30/checkpoint-288/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_30/checkpoint-256] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_30/checkpoint-304\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_30/checkpoint-304/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_30/checkpoint-304/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_30/checkpoint-304/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_30/checkpoint-304/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_30/checkpoint-272] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_30/checkpoint-320\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_30/checkpoint-320/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_30/checkpoint-320/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_30/checkpoint-320/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_30/checkpoint-320/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_30/checkpoint-288] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>██▇▇▇▆▆▅▅▄▄▃▃▂▂▂▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▆█▅▁▂▇▅▅▃▁▅▅▆▄▄▆▅▄▅▆</td></tr><tr><td>eval/samples_per_second</td><td>▃▁▄█▇▂▄▄▅█▄▄▃▄▄▃▃▅▄▃</td></tr><tr><td>eval/steps_per_second</td><td>▃▁▄█▇▂▄▄▅█▄▄▃▄▄▃▃▅▄▃</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇█████</td></tr><tr><td>train/learning_rate</td><td>██▇▇▇▆▆▅▅▅▄▄▄▃▃▂▂▂▁▁</td></tr><tr><td>train/loss</td><td>███▇▇▇▆▆▅▅▄▄▃▃▂▂▁▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.59308</td></tr><tr><td>eval/runtime</td><td>0.0684</td></tr><tr><td>eval/samples_per_second</td><td>2925.48</td></tr><tr><td>eval/steps_per_second</td><td>29.255</td></tr><tr><td>train/epoch</td><td>20.0</td></tr><tr><td>train/global_step</td><td>320</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.6019</td></tr><tr><td>train/total_flos</td><td>212168160000.0</td></tr><tr><td>train/train_loss</td><td>0.64496</td></tr><tr><td>train/train_runtime</td><td>20.5117</td></tr><tr><td>train/train_samples_per_second</td><td>1950.106</td></tr><tr><td>train/train_steps_per_second</td><td>15.601</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">pos syn style 30</strong>: <a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/3fyi41or\" target=\"_blank\">https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/3fyi41or</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/scratch/data_jz17d/wandb_tmp/wandb/run-20221114_113410-3fyi41or/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f58adbf2614249ff80027337e4040f65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f58501b47a848c8b7b41d81bd22845f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"/scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 32,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 128,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 128,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 54\n",
      "}\n",
      "\n",
      "loading weights file /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/ and are newly initialized: ['bert.pooler.dense.weight', 'classifier.weight', 'bert.pooler.dense.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch/data_jz17d/wandb_tmp/wandb/run-20221114_113440-ud29rfra</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/ud29rfra\" target=\"_blank\">pos syn style 31</a></strong> to <a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2000\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 128\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 320\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='320' max='320' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [320/320 00:18, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.692300</td>\n",
       "      <td>0.690286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.689800</td>\n",
       "      <td>0.687695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.687400</td>\n",
       "      <td>0.684954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.684600</td>\n",
       "      <td>0.682012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.681900</td>\n",
       "      <td>0.678741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.678600</td>\n",
       "      <td>0.675167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.675100</td>\n",
       "      <td>0.671259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.671600</td>\n",
       "      <td>0.667084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.667700</td>\n",
       "      <td>0.662694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.663500</td>\n",
       "      <td>0.658195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.659100</td>\n",
       "      <td>0.653708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.655200</td>\n",
       "      <td>0.649308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.650900</td>\n",
       "      <td>0.645133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.647500</td>\n",
       "      <td>0.641319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.643100</td>\n",
       "      <td>0.637963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.640900</td>\n",
       "      <td>0.635105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.638800</td>\n",
       "      <td>0.632885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.636100</td>\n",
       "      <td>0.631271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.636100</td>\n",
       "      <td>0.630303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.635200</td>\n",
       "      <td>0.629970</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_31/checkpoint-16\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_31/checkpoint-16/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_31/checkpoint-16/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_31/checkpoint-16/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_31/checkpoint-16/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_31/checkpoint-32\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_31/checkpoint-32/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_31/checkpoint-32/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_31/checkpoint-32/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_31/checkpoint-32/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_31/checkpoint-48\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_31/checkpoint-48/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_31/checkpoint-48/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_31/checkpoint-48/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_31/checkpoint-48/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_31/checkpoint-16] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_31/checkpoint-64\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_31/checkpoint-64/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_31/checkpoint-64/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_31/checkpoint-64/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_31/checkpoint-64/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_31/checkpoint-32] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_31/checkpoint-80\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_31/checkpoint-80/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_31/checkpoint-80/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_31/checkpoint-80/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_31/checkpoint-80/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_31/checkpoint-48] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_31/checkpoint-96\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_31/checkpoint-96/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_31/checkpoint-96/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_31/checkpoint-96/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_31/checkpoint-96/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_31/checkpoint-64] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_31/checkpoint-112\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_31/checkpoint-112/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_31/checkpoint-112/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_31/checkpoint-112/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_31/checkpoint-112/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_31/checkpoint-80] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_31/checkpoint-128\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_31/checkpoint-128/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_31/checkpoint-128/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_31/checkpoint-128/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_31/checkpoint-128/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_31/checkpoint-96] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_31/checkpoint-144\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_31/checkpoint-144/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_31/checkpoint-144/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_31/checkpoint-144/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_31/checkpoint-144/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_31/checkpoint-112] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_31/checkpoint-160\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_31/checkpoint-160/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_31/checkpoint-160/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_31/checkpoint-160/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_31/checkpoint-160/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_31/checkpoint-128] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_31/checkpoint-176\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_31/checkpoint-176/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_31/checkpoint-176/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_31/checkpoint-176/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_31/checkpoint-176/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_31/checkpoint-144] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_31/checkpoint-192\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_31/checkpoint-192/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_31/checkpoint-192/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_31/checkpoint-192/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_31/checkpoint-192/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_31/checkpoint-160] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_31/checkpoint-208\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_31/checkpoint-208/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_31/checkpoint-208/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_31/checkpoint-208/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_31/checkpoint-208/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_31/checkpoint-176] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_31/checkpoint-224\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_31/checkpoint-224/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_31/checkpoint-224/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_31/checkpoint-224/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_31/checkpoint-224/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_31/checkpoint-192] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_31/checkpoint-240\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_31/checkpoint-240/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_31/checkpoint-240/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_31/checkpoint-240/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_31/checkpoint-240/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_31/checkpoint-208] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_31/checkpoint-256\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_31/checkpoint-256/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_31/checkpoint-256/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_31/checkpoint-256/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_31/checkpoint-256/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_31/checkpoint-224] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_31/checkpoint-272\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_31/checkpoint-272/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_31/checkpoint-272/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_31/checkpoint-272/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_31/checkpoint-272/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_31/checkpoint-240] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_31/checkpoint-288\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_31/checkpoint-288/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_31/checkpoint-288/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_31/checkpoint-288/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_31/checkpoint-288/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_31/checkpoint-256] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_31/checkpoint-304\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_31/checkpoint-304/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_31/checkpoint-304/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_31/checkpoint-304/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_31/checkpoint-304/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_31/checkpoint-272] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_31/checkpoint-320\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_31/checkpoint-320/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_31/checkpoint-320/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_31/checkpoint-320/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_31/checkpoint-320/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_31/checkpoint-288] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>██▇▇▇▆▆▅▅▄▄▃▃▂▂▂▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▅▄▄▁▄▄▅▄▄▄▄▁█▅▅▄▄▅▄▅</td></tr><tr><td>eval/samples_per_second</td><td>▄▄▅█▄▄▄▄▅▅▅█▁▃▃▄▄▄▅▄</td></tr><tr><td>eval/steps_per_second</td><td>▄▄▅█▄▄▄▄▅▅▅█▁▃▃▄▄▄▅▄</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇█████</td></tr><tr><td>train/learning_rate</td><td>██▇▇▇▆▆▅▅▅▄▄▄▃▃▂▂▂▁▁</td></tr><tr><td>train/loss</td><td>██▇▇▇▆▆▅▅▄▄▃▃▃▂▂▁▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.62997</td></tr><tr><td>eval/runtime</td><td>0.0666</td></tr><tr><td>eval/samples_per_second</td><td>3004.839</td></tr><tr><td>eval/steps_per_second</td><td>30.048</td></tr><tr><td>train/epoch</td><td>20.0</td></tr><tr><td>train/global_step</td><td>320</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.6352</td></tr><tr><td>train/total_flos</td><td>212168160000.0</td></tr><tr><td>train/train_loss</td><td>0.66176</td></tr><tr><td>train/train_runtime</td><td>18.1395</td></tr><tr><td>train/train_samples_per_second</td><td>2205.138</td></tr><tr><td>train/train_steps_per_second</td><td>17.641</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">pos syn style 31</strong>: <a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/ud29rfra\" target=\"_blank\">https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/ud29rfra</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/scratch/data_jz17d/wandb_tmp/wandb/run-20221114_113440-ud29rfra/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ce96e7e041a456699b2685b3ea87d45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30987412d21a4831a44518cab1a5d04b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"/scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 32,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 128,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 128,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 54\n",
      "}\n",
      "\n",
      "loading weights file /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/ and are newly initialized: ['bert.pooler.dense.weight', 'classifier.weight', 'bert.pooler.dense.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch/data_jz17d/wandb_tmp/wandb/run-20221114_113507-1h8a67lx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/1h8a67lx\" target=\"_blank\">pos syn style 32</a></strong> to <a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2000\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 128\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 320\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='320' max='320' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [320/320 00:24, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.692600</td>\n",
       "      <td>0.691461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.691600</td>\n",
       "      <td>0.690318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.690900</td>\n",
       "      <td>0.688963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.689800</td>\n",
       "      <td>0.687388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.688600</td>\n",
       "      <td>0.685591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.686800</td>\n",
       "      <td>0.683455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.684800</td>\n",
       "      <td>0.681077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.682500</td>\n",
       "      <td>0.678181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.680300</td>\n",
       "      <td>0.674973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.677900</td>\n",
       "      <td>0.671364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.674800</td>\n",
       "      <td>0.667489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.672200</td>\n",
       "      <td>0.663265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.669800</td>\n",
       "      <td>0.659411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.666800</td>\n",
       "      <td>0.656022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.665700</td>\n",
       "      <td>0.653509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.663500</td>\n",
       "      <td>0.651301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.661500</td>\n",
       "      <td>0.649676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.661000</td>\n",
       "      <td>0.648539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.660200</td>\n",
       "      <td>0.647895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.659600</td>\n",
       "      <td>0.647669</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_32/checkpoint-16\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_32/checkpoint-16/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_32/checkpoint-16/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_32/checkpoint-16/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_32/checkpoint-16/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_32/checkpoint-32\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_32/checkpoint-32/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_32/checkpoint-32/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_32/checkpoint-32/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_32/checkpoint-32/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_32/checkpoint-48\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_32/checkpoint-48/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_32/checkpoint-48/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_32/checkpoint-48/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_32/checkpoint-48/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_32/checkpoint-16] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_32/checkpoint-64\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_32/checkpoint-64/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_32/checkpoint-64/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_32/checkpoint-64/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_32/checkpoint-64/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_32/checkpoint-32] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_32/checkpoint-80\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_32/checkpoint-80/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_32/checkpoint-80/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_32/checkpoint-80/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_32/checkpoint-80/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_32/checkpoint-48] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_32/checkpoint-96\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_32/checkpoint-96/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_32/checkpoint-96/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_32/checkpoint-96/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_32/checkpoint-96/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_32/checkpoint-64] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_32/checkpoint-112\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_32/checkpoint-112/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_32/checkpoint-112/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_32/checkpoint-112/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_32/checkpoint-112/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_32/checkpoint-80] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_32/checkpoint-128\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_32/checkpoint-128/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_32/checkpoint-128/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_32/checkpoint-128/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_32/checkpoint-128/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_32/checkpoint-96] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_32/checkpoint-144\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_32/checkpoint-144/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_32/checkpoint-144/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_32/checkpoint-144/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_32/checkpoint-144/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_32/checkpoint-112] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_32/checkpoint-160\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_32/checkpoint-160/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_32/checkpoint-160/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_32/checkpoint-160/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_32/checkpoint-160/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_32/checkpoint-128] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_32/checkpoint-176\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_32/checkpoint-176/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_32/checkpoint-176/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_32/checkpoint-176/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_32/checkpoint-176/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_32/checkpoint-144] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_32/checkpoint-192\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_32/checkpoint-192/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_32/checkpoint-192/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_32/checkpoint-192/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_32/checkpoint-192/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_32/checkpoint-160] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_32/checkpoint-208\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_32/checkpoint-208/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_32/checkpoint-208/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_32/checkpoint-208/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_32/checkpoint-208/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_32/checkpoint-176] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_32/checkpoint-224\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_32/checkpoint-224/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_32/checkpoint-224/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_32/checkpoint-224/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_32/checkpoint-224/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_32/checkpoint-192] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_32/checkpoint-240\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_32/checkpoint-240/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_32/checkpoint-240/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_32/checkpoint-240/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_32/checkpoint-240/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_32/checkpoint-208] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_32/checkpoint-256\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_32/checkpoint-256/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_32/checkpoint-256/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_32/checkpoint-256/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_32/checkpoint-256/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_32/checkpoint-224] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_32/checkpoint-272\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_32/checkpoint-272/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_32/checkpoint-272/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_32/checkpoint-272/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_32/checkpoint-272/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_32/checkpoint-240] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_32/checkpoint-288\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_32/checkpoint-288/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_32/checkpoint-288/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_32/checkpoint-288/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_32/checkpoint-288/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_32/checkpoint-256] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_32/checkpoint-304\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_32/checkpoint-304/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_32/checkpoint-304/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_32/checkpoint-304/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_32/checkpoint-304/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_32/checkpoint-272] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_32/checkpoint-320\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_32/checkpoint-320/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_32/checkpoint-320/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_32/checkpoint-320/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_32/checkpoint-320/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_32/checkpoint-288] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>███▇▇▇▆▆▅▅▄▃▃▂▂▂▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▅▅▆▅▅▆▃▆▁▇▇▇▅█▆▇▆▇▂▇</td></tr><tr><td>eval/samples_per_second</td><td>▄▃▃▃▄▃▅▃█▂▂▂▄▁▃▂▃▂▇▂</td></tr><tr><td>eval/steps_per_second</td><td>▄▃▃▃▄▃▅▃█▂▂▂▄▁▃▂▃▂▇▂</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇█████</td></tr><tr><td>train/learning_rate</td><td>██▇▇▇▆▆▅▅▅▄▄▄▃▃▂▂▂▁▁</td></tr><tr><td>train/loss</td><td>███▇▇▇▆▆▅▅▄▄▃▃▂▂▁▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.64767</td></tr><tr><td>eval/runtime</td><td>0.0671</td></tr><tr><td>eval/samples_per_second</td><td>2981.111</td></tr><tr><td>eval/steps_per_second</td><td>29.811</td></tr><tr><td>train/epoch</td><td>20.0</td></tr><tr><td>train/global_step</td><td>320</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.6596</td></tr><tr><td>train/total_flos</td><td>212168160000.0</td></tr><tr><td>train/train_loss</td><td>0.67605</td></tr><tr><td>train/train_runtime</td><td>24.7353</td></tr><tr><td>train/train_samples_per_second</td><td>1617.12</td></tr><tr><td>train/train_steps_per_second</td><td>12.937</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">pos syn style 32</strong>: <a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/1h8a67lx\" target=\"_blank\">https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/1h8a67lx</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/scratch/data_jz17d/wandb_tmp/wandb/run-20221114_113507-1h8a67lx/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "730050ce49a1430982331be32e808200",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76131db5cbcf41daacf6928e0949b59d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"/scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 32,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 128,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 128,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 54\n",
      "}\n",
      "\n",
      "loading weights file /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/ and are newly initialized: ['bert.pooler.dense.weight', 'classifier.weight', 'bert.pooler.dense.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch/data_jz17d/wandb_tmp/wandb/run-20221114_113539-243alans</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/243alans\" target=\"_blank\">pos syn style 33</a></strong> to <a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2000\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 128\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 320\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='320' max='320' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [320/320 00:16, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.692900</td>\n",
       "      <td>0.692532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.692700</td>\n",
       "      <td>0.692349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.692600</td>\n",
       "      <td>0.692165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.692500</td>\n",
       "      <td>0.691996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.692600</td>\n",
       "      <td>0.691842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.692300</td>\n",
       "      <td>0.691691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.692300</td>\n",
       "      <td>0.691564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.692100</td>\n",
       "      <td>0.691435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.692100</td>\n",
       "      <td>0.691314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.692000</td>\n",
       "      <td>0.691213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.691900</td>\n",
       "      <td>0.691121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.691900</td>\n",
       "      <td>0.691035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.691800</td>\n",
       "      <td>0.690956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.691700</td>\n",
       "      <td>0.690889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.691700</td>\n",
       "      <td>0.690836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.691700</td>\n",
       "      <td>0.690784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.691800</td>\n",
       "      <td>0.690750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.691600</td>\n",
       "      <td>0.690723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.691700</td>\n",
       "      <td>0.690710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.691700</td>\n",
       "      <td>0.690705</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_33/checkpoint-16\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_33/checkpoint-16/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_33/checkpoint-16/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_33/checkpoint-16/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_33/checkpoint-16/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_33/checkpoint-32\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_33/checkpoint-32/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_33/checkpoint-32/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_33/checkpoint-32/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_33/checkpoint-32/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_33/checkpoint-48\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_33/checkpoint-48/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_33/checkpoint-48/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_33/checkpoint-48/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_33/checkpoint-48/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_33/checkpoint-16] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_33/checkpoint-64\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_33/checkpoint-64/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_33/checkpoint-64/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_33/checkpoint-64/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_33/checkpoint-64/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_33/checkpoint-32] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_33/checkpoint-80\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_33/checkpoint-80/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_33/checkpoint-80/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_33/checkpoint-80/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_33/checkpoint-80/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_33/checkpoint-48] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_33/checkpoint-96\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_33/checkpoint-96/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_33/checkpoint-96/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_33/checkpoint-96/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_33/checkpoint-96/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_33/checkpoint-64] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_33/checkpoint-112\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_33/checkpoint-112/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_33/checkpoint-112/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_33/checkpoint-112/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_33/checkpoint-112/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_33/checkpoint-80] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_33/checkpoint-128\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_33/checkpoint-128/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_33/checkpoint-128/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_33/checkpoint-128/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_33/checkpoint-128/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_33/checkpoint-96] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_33/checkpoint-144\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_33/checkpoint-144/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_33/checkpoint-144/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_33/checkpoint-144/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_33/checkpoint-144/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_33/checkpoint-112] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_33/checkpoint-160\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_33/checkpoint-160/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_33/checkpoint-160/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_33/checkpoint-160/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_33/checkpoint-160/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_33/checkpoint-128] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_33/checkpoint-176\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_33/checkpoint-176/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_33/checkpoint-176/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_33/checkpoint-176/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_33/checkpoint-176/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_33/checkpoint-144] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_33/checkpoint-192\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_33/checkpoint-192/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_33/checkpoint-192/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_33/checkpoint-192/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_33/checkpoint-192/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_33/checkpoint-160] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_33/checkpoint-208\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_33/checkpoint-208/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_33/checkpoint-208/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_33/checkpoint-208/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_33/checkpoint-208/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_33/checkpoint-176] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_33/checkpoint-224\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_33/checkpoint-224/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_33/checkpoint-224/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_33/checkpoint-224/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_33/checkpoint-224/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_33/checkpoint-192] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_33/checkpoint-240\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_33/checkpoint-240/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_33/checkpoint-240/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_33/checkpoint-240/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_33/checkpoint-240/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_33/checkpoint-208] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_33/checkpoint-256\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_33/checkpoint-256/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_33/checkpoint-256/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_33/checkpoint-256/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_33/checkpoint-256/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_33/checkpoint-224] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_33/checkpoint-272\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_33/checkpoint-272/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_33/checkpoint-272/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_33/checkpoint-272/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_33/checkpoint-272/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_33/checkpoint-240] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_33/checkpoint-288\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_33/checkpoint-288/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_33/checkpoint-288/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_33/checkpoint-288/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_33/checkpoint-288/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_33/checkpoint-256] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_33/checkpoint-304\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_33/checkpoint-304/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_33/checkpoint-304/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_33/checkpoint-304/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_33/checkpoint-304/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_33/checkpoint-272] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_33/checkpoint-320\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_33/checkpoint-320/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_33/checkpoint-320/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_33/checkpoint-320/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_33/checkpoint-320/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_33/checkpoint-288] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▇▇▆▅▅▄▄▃▃▃▂▂▂▂▁▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▁▆▆▇▇▄▅▆▇▇▆▇▆▂█▁▅▅▆▇</td></tr><tr><td>eval/samples_per_second</td><td>▇▃▂▂▂▅▃▃▂▂▃▂▃▇▁█▃▃▂▂</td></tr><tr><td>eval/steps_per_second</td><td>▇▃▂▂▂▅▃▃▂▂▃▂▃▇▁█▃▃▂▂</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇█████</td></tr><tr><td>train/learning_rate</td><td>██▇▇▇▆▆▅▅▅▄▄▄▃▃▂▂▂▁▁</td></tr><tr><td>train/loss</td><td>█▇▆▆▆▅▅▄▄▃▃▃▂▂▂▂▂▁▂▂</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.6907</td></tr><tr><td>eval/runtime</td><td>0.068</td></tr><tr><td>eval/samples_per_second</td><td>2940.719</td></tr><tr><td>eval/steps_per_second</td><td>29.407</td></tr><tr><td>train/epoch</td><td>20.0</td></tr><tr><td>train/global_step</td><td>320</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.6917</td></tr><tr><td>train/total_flos</td><td>212168160000.0</td></tr><tr><td>train/train_loss</td><td>0.69209</td></tr><tr><td>train/train_runtime</td><td>16.0929</td></tr><tr><td>train/train_samples_per_second</td><td>2485.565</td></tr><tr><td>train/train_steps_per_second</td><td>19.885</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">pos syn style 33</strong>: <a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/243alans\" target=\"_blank\">https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/243alans</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/scratch/data_jz17d/wandb_tmp/wandb/run-20221114_113539-243alans/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4154ee302d3b49e1bbf344981f84a952",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "607fc51a9e69486589e0ebf87ee79870",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"/scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 32,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 128,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 128,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 54\n",
      "}\n",
      "\n",
      "loading weights file /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/ and are newly initialized: ['bert.pooler.dense.weight', 'classifier.weight', 'bert.pooler.dense.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch/data_jz17d/wandb_tmp/wandb/run-20221114_113604-wkrj1eik</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/wkrj1eik\" target=\"_blank\">pos syn style 34</a></strong> to <a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2000\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 128\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 320\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='320' max='320' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [320/320 00:19, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.692600</td>\n",
       "      <td>0.691696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.691900</td>\n",
       "      <td>0.690767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.691300</td>\n",
       "      <td>0.689722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.690500</td>\n",
       "      <td>0.688607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.689900</td>\n",
       "      <td>0.687405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.688600</td>\n",
       "      <td>0.686051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.687600</td>\n",
       "      <td>0.684636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.686200</td>\n",
       "      <td>0.682993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.685000</td>\n",
       "      <td>0.681276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.683900</td>\n",
       "      <td>0.679550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.682400</td>\n",
       "      <td>0.677792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.680700</td>\n",
       "      <td>0.676010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.679600</td>\n",
       "      <td>0.674352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.678000</td>\n",
       "      <td>0.672831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.677000</td>\n",
       "      <td>0.671515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.676000</td>\n",
       "      <td>0.670325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.675500</td>\n",
       "      <td>0.669446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.674100</td>\n",
       "      <td>0.668768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.674000</td>\n",
       "      <td>0.668385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.673800</td>\n",
       "      <td>0.668249</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_34/checkpoint-16\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_34/checkpoint-16/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_34/checkpoint-16/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_34/checkpoint-16/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_34/checkpoint-16/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_34/checkpoint-32\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_34/checkpoint-32/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_34/checkpoint-32/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_34/checkpoint-32/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_34/checkpoint-32/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_34/checkpoint-48\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_34/checkpoint-48/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_34/checkpoint-48/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_34/checkpoint-48/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_34/checkpoint-48/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_34/checkpoint-16] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_34/checkpoint-64\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_34/checkpoint-64/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_34/checkpoint-64/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_34/checkpoint-64/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_34/checkpoint-64/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_34/checkpoint-32] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_34/checkpoint-80\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_34/checkpoint-80/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_34/checkpoint-80/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_34/checkpoint-80/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_34/checkpoint-80/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_34/checkpoint-48] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_34/checkpoint-96\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_34/checkpoint-96/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_34/checkpoint-96/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_34/checkpoint-96/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_34/checkpoint-96/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_34/checkpoint-64] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_34/checkpoint-112\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_34/checkpoint-112/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_34/checkpoint-112/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_34/checkpoint-112/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_34/checkpoint-112/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_34/checkpoint-80] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_34/checkpoint-128\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_34/checkpoint-128/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_34/checkpoint-128/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_34/checkpoint-128/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_34/checkpoint-128/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_34/checkpoint-96] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_34/checkpoint-144\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_34/checkpoint-144/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_34/checkpoint-144/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_34/checkpoint-144/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_34/checkpoint-144/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_34/checkpoint-112] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_34/checkpoint-160\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_34/checkpoint-160/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_34/checkpoint-160/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_34/checkpoint-160/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_34/checkpoint-160/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_34/checkpoint-128] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_34/checkpoint-176\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_34/checkpoint-176/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_34/checkpoint-176/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_34/checkpoint-176/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_34/checkpoint-176/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_34/checkpoint-144] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_34/checkpoint-192\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_34/checkpoint-192/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_34/checkpoint-192/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_34/checkpoint-192/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_34/checkpoint-192/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_34/checkpoint-160] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_34/checkpoint-208\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_34/checkpoint-208/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_34/checkpoint-208/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_34/checkpoint-208/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_34/checkpoint-208/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_34/checkpoint-176] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_34/checkpoint-224\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_34/checkpoint-224/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_34/checkpoint-224/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_34/checkpoint-224/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_34/checkpoint-224/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_34/checkpoint-192] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_34/checkpoint-240\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_34/checkpoint-240/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_34/checkpoint-240/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_34/checkpoint-240/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_34/checkpoint-240/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_34/checkpoint-208] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_34/checkpoint-256\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_34/checkpoint-256/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_34/checkpoint-256/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_34/checkpoint-256/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_34/checkpoint-256/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_34/checkpoint-224] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_34/checkpoint-272\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_34/checkpoint-272/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_34/checkpoint-272/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_34/checkpoint-272/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_34/checkpoint-272/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_34/checkpoint-240] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_34/checkpoint-288\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_34/checkpoint-288/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_34/checkpoint-288/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_34/checkpoint-288/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_34/checkpoint-288/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_34/checkpoint-256] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_34/checkpoint-304\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_34/checkpoint-304/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_34/checkpoint-304/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_34/checkpoint-304/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_34/checkpoint-304/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_34/checkpoint-272] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_34/checkpoint-320\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_34/checkpoint-320/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_34/checkpoint-320/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_34/checkpoint-320/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_34/checkpoint-320/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_34/checkpoint-288] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>██▇▇▇▆▆▅▅▄▄▃▃▂▂▂▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▄▄▁▁▄▁▅▃▄▄▄▅▄▄█▃▄▄▃▄</td></tr><tr><td>eval/samples_per_second</td><td>▅▄▇█▄▇▃▅▅▅▅▄▄▄▁▅▄▄▅▅</td></tr><tr><td>eval/steps_per_second</td><td>▅▄▇█▄▇▃▅▅▅▅▄▄▄▁▅▄▄▅▅</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇█████</td></tr><tr><td>train/learning_rate</td><td>██▇▇▇▆▆▅▅▅▄▄▄▃▃▂▂▂▁▁</td></tr><tr><td>train/loss</td><td>███▇▇▇▆▆▅▅▄▄▃▃▂▂▂▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.66825</td></tr><tr><td>eval/runtime</td><td>0.0651</td></tr><tr><td>eval/samples_per_second</td><td>3070.366</td></tr><tr><td>eval/steps_per_second</td><td>30.704</td></tr><tr><td>train/epoch</td><td>20.0</td></tr><tr><td>train/global_step</td><td>320</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.6738</td></tr><tr><td>train/total_flos</td><td>212168160000.0</td></tr><tr><td>train/train_loss</td><td>0.68294</td></tr><tr><td>train/train_runtime</td><td>19.8065</td></tr><tr><td>train/train_samples_per_second</td><td>2019.543</td></tr><tr><td>train/train_steps_per_second</td><td>16.156</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">pos syn style 34</strong>: <a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/wkrj1eik\" target=\"_blank\">https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/wkrj1eik</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/scratch/data_jz17d/wandb_tmp/wandb/run-20221114_113604-wkrj1eik/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf0391c036a348b1beedd1ad7bc0651d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f12de12951c423b925f14e9716603ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"/scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 32,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 128,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 128,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 54\n",
      "}\n",
      "\n",
      "loading weights file /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/ and are newly initialized: ['bert.pooler.dense.weight', 'classifier.weight', 'bert.pooler.dense.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch/data_jz17d/wandb_tmp/wandb/run-20221114_113633-2bfv5q9v</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/2bfv5q9v\" target=\"_blank\">pos syn style 35</a></strong> to <a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2000\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 128\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 320\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='320' max='320' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [320/320 00:17, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.692700</td>\n",
       "      <td>0.691849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.692000</td>\n",
       "      <td>0.691079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.691600</td>\n",
       "      <td>0.690262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.691000</td>\n",
       "      <td>0.689442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.690600</td>\n",
       "      <td>0.688610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.689700</td>\n",
       "      <td>0.687710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.689100</td>\n",
       "      <td>0.686840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.688300</td>\n",
       "      <td>0.685857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.687700</td>\n",
       "      <td>0.684835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.687200</td>\n",
       "      <td>0.683876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.686300</td>\n",
       "      <td>0.682921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.685400</td>\n",
       "      <td>0.681962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.684900</td>\n",
       "      <td>0.681054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.684000</td>\n",
       "      <td>0.680233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.683400</td>\n",
       "      <td>0.679523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.683200</td>\n",
       "      <td>0.678875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.683000</td>\n",
       "      <td>0.678415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.682100</td>\n",
       "      <td>0.678051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.682300</td>\n",
       "      <td>0.677850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.681900</td>\n",
       "      <td>0.677781</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_35/checkpoint-16\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_35/checkpoint-16/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_35/checkpoint-16/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_35/checkpoint-16/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_35/checkpoint-16/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_35/checkpoint-32\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_35/checkpoint-32/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_35/checkpoint-32/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_35/checkpoint-32/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_35/checkpoint-32/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_35/checkpoint-48\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_35/checkpoint-48/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_35/checkpoint-48/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_35/checkpoint-48/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_35/checkpoint-48/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_35/checkpoint-16] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_35/checkpoint-64\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_35/checkpoint-64/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_35/checkpoint-64/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_35/checkpoint-64/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_35/checkpoint-64/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_35/checkpoint-32] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_35/checkpoint-80\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_35/checkpoint-80/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_35/checkpoint-80/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_35/checkpoint-80/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_35/checkpoint-80/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_35/checkpoint-48] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_35/checkpoint-96\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_35/checkpoint-96/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_35/checkpoint-96/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_35/checkpoint-96/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_35/checkpoint-96/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_35/checkpoint-64] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_35/checkpoint-112\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_35/checkpoint-112/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_35/checkpoint-112/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_35/checkpoint-112/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_35/checkpoint-112/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_35/checkpoint-80] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_35/checkpoint-128\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_35/checkpoint-128/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_35/checkpoint-128/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_35/checkpoint-128/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_35/checkpoint-128/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_35/checkpoint-96] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_35/checkpoint-144\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_35/checkpoint-144/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_35/checkpoint-144/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_35/checkpoint-144/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_35/checkpoint-144/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_35/checkpoint-112] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_35/checkpoint-160\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_35/checkpoint-160/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_35/checkpoint-160/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_35/checkpoint-160/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_35/checkpoint-160/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_35/checkpoint-128] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_35/checkpoint-176\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_35/checkpoint-176/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_35/checkpoint-176/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_35/checkpoint-176/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_35/checkpoint-176/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_35/checkpoint-144] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_35/checkpoint-192\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_35/checkpoint-192/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_35/checkpoint-192/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_35/checkpoint-192/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_35/checkpoint-192/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_35/checkpoint-160] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_35/checkpoint-208\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_35/checkpoint-208/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_35/checkpoint-208/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_35/checkpoint-208/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_35/checkpoint-208/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_35/checkpoint-176] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_35/checkpoint-224\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_35/checkpoint-224/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_35/checkpoint-224/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_35/checkpoint-224/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_35/checkpoint-224/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_35/checkpoint-192] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_35/checkpoint-240\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_35/checkpoint-240/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_35/checkpoint-240/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_35/checkpoint-240/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_35/checkpoint-240/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_35/checkpoint-208] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_35/checkpoint-256\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_35/checkpoint-256/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_35/checkpoint-256/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_35/checkpoint-256/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_35/checkpoint-256/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_35/checkpoint-224] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_35/checkpoint-272\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_35/checkpoint-272/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_35/checkpoint-272/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_35/checkpoint-272/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_35/checkpoint-272/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_35/checkpoint-240] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_35/checkpoint-288\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_35/checkpoint-288/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_35/checkpoint-288/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_35/checkpoint-288/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_35/checkpoint-288/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_35/checkpoint-256] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_35/checkpoint-304\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_35/checkpoint-304/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_35/checkpoint-304/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_35/checkpoint-304/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_35/checkpoint-304/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_35/checkpoint-272] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_35/checkpoint-320\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_35/checkpoint-320/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_35/checkpoint-320/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_35/checkpoint-320/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_35/checkpoint-320/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_35/checkpoint-288] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>██▇▇▆▆▆▅▅▄▄▃▃▂▂▂▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▁▆▅▄▅▂▄▄▆▅▁▄▆▁▆█▄▅▆▆</td></tr><tr><td>eval/samples_per_second</td><td>█▃▄▄▄▆▄▅▃▄▇▅▃█▃▁▄▄▂▃</td></tr><tr><td>eval/steps_per_second</td><td>█▃▄▄▄▆▄▅▃▄▇▅▃█▃▁▄▄▂▃</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇█████</td></tr><tr><td>train/learning_rate</td><td>██▇▇▇▆▆▅▅▅▄▄▄▃▃▂▂▂▁▁</td></tr><tr><td>train/loss</td><td>██▇▇▇▆▆▅▅▄▄▃▃▂▂▂▂▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.67778</td></tr><tr><td>eval/runtime</td><td>0.0681</td></tr><tr><td>eval/samples_per_second</td><td>2938.268</td></tr><tr><td>eval/steps_per_second</td><td>29.383</td></tr><tr><td>train/epoch</td><td>20.0</td></tr><tr><td>train/global_step</td><td>320</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.6819</td></tr><tr><td>train/total_flos</td><td>212168160000.0</td></tr><tr><td>train/train_loss</td><td>0.68683</td></tr><tr><td>train/train_runtime</td><td>18.0429</td></tr><tr><td>train/train_samples_per_second</td><td>2216.942</td></tr><tr><td>train/train_steps_per_second</td><td>17.736</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">pos syn style 35</strong>: <a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/2bfv5q9v\" target=\"_blank\">https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/2bfv5q9v</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/scratch/data_jz17d/wandb_tmp/wandb/run-20221114_113633-2bfv5q9v/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28a0bdfeb7e04fc9abfaab183c840654",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d758a73d8ae46199e630c1769fc069e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"/scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 32,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 128,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 128,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 54\n",
      "}\n",
      "\n",
      "loading weights file /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/ and are newly initialized: ['bert.pooler.dense.weight', 'classifier.weight', 'bert.pooler.dense.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch/data_jz17d/wandb_tmp/wandb/run-20221114_113700-wyqvs49w</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/wyqvs49w\" target=\"_blank\">pos syn style 36</a></strong> to <a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2000\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 128\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 320\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='320' max='320' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [320/320 00:24, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.692200</td>\n",
       "      <td>0.690217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.690300</td>\n",
       "      <td>0.688100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.688300</td>\n",
       "      <td>0.685432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.685500</td>\n",
       "      <td>0.681982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.682000</td>\n",
       "      <td>0.677450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.677300</td>\n",
       "      <td>0.671771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.671600</td>\n",
       "      <td>0.664850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.665300</td>\n",
       "      <td>0.656767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.658700</td>\n",
       "      <td>0.648127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.651700</td>\n",
       "      <td>0.640059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.644200</td>\n",
       "      <td>0.632430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.638200</td>\n",
       "      <td>0.625523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.632300</td>\n",
       "      <td>0.619257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.625900</td>\n",
       "      <td>0.613778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.623300</td>\n",
       "      <td>0.609166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.617400</td>\n",
       "      <td>0.605371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.615200</td>\n",
       "      <td>0.602444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.612700</td>\n",
       "      <td>0.600339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.611800</td>\n",
       "      <td>0.599104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.610300</td>\n",
       "      <td>0.598688</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_36/checkpoint-16\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_36/checkpoint-16/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_36/checkpoint-16/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_36/checkpoint-16/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_36/checkpoint-16/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_36/checkpoint-32\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_36/checkpoint-32/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_36/checkpoint-32/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_36/checkpoint-32/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_36/checkpoint-32/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_36/checkpoint-48\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_36/checkpoint-48/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_36/checkpoint-48/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_36/checkpoint-48/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_36/checkpoint-48/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_36/checkpoint-16] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_36/checkpoint-64\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_36/checkpoint-64/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_36/checkpoint-64/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_36/checkpoint-64/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_36/checkpoint-64/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_36/checkpoint-32] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_36/checkpoint-80\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_36/checkpoint-80/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_36/checkpoint-80/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_36/checkpoint-80/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_36/checkpoint-80/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_36/checkpoint-48] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_36/checkpoint-96\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_36/checkpoint-96/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_36/checkpoint-96/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_36/checkpoint-96/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_36/checkpoint-96/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_36/checkpoint-64] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_36/checkpoint-112\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_36/checkpoint-112/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_36/checkpoint-112/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_36/checkpoint-112/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_36/checkpoint-112/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_36/checkpoint-80] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_36/checkpoint-128\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_36/checkpoint-128/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_36/checkpoint-128/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_36/checkpoint-128/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_36/checkpoint-128/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_36/checkpoint-96] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_36/checkpoint-144\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_36/checkpoint-144/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_36/checkpoint-144/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_36/checkpoint-144/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_36/checkpoint-144/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_36/checkpoint-112] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_36/checkpoint-160\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_36/checkpoint-160/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_36/checkpoint-160/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_36/checkpoint-160/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_36/checkpoint-160/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_36/checkpoint-128] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_36/checkpoint-176\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_36/checkpoint-176/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_36/checkpoint-176/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_36/checkpoint-176/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_36/checkpoint-176/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_36/checkpoint-144] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_36/checkpoint-192\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_36/checkpoint-192/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_36/checkpoint-192/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_36/checkpoint-192/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_36/checkpoint-192/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_36/checkpoint-160] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_36/checkpoint-208\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_36/checkpoint-208/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_36/checkpoint-208/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_36/checkpoint-208/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_36/checkpoint-208/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_36/checkpoint-176] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_36/checkpoint-224\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_36/checkpoint-224/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_36/checkpoint-224/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_36/checkpoint-224/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_36/checkpoint-224/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_36/checkpoint-192] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_36/checkpoint-240\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_36/checkpoint-240/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_36/checkpoint-240/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_36/checkpoint-240/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_36/checkpoint-240/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_36/checkpoint-208] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_36/checkpoint-256\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_36/checkpoint-256/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_36/checkpoint-256/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_36/checkpoint-256/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_36/checkpoint-256/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_36/checkpoint-224] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_36/checkpoint-272\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_36/checkpoint-272/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_36/checkpoint-272/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_36/checkpoint-272/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_36/checkpoint-272/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_36/checkpoint-240] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_36/checkpoint-288\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_36/checkpoint-288/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_36/checkpoint-288/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_36/checkpoint-288/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_36/checkpoint-288/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_36/checkpoint-256] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_36/checkpoint-304\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_36/checkpoint-304/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_36/checkpoint-304/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_36/checkpoint-304/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_36/checkpoint-304/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_36/checkpoint-272] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_36/checkpoint-320\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_36/checkpoint-320/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_36/checkpoint-320/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_36/checkpoint-320/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_36/checkpoint-320/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_36/checkpoint-288] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>███▇▇▇▆▅▅▄▄▃▃▂▂▂▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▅▅▁▁▅▁▅▂▄▄▅▅▇▅▄▅▅█▆▂</td></tr><tr><td>eval/samples_per_second</td><td>▄▄▇█▄█▄▇▅▄▄▄▂▃▄▄▄▁▃▇</td></tr><tr><td>eval/steps_per_second</td><td>▄▄▇█▄█▄▇▅▄▄▄▂▃▄▄▄▁▃▇</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇█████</td></tr><tr><td>train/learning_rate</td><td>██▇▇▇▆▆▅▅▅▄▄▄▃▃▂▂▂▁▁</td></tr><tr><td>train/loss</td><td>███▇▇▇▆▆▅▅▄▃▃▂▂▂▁▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.59869</td></tr><tr><td>eval/runtime</td><td>0.06</td></tr><tr><td>eval/samples_per_second</td><td>3334.065</td></tr><tr><td>eval/steps_per_second</td><td>33.341</td></tr><tr><td>train/epoch</td><td>20.0</td></tr><tr><td>train/global_step</td><td>320</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.6103</td></tr><tr><td>train/total_flos</td><td>212168160000.0</td></tr><tr><td>train/train_loss</td><td>0.64972</td></tr><tr><td>train/train_runtime</td><td>24.9842</td></tr><tr><td>train/train_samples_per_second</td><td>1601.011</td></tr><tr><td>train/train_steps_per_second</td><td>12.808</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">pos syn style 36</strong>: <a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/wyqvs49w\" target=\"_blank\">https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/wyqvs49w</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/scratch/data_jz17d/wandb_tmp/wandb/run-20221114_113700-wyqvs49w/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e81e9d8a861f444383c3ab6607beb9f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe7e2268ee9e489fbb773f0df3094400",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"/scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 32,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 128,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 128,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 54\n",
      "}\n",
      "\n",
      "loading weights file /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/ and are newly initialized: ['bert.pooler.dense.weight', 'classifier.weight', 'bert.pooler.dense.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch/data_jz17d/wandb_tmp/wandb/run-20221114_113734-2gqxruvz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/2gqxruvz\" target=\"_blank\">pos syn style 37</a></strong> to <a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2000\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 128\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 320\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='320' max='320' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [320/320 00:16, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.693000</td>\n",
       "      <td>0.691972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.692600</td>\n",
       "      <td>0.691695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.692500</td>\n",
       "      <td>0.691423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.692300</td>\n",
       "      <td>0.691171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.692100</td>\n",
       "      <td>0.690933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.691800</td>\n",
       "      <td>0.690708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.691700</td>\n",
       "      <td>0.690509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.691400</td>\n",
       "      <td>0.690320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.691300</td>\n",
       "      <td>0.690144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.691100</td>\n",
       "      <td>0.689987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.690900</td>\n",
       "      <td>0.689846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.690800</td>\n",
       "      <td>0.689719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.690700</td>\n",
       "      <td>0.689600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.690600</td>\n",
       "      <td>0.689499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.690500</td>\n",
       "      <td>0.689418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.690500</td>\n",
       "      <td>0.689344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.690500</td>\n",
       "      <td>0.689292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.690300</td>\n",
       "      <td>0.689253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.690500</td>\n",
       "      <td>0.689231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.690400</td>\n",
       "      <td>0.689223</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_37/checkpoint-16\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_37/checkpoint-16/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_37/checkpoint-16/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_37/checkpoint-16/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_37/checkpoint-16/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_37/checkpoint-32\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_37/checkpoint-32/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_37/checkpoint-32/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_37/checkpoint-32/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_37/checkpoint-32/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_37/checkpoint-48\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_37/checkpoint-48/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_37/checkpoint-48/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_37/checkpoint-48/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_37/checkpoint-48/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_37/checkpoint-16] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_37/checkpoint-64\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_37/checkpoint-64/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_37/checkpoint-64/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_37/checkpoint-64/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_37/checkpoint-64/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_37/checkpoint-32] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_37/checkpoint-80\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_37/checkpoint-80/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_37/checkpoint-80/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_37/checkpoint-80/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_37/checkpoint-80/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_37/checkpoint-48] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_37/checkpoint-96\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_37/checkpoint-96/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_37/checkpoint-96/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_37/checkpoint-96/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_37/checkpoint-96/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_37/checkpoint-64] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_37/checkpoint-112\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_37/checkpoint-112/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_37/checkpoint-112/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_37/checkpoint-112/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_37/checkpoint-112/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_37/checkpoint-80] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_37/checkpoint-128\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_37/checkpoint-128/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_37/checkpoint-128/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_37/checkpoint-128/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_37/checkpoint-128/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_37/checkpoint-96] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_37/checkpoint-144\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_37/checkpoint-144/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_37/checkpoint-144/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_37/checkpoint-144/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_37/checkpoint-144/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_37/checkpoint-112] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_37/checkpoint-160\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_37/checkpoint-160/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_37/checkpoint-160/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_37/checkpoint-160/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_37/checkpoint-160/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_37/checkpoint-128] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_37/checkpoint-176\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_37/checkpoint-176/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_37/checkpoint-176/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_37/checkpoint-176/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_37/checkpoint-176/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_37/checkpoint-144] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_37/checkpoint-192\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_37/checkpoint-192/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_37/checkpoint-192/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_37/checkpoint-192/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_37/checkpoint-192/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_37/checkpoint-160] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_37/checkpoint-208\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_37/checkpoint-208/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_37/checkpoint-208/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_37/checkpoint-208/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_37/checkpoint-208/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_37/checkpoint-176] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_37/checkpoint-224\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_37/checkpoint-224/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_37/checkpoint-224/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_37/checkpoint-224/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_37/checkpoint-224/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_37/checkpoint-192] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_37/checkpoint-240\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_37/checkpoint-240/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_37/checkpoint-240/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_37/checkpoint-240/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_37/checkpoint-240/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_37/checkpoint-208] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_37/checkpoint-256\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_37/checkpoint-256/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_37/checkpoint-256/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_37/checkpoint-256/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_37/checkpoint-256/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_37/checkpoint-224] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_37/checkpoint-272\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_37/checkpoint-272/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_37/checkpoint-272/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_37/checkpoint-272/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_37/checkpoint-272/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_37/checkpoint-240] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_37/checkpoint-288\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_37/checkpoint-288/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_37/checkpoint-288/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_37/checkpoint-288/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_37/checkpoint-288/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_37/checkpoint-256] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_37/checkpoint-304\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_37/checkpoint-304/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_37/checkpoint-304/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_37/checkpoint-304/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_37/checkpoint-304/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_37/checkpoint-272] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_37/checkpoint-320\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_37/checkpoint-320/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_37/checkpoint-320/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_37/checkpoint-320/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_37/checkpoint-320/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_37/checkpoint-288] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▇▇▆▅▅▄▄▃▃▃▂▂▂▁▁▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▂▇▅▄█▄▇▇▅▅▄▄▅▄▃▃▁▄▄▃</td></tr><tr><td>eval/samples_per_second</td><td>▇▂▄▅▁▄▂▂▄▄▅▅▄▄▅▆█▅▄▅</td></tr><tr><td>eval/steps_per_second</td><td>▇▂▄▅▁▄▂▂▄▄▅▅▄▄▅▆█▅▄▅</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇█████</td></tr><tr><td>train/learning_rate</td><td>██▇▇▇▆▆▅▅▅▄▄▄▃▃▂▂▂▁▁</td></tr><tr><td>train/loss</td><td>█▇▇▆▆▅▅▄▄▃▃▂▂▂▂▂▂▁▂▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.68922</td></tr><tr><td>eval/runtime</td><td>0.0654</td></tr><tr><td>eval/samples_per_second</td><td>3060.24</td></tr><tr><td>eval/steps_per_second</td><td>30.602</td></tr><tr><td>train/epoch</td><td>20.0</td></tr><tr><td>train/global_step</td><td>320</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.6904</td></tr><tr><td>train/total_flos</td><td>212168160000.0</td></tr><tr><td>train/train_loss</td><td>0.69127</td></tr><tr><td>train/train_runtime</td><td>16.1909</td></tr><tr><td>train/train_samples_per_second</td><td>2470.517</td></tr><tr><td>train/train_steps_per_second</td><td>19.764</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">pos syn style 37</strong>: <a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/2gqxruvz\" target=\"_blank\">https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/2gqxruvz</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/scratch/data_jz17d/wandb_tmp/wandb/run-20221114_113734-2gqxruvz/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f4648435bef409caae25ccb8308058e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90f69ad774c240f18c22c7b65277bb73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"/scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 32,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 128,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 128,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 54\n",
      "}\n",
      "\n",
      "loading weights file /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/ and are newly initialized: ['bert.pooler.dense.weight', 'classifier.weight', 'bert.pooler.dense.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch/data_jz17d/wandb_tmp/wandb/run-20221114_113758-3trmnuoj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/3trmnuoj\" target=\"_blank\">pos syn style 38</a></strong> to <a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2000\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 128\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 320\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='320' max='320' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [320/320 00:20, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.692400</td>\n",
       "      <td>0.690623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.690800</td>\n",
       "      <td>0.688988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.689500</td>\n",
       "      <td>0.687119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.687700</td>\n",
       "      <td>0.684950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.685500</td>\n",
       "      <td>0.682356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.682900</td>\n",
       "      <td>0.679358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>0.676004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.676500</td>\n",
       "      <td>0.672169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.673400</td>\n",
       "      <td>0.668085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.669600</td>\n",
       "      <td>0.663773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.665700</td>\n",
       "      <td>0.659354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.661300</td>\n",
       "      <td>0.654993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.657300</td>\n",
       "      <td>0.650809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.653100</td>\n",
       "      <td>0.646948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.650600</td>\n",
       "      <td>0.643593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.647300</td>\n",
       "      <td>0.640759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.646100</td>\n",
       "      <td>0.638509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.642600</td>\n",
       "      <td>0.636860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.643200</td>\n",
       "      <td>0.635889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.641800</td>\n",
       "      <td>0.635557</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_38/checkpoint-16\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_38/checkpoint-16/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_38/checkpoint-16/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_38/checkpoint-16/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_38/checkpoint-16/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_38/checkpoint-32\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_38/checkpoint-32/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_38/checkpoint-32/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_38/checkpoint-32/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_38/checkpoint-32/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_38/checkpoint-48\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_38/checkpoint-48/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_38/checkpoint-48/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_38/checkpoint-48/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_38/checkpoint-48/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_38/checkpoint-16] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_38/checkpoint-64\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_38/checkpoint-64/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_38/checkpoint-64/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_38/checkpoint-64/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_38/checkpoint-64/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_38/checkpoint-32] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_38/checkpoint-80\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_38/checkpoint-80/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_38/checkpoint-80/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_38/checkpoint-80/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_38/checkpoint-80/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_38/checkpoint-48] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_38/checkpoint-96\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_38/checkpoint-96/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_38/checkpoint-96/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_38/checkpoint-96/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_38/checkpoint-96/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_38/checkpoint-64] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_38/checkpoint-112\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_38/checkpoint-112/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_38/checkpoint-112/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_38/checkpoint-112/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_38/checkpoint-112/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_38/checkpoint-80] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_38/checkpoint-128\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_38/checkpoint-128/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_38/checkpoint-128/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_38/checkpoint-128/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_38/checkpoint-128/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_38/checkpoint-96] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_38/checkpoint-144\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_38/checkpoint-144/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_38/checkpoint-144/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_38/checkpoint-144/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_38/checkpoint-144/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_38/checkpoint-112] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_38/checkpoint-160\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_38/checkpoint-160/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_38/checkpoint-160/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_38/checkpoint-160/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_38/checkpoint-160/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_38/checkpoint-128] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_38/checkpoint-176\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_38/checkpoint-176/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_38/checkpoint-176/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_38/checkpoint-176/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_38/checkpoint-176/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_38/checkpoint-144] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_38/checkpoint-192\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_38/checkpoint-192/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_38/checkpoint-192/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_38/checkpoint-192/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_38/checkpoint-192/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_38/checkpoint-160] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_38/checkpoint-208\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_38/checkpoint-208/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_38/checkpoint-208/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_38/checkpoint-208/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_38/checkpoint-208/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_38/checkpoint-176] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_38/checkpoint-224\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_38/checkpoint-224/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_38/checkpoint-224/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_38/checkpoint-224/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_38/checkpoint-224/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_38/checkpoint-192] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_38/checkpoint-240\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_38/checkpoint-240/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_38/checkpoint-240/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_38/checkpoint-240/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_38/checkpoint-240/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_38/checkpoint-208] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_38/checkpoint-256\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_38/checkpoint-256/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_38/checkpoint-256/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_38/checkpoint-256/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_38/checkpoint-256/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_38/checkpoint-224] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_38/checkpoint-272\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_38/checkpoint-272/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_38/checkpoint-272/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_38/checkpoint-272/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_38/checkpoint-272/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_38/checkpoint-240] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_38/checkpoint-288\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_38/checkpoint-288/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_38/checkpoint-288/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_38/checkpoint-288/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_38/checkpoint-288/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_38/checkpoint-256] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_38/checkpoint-304\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_38/checkpoint-304/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_38/checkpoint-304/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_38/checkpoint-304/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_38/checkpoint-304/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_38/checkpoint-272] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_38/checkpoint-320\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_38/checkpoint-320/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_38/checkpoint-320/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_38/checkpoint-320/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_38/checkpoint-320/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_38/checkpoint-288] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>███▇▇▇▆▆▅▅▄▃▃▂▂▂▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▁▆▇▆▇▆▆▆▆▃██▇▅▇▆▇▁▂▄</td></tr><tr><td>eval/samples_per_second</td><td>▇▃▂▃▂▃▃▃▃▆▁▁▂▄▂▂▂█▇▅</td></tr><tr><td>eval/steps_per_second</td><td>▇▃▂▃▂▃▃▃▃▆▁▁▂▄▂▂▂█▇▅</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇█████</td></tr><tr><td>train/learning_rate</td><td>██▇▇▇▆▆▅▅▅▄▄▄▃▃▂▂▂▁▁</td></tr><tr><td>train/loss</td><td>███▇▇▇▆▆▅▅▄▄▃▃▂▂▂▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.63556</td></tr><tr><td>eval/runtime</td><td>0.0633</td></tr><tr><td>eval/samples_per_second</td><td>3160.837</td></tr><tr><td>eval/steps_per_second</td><td>31.608</td></tr><tr><td>train/epoch</td><td>20.0</td></tr><tr><td>train/global_step</td><td>320</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.6418</td></tr><tr><td>train/total_flos</td><td>212168160000.0</td></tr><tr><td>train/train_loss</td><td>0.66688</td></tr><tr><td>train/train_runtime</td><td>20.5584</td></tr><tr><td>train/train_samples_per_second</td><td>1945.68</td></tr><tr><td>train/train_steps_per_second</td><td>15.565</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">pos syn style 38</strong>: <a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/3trmnuoj\" target=\"_blank\">https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/3trmnuoj</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/scratch/data_jz17d/wandb_tmp/wandb/run-20221114_113758-3trmnuoj/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8945fac86cf43cc84fe61a55807bc91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a7b204c0cc14d6e8b673609ff321fe3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"/scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 32,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 128,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 128,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 54\n",
      "}\n",
      "\n",
      "loading weights file /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/ and are newly initialized: ['bert.pooler.dense.weight', 'classifier.weight', 'bert.pooler.dense.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch/data_jz17d/wandb_tmp/wandb/run-20221114_113828-15nrgscl</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/15nrgscl\" target=\"_blank\">pos syn style 39</a></strong> to <a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2000\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 128\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 320\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='320' max='320' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [320/320 00:17, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.692500</td>\n",
       "      <td>0.690881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.691200</td>\n",
       "      <td>0.689569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.690200</td>\n",
       "      <td>0.688177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.688900</td>\n",
       "      <td>0.686696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.685045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.685900</td>\n",
       "      <td>0.683231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.684300</td>\n",
       "      <td>0.681306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.682300</td>\n",
       "      <td>0.679211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.680800</td>\n",
       "      <td>0.677023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.678800</td>\n",
       "      <td>0.674799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.676700</td>\n",
       "      <td>0.672544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.674400</td>\n",
       "      <td>0.670341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.672400</td>\n",
       "      <td>0.668190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.670300</td>\n",
       "      <td>0.666237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.668600</td>\n",
       "      <td>0.664531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.667500</td>\n",
       "      <td>0.663056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.666900</td>\n",
       "      <td>0.661921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.665100</td>\n",
       "      <td>0.661084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.665400</td>\n",
       "      <td>0.660588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.664600</td>\n",
       "      <td>0.660419</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_39/checkpoint-16\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_39/checkpoint-16/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_39/checkpoint-16/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_39/checkpoint-16/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_39/checkpoint-16/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_39/checkpoint-32\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_39/checkpoint-32/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_39/checkpoint-32/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_39/checkpoint-32/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_39/checkpoint-32/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_39/checkpoint-48\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_39/checkpoint-48/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_39/checkpoint-48/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_39/checkpoint-48/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_39/checkpoint-48/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_39/checkpoint-16] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_39/checkpoint-64\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_39/checkpoint-64/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_39/checkpoint-64/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_39/checkpoint-64/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_39/checkpoint-64/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_39/checkpoint-32] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_39/checkpoint-80\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_39/checkpoint-80/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_39/checkpoint-80/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_39/checkpoint-80/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_39/checkpoint-80/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_39/checkpoint-48] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_39/checkpoint-96\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_39/checkpoint-96/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_39/checkpoint-96/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_39/checkpoint-96/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_39/checkpoint-96/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_39/checkpoint-64] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_39/checkpoint-112\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_39/checkpoint-112/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_39/checkpoint-112/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_39/checkpoint-112/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_39/checkpoint-112/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_39/checkpoint-80] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_39/checkpoint-128\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_39/checkpoint-128/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_39/checkpoint-128/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_39/checkpoint-128/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_39/checkpoint-128/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_39/checkpoint-96] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_39/checkpoint-144\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_39/checkpoint-144/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_39/checkpoint-144/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_39/checkpoint-144/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_39/checkpoint-144/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_39/checkpoint-112] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_39/checkpoint-160\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_39/checkpoint-160/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_39/checkpoint-160/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_39/checkpoint-160/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_39/checkpoint-160/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_39/checkpoint-128] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_39/checkpoint-176\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_39/checkpoint-176/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_39/checkpoint-176/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_39/checkpoint-176/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_39/checkpoint-176/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_39/checkpoint-144] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_39/checkpoint-192\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_39/checkpoint-192/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_39/checkpoint-192/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_39/checkpoint-192/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_39/checkpoint-192/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_39/checkpoint-160] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_39/checkpoint-208\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_39/checkpoint-208/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_39/checkpoint-208/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_39/checkpoint-208/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_39/checkpoint-208/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_39/checkpoint-176] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_39/checkpoint-224\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_39/checkpoint-224/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_39/checkpoint-224/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_39/checkpoint-224/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_39/checkpoint-224/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_39/checkpoint-192] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_39/checkpoint-240\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_39/checkpoint-240/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_39/checkpoint-240/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_39/checkpoint-240/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_39/checkpoint-240/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_39/checkpoint-208] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_39/checkpoint-256\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_39/checkpoint-256/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_39/checkpoint-256/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_39/checkpoint-256/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_39/checkpoint-256/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_39/checkpoint-224] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_39/checkpoint-272\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_39/checkpoint-272/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_39/checkpoint-272/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_39/checkpoint-272/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_39/checkpoint-272/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_39/checkpoint-240] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_39/checkpoint-288\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_39/checkpoint-288/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_39/checkpoint-288/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_39/checkpoint-288/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_39/checkpoint-288/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_39/checkpoint-256] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_39/checkpoint-304\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_39/checkpoint-304/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_39/checkpoint-304/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_39/checkpoint-304/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_39/checkpoint-304/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_39/checkpoint-272] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_39/checkpoint-320\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_39/checkpoint-320/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_39/checkpoint-320/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_39/checkpoint-320/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_39/checkpoint-320/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_39/checkpoint-288] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>██▇▇▇▆▆▅▅▄▄▃▃▂▂▂▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▁▃█▅▆▇▂▃▂▄▃▅▃▁▃▅▄▃▄▄</td></tr><tr><td>eval/samples_per_second</td><td>█▅▁▄▃▂▇▆▇▅▅▄▆█▅▄▄▆▅▅</td></tr><tr><td>eval/steps_per_second</td><td>█▅▁▄▃▂▇▆▇▅▅▄▆█▅▄▄▆▅▅</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇█████</td></tr><tr><td>train/learning_rate</td><td>██▇▇▇▆▆▅▅▅▄▄▄▃▃▂▂▂▁▁</td></tr><tr><td>train/loss</td><td>██▇▇▇▆▆▅▅▅▄▃▃▂▂▂▂▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.66042</td></tr><tr><td>eval/runtime</td><td>0.066</td></tr><tr><td>eval/samples_per_second</td><td>3031.103</td></tr><tr><td>eval/steps_per_second</td><td>30.311</td></tr><tr><td>train/epoch</td><td>20.0</td></tr><tr><td>train/global_step</td><td>320</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.6646</td></tr><tr><td>train/total_flos</td><td>212168160000.0</td></tr><tr><td>train/train_loss</td><td>0.67772</td></tr><tr><td>train/train_runtime</td><td>17.8078</td></tr><tr><td>train/train_samples_per_second</td><td>2246.207</td></tr><tr><td>train/train_steps_per_second</td><td>17.97</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">pos syn style 39</strong>: <a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/15nrgscl\" target=\"_blank\">https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/15nrgscl</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/scratch/data_jz17d/wandb_tmp/wandb/run-20221114_113828-15nrgscl/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bec2f88ebd3f41359994bec5275b634f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e078cdd00374c748817bcffef5b1077",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"/scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 32,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 128,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 128,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 54\n",
      "}\n",
      "\n",
      "loading weights file /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/ and are newly initialized: ['bert.pooler.dense.weight', 'classifier.weight', 'bert.pooler.dense.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "604285584d4446bbbff056c7c845e353",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.03334367275238037, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch/data_jz17d/wandb_tmp/wandb/run-20221114_113855-3bk5m6ul</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/3bk5m6ul\" target=\"_blank\">pos syn style 40</a></strong> to <a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2000\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 128\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 320\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='320' max='320' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [320/320 00:25, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.692000</td>\n",
       "      <td>0.689479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.689000</td>\n",
       "      <td>0.686237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.685700</td>\n",
       "      <td>0.682076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.681400</td>\n",
       "      <td>0.676584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.675800</td>\n",
       "      <td>0.669458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.668700</td>\n",
       "      <td>0.660724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.660700</td>\n",
       "      <td>0.650573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.651500</td>\n",
       "      <td>0.639577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.642500</td>\n",
       "      <td>0.628592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.633200</td>\n",
       "      <td>0.618174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.623700</td>\n",
       "      <td>0.608459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.615400</td>\n",
       "      <td>0.599372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.607700</td>\n",
       "      <td>0.591277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.600800</td>\n",
       "      <td>0.584300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.595100</td>\n",
       "      <td>0.578339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.589500</td>\n",
       "      <td>0.573379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.585100</td>\n",
       "      <td>0.569685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.582800</td>\n",
       "      <td>0.567040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.580600</td>\n",
       "      <td>0.565444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.579400</td>\n",
       "      <td>0.564905</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_40/checkpoint-16\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_40/checkpoint-16/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_40/checkpoint-16/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_40/checkpoint-16/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_40/checkpoint-16/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_40/checkpoint-32\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_40/checkpoint-32/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_40/checkpoint-32/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_40/checkpoint-32/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_40/checkpoint-32/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_40/checkpoint-48\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_40/checkpoint-48/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_40/checkpoint-48/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_40/checkpoint-48/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_40/checkpoint-48/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_40/checkpoint-16] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_40/checkpoint-64\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_40/checkpoint-64/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_40/checkpoint-64/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_40/checkpoint-64/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_40/checkpoint-64/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_40/checkpoint-32] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_40/checkpoint-80\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_40/checkpoint-80/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_40/checkpoint-80/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_40/checkpoint-80/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_40/checkpoint-80/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_40/checkpoint-48] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_40/checkpoint-96\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_40/checkpoint-96/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_40/checkpoint-96/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_40/checkpoint-96/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_40/checkpoint-96/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_40/checkpoint-64] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_40/checkpoint-112\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_40/checkpoint-112/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_40/checkpoint-112/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_40/checkpoint-112/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_40/checkpoint-112/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_40/checkpoint-80] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_40/checkpoint-128\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_40/checkpoint-128/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_40/checkpoint-128/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_40/checkpoint-128/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_40/checkpoint-128/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_40/checkpoint-96] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_40/checkpoint-144\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_40/checkpoint-144/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_40/checkpoint-144/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_40/checkpoint-144/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_40/checkpoint-144/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_40/checkpoint-112] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_40/checkpoint-160\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_40/checkpoint-160/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_40/checkpoint-160/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_40/checkpoint-160/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_40/checkpoint-160/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_40/checkpoint-128] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_40/checkpoint-176\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_40/checkpoint-176/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_40/checkpoint-176/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_40/checkpoint-176/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_40/checkpoint-176/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_40/checkpoint-144] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_40/checkpoint-192\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_40/checkpoint-192/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_40/checkpoint-192/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_40/checkpoint-192/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_40/checkpoint-192/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_40/checkpoint-160] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_40/checkpoint-208\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_40/checkpoint-208/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_40/checkpoint-208/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_40/checkpoint-208/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_40/checkpoint-208/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_40/checkpoint-176] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_40/checkpoint-224\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_40/checkpoint-224/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_40/checkpoint-224/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_40/checkpoint-224/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_40/checkpoint-224/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_40/checkpoint-192] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_40/checkpoint-240\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_40/checkpoint-240/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_40/checkpoint-240/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_40/checkpoint-240/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_40/checkpoint-240/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_40/checkpoint-208] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_40/checkpoint-256\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_40/checkpoint-256/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_40/checkpoint-256/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_40/checkpoint-256/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_40/checkpoint-256/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_40/checkpoint-224] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_40/checkpoint-272\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_40/checkpoint-272/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_40/checkpoint-272/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_40/checkpoint-272/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_40/checkpoint-272/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_40/checkpoint-240] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_40/checkpoint-288\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_40/checkpoint-288/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_40/checkpoint-288/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_40/checkpoint-288/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_40/checkpoint-288/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_40/checkpoint-256] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_40/checkpoint-304\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_40/checkpoint-304/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_40/checkpoint-304/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_40/checkpoint-304/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_40/checkpoint-304/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_40/checkpoint-272] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_40/checkpoint-320\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_40/checkpoint-320/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_40/checkpoint-320/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_40/checkpoint-320/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_40/checkpoint-320/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_40/checkpoint-288] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>███▇▇▆▆▅▅▄▃▃▂▂▂▁▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▃▅▂▆▁▁▅▇▆▆▆▇▆█▅▇▇▅█▇</td></tr><tr><td>eval/samples_per_second</td><td>▅▄▇▂██▃▂▃▃▃▂▃▁▃▂▂▃▁▂</td></tr><tr><td>eval/steps_per_second</td><td>▅▄▇▂██▃▂▃▃▃▂▃▁▃▂▂▃▁▂</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇█████</td></tr><tr><td>train/learning_rate</td><td>██▇▇▇▆▆▅▅▅▄▄▄▃▃▂▂▂▁▁</td></tr><tr><td>train/loss</td><td>███▇▇▇▆▅▅▄▄▃▃▂▂▂▁▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.56491</td></tr><tr><td>eval/runtime</td><td>0.0685</td></tr><tr><td>eval/samples_per_second</td><td>2919.107</td></tr><tr><td>eval/steps_per_second</td><td>29.191</td></tr><tr><td>train/epoch</td><td>20.0</td></tr><tr><td>train/global_step</td><td>320</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.5794</td></tr><tr><td>train/total_flos</td><td>212168160000.0</td></tr><tr><td>train/train_loss</td><td>0.63202</td></tr><tr><td>train/train_runtime</td><td>25.1876</td></tr><tr><td>train/train_samples_per_second</td><td>1588.081</td></tr><tr><td>train/train_steps_per_second</td><td>12.705</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">pos syn style 40</strong>: <a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/3bk5m6ul\" target=\"_blank\">https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/3bk5m6ul</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/scratch/data_jz17d/wandb_tmp/wandb/run-20221114_113855-3bk5m6ul/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c222f0d01bb42ca854bb07d92ef1a20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12bc7fb8bf244348a8767338d9ef6c29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"/scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 32,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 128,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 128,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 54\n",
      "}\n",
      "\n",
      "loading weights file /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/ and are newly initialized: ['bert.pooler.dense.weight', 'classifier.weight', 'bert.pooler.dense.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch/data_jz17d/wandb_tmp/wandb/run-20221114_113939-2d4waf62</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/2d4waf62\" target=\"_blank\">pos syn style 41</a></strong> to <a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2000\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 128\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 320\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='320' max='320' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [320/320 00:15, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.693100</td>\n",
       "      <td>0.692089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.692600</td>\n",
       "      <td>0.691669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.692300</td>\n",
       "      <td>0.691262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.692000</td>\n",
       "      <td>0.690882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.691600</td>\n",
       "      <td>0.690526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.691300</td>\n",
       "      <td>0.690193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.691000</td>\n",
       "      <td>0.689892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.690700</td>\n",
       "      <td>0.689610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.690600</td>\n",
       "      <td>0.689347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.690300</td>\n",
       "      <td>0.689111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.690000</td>\n",
       "      <td>0.688896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.689900</td>\n",
       "      <td>0.688703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.689600</td>\n",
       "      <td>0.688527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.689600</td>\n",
       "      <td>0.688377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.689300</td>\n",
       "      <td>0.688255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.689200</td>\n",
       "      <td>0.688147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.689200</td>\n",
       "      <td>0.688067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.689000</td>\n",
       "      <td>0.688009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.689200</td>\n",
       "      <td>0.687976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.689200</td>\n",
       "      <td>0.687964</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_41/checkpoint-16\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_41/checkpoint-16/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_41/checkpoint-16/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_41/checkpoint-16/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_41/checkpoint-16/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_41/checkpoint-32\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_41/checkpoint-32/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_41/checkpoint-32/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_41/checkpoint-32/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_41/checkpoint-32/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_41/checkpoint-48\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_41/checkpoint-48/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_41/checkpoint-48/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_41/checkpoint-48/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_41/checkpoint-48/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_41/checkpoint-16] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_41/checkpoint-64\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_41/checkpoint-64/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_41/checkpoint-64/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_41/checkpoint-64/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_41/checkpoint-64/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_41/checkpoint-32] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_41/checkpoint-80\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_41/checkpoint-80/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_41/checkpoint-80/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_41/checkpoint-80/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_41/checkpoint-80/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_41/checkpoint-48] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_41/checkpoint-96\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_41/checkpoint-96/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_41/checkpoint-96/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_41/checkpoint-96/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_41/checkpoint-96/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_41/checkpoint-64] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_41/checkpoint-112\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_41/checkpoint-112/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_41/checkpoint-112/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_41/checkpoint-112/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_41/checkpoint-112/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_41/checkpoint-80] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_41/checkpoint-128\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_41/checkpoint-128/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_41/checkpoint-128/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_41/checkpoint-128/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_41/checkpoint-128/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_41/checkpoint-96] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_41/checkpoint-144\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_41/checkpoint-144/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_41/checkpoint-144/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_41/checkpoint-144/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_41/checkpoint-144/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_41/checkpoint-112] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_41/checkpoint-160\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_41/checkpoint-160/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_41/checkpoint-160/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_41/checkpoint-160/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_41/checkpoint-160/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_41/checkpoint-128] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_41/checkpoint-176\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_41/checkpoint-176/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_41/checkpoint-176/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_41/checkpoint-176/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_41/checkpoint-176/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_41/checkpoint-144] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_41/checkpoint-192\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_41/checkpoint-192/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_41/checkpoint-192/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_41/checkpoint-192/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_41/checkpoint-192/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_41/checkpoint-160] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_41/checkpoint-208\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_41/checkpoint-208/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_41/checkpoint-208/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_41/checkpoint-208/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_41/checkpoint-208/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_41/checkpoint-176] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_41/checkpoint-224\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_41/checkpoint-224/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_41/checkpoint-224/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_41/checkpoint-224/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_41/checkpoint-224/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_41/checkpoint-192] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_41/checkpoint-240\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_41/checkpoint-240/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_41/checkpoint-240/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_41/checkpoint-240/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_41/checkpoint-240/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_41/checkpoint-208] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_41/checkpoint-256\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_41/checkpoint-256/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_41/checkpoint-256/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_41/checkpoint-256/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_41/checkpoint-256/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_41/checkpoint-224] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_41/checkpoint-272\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_41/checkpoint-272/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_41/checkpoint-272/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_41/checkpoint-272/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_41/checkpoint-272/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_41/checkpoint-240] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_41/checkpoint-288\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_41/checkpoint-288/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_41/checkpoint-288/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_41/checkpoint-288/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_41/checkpoint-288/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_41/checkpoint-256] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_41/checkpoint-304\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_41/checkpoint-304/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_41/checkpoint-304/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_41/checkpoint-304/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_41/checkpoint-304/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_41/checkpoint-272] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_41/checkpoint-320\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_41/checkpoint-320/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_41/checkpoint-320/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_41/checkpoint-320/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_41/checkpoint-320/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_41/checkpoint-288] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▇▇▆▅▅▄▄▃▃▃▂▂▂▁▁▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▁▂▅▅▅▅▅▅▅▅▅▅▄▁▁▆▄█▅▃</td></tr><tr><td>eval/samples_per_second</td><td>█▇▄▄▃▄▄▄▄▄▄▄▅██▃▄▁▃▆</td></tr><tr><td>eval/steps_per_second</td><td>█▇▄▄▃▄▄▄▄▄▄▄▅██▃▄▁▃▆</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇█████</td></tr><tr><td>train/learning_rate</td><td>██▇▇▇▆▆▅▅▅▄▄▄▃▃▂▂▂▁▁</td></tr><tr><td>train/loss</td><td>█▇▇▆▅▅▄▄▄▃▃▃▂▂▂▁▁▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.68796</td></tr><tr><td>eval/runtime</td><td>0.0621</td></tr><tr><td>eval/samples_per_second</td><td>3221.258</td></tr><tr><td>eval/steps_per_second</td><td>32.213</td></tr><tr><td>train/epoch</td><td>20.0</td></tr><tr><td>train/global_step</td><td>320</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.6892</td></tr><tr><td>train/total_flos</td><td>212168160000.0</td></tr><tr><td>train/train_loss</td><td>0.69049</td></tr><tr><td>train/train_runtime</td><td>15.277</td></tr><tr><td>train/train_samples_per_second</td><td>2618.322</td></tr><tr><td>train/train_steps_per_second</td><td>20.947</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">pos syn style 41</strong>: <a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/2d4waf62\" target=\"_blank\">https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/2d4waf62</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/scratch/data_jz17d/wandb_tmp/wandb/run-20221114_113939-2d4waf62/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7ff4d3e05604874a3ad93fe7587c48d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e2b1b065acd457389a924cc7586d459",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"/scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 32,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 128,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 128,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 54\n",
      "}\n",
      "\n",
      "loading weights file /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/ and are newly initialized: ['bert.pooler.dense.weight', 'classifier.weight', 'bert.pooler.dense.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch/data_jz17d/wandb_tmp/wandb/run-20221114_114002-2d0xdjya</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/2d0xdjya\" target=\"_blank\">pos syn style 42</a></strong> to <a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2000\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 128\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 320\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='320' max='320' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [320/320 00:19, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.692300</td>\n",
       "      <td>0.690096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.689900</td>\n",
       "      <td>0.687622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.687600</td>\n",
       "      <td>0.684771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.684800</td>\n",
       "      <td>0.681445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.681400</td>\n",
       "      <td>0.677440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.677400</td>\n",
       "      <td>0.672731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.673000</td>\n",
       "      <td>0.667401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.667800</td>\n",
       "      <td>0.661415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.662600</td>\n",
       "      <td>0.654939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.656900</td>\n",
       "      <td>0.648232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.650800</td>\n",
       "      <td>0.641578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.645100</td>\n",
       "      <td>0.635030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.639000</td>\n",
       "      <td>0.628899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.634500</td>\n",
       "      <td>0.623304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.629300</td>\n",
       "      <td>0.618428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.625100</td>\n",
       "      <td>0.614361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.622400</td>\n",
       "      <td>0.611166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.619300</td>\n",
       "      <td>0.608848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.618400</td>\n",
       "      <td>0.607450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.616800</td>\n",
       "      <td>0.606974</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_42/checkpoint-16\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_42/checkpoint-16/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_42/checkpoint-16/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_42/checkpoint-16/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_42/checkpoint-16/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_42/checkpoint-32\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_42/checkpoint-32/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_42/checkpoint-32/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_42/checkpoint-32/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_42/checkpoint-32/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_42/checkpoint-48\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_42/checkpoint-48/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_42/checkpoint-48/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_42/checkpoint-48/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_42/checkpoint-48/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_42/checkpoint-16] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_42/checkpoint-64\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_42/checkpoint-64/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_42/checkpoint-64/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_42/checkpoint-64/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_42/checkpoint-64/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_42/checkpoint-32] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_42/checkpoint-80\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_42/checkpoint-80/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_42/checkpoint-80/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_42/checkpoint-80/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_42/checkpoint-80/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_42/checkpoint-48] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_42/checkpoint-96\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_42/checkpoint-96/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_42/checkpoint-96/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_42/checkpoint-96/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_42/checkpoint-96/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_42/checkpoint-64] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_42/checkpoint-112\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_42/checkpoint-112/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_42/checkpoint-112/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_42/checkpoint-112/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_42/checkpoint-112/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_42/checkpoint-80] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_42/checkpoint-128\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_42/checkpoint-128/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_42/checkpoint-128/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_42/checkpoint-128/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_42/checkpoint-128/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_42/checkpoint-96] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_42/checkpoint-144\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_42/checkpoint-144/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_42/checkpoint-144/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_42/checkpoint-144/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_42/checkpoint-144/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_42/checkpoint-112] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_42/checkpoint-160\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_42/checkpoint-160/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_42/checkpoint-160/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_42/checkpoint-160/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_42/checkpoint-160/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_42/checkpoint-128] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_42/checkpoint-176\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_42/checkpoint-176/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_42/checkpoint-176/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_42/checkpoint-176/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_42/checkpoint-176/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_42/checkpoint-144] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_42/checkpoint-192\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_42/checkpoint-192/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_42/checkpoint-192/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_42/checkpoint-192/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_42/checkpoint-192/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_42/checkpoint-160] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_42/checkpoint-208\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_42/checkpoint-208/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_42/checkpoint-208/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_42/checkpoint-208/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_42/checkpoint-208/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_42/checkpoint-176] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_42/checkpoint-224\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_42/checkpoint-224/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_42/checkpoint-224/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_42/checkpoint-224/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_42/checkpoint-224/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_42/checkpoint-192] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_42/checkpoint-240\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_42/checkpoint-240/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_42/checkpoint-240/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_42/checkpoint-240/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_42/checkpoint-240/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_42/checkpoint-208] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_42/checkpoint-256\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_42/checkpoint-256/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_42/checkpoint-256/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_42/checkpoint-256/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_42/checkpoint-256/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_42/checkpoint-224] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_42/checkpoint-272\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_42/checkpoint-272/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_42/checkpoint-272/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_42/checkpoint-272/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_42/checkpoint-272/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_42/checkpoint-240] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_42/checkpoint-288\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_42/checkpoint-288/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_42/checkpoint-288/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_42/checkpoint-288/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_42/checkpoint-288/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_42/checkpoint-256] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_42/checkpoint-304\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_42/checkpoint-304/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_42/checkpoint-304/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_42/checkpoint-304/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_42/checkpoint-304/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_42/checkpoint-272] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_42/checkpoint-320\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_42/checkpoint-320/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_42/checkpoint-320/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_42/checkpoint-320/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_42/checkpoint-320/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_42/checkpoint-288] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>███▇▇▇▆▆▅▄▄▃▃▂▂▂▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▂▆▇▅█▆▇▁█▁▂▃▇█▆▇▇██▇</td></tr><tr><td>eval/samples_per_second</td><td>▇▃▂▃▁▃▂█▁█▇▆▂▁▂▂▂▁▁▂</td></tr><tr><td>eval/steps_per_second</td><td>▇▃▂▃▁▃▂█▁█▇▆▂▁▂▂▂▁▁▂</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇█████</td></tr><tr><td>train/learning_rate</td><td>██▇▇▇▆▆▅▅▅▄▄▄▃▃▂▂▂▁▁</td></tr><tr><td>train/loss</td><td>███▇▇▇▆▆▅▅▄▄▃▃▂▂▂▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.60697</td></tr><tr><td>eval/runtime</td><td>0.066</td></tr><tr><td>eval/samples_per_second</td><td>3029.483</td></tr><tr><td>eval/steps_per_second</td><td>30.295</td></tr><tr><td>train/epoch</td><td>20.0</td></tr><tr><td>train/global_step</td><td>320</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.6168</td></tr><tr><td>train/total_flos</td><td>212168160000.0</td></tr><tr><td>train/train_loss</td><td>0.65373</td></tr><tr><td>train/train_runtime</td><td>19.8587</td></tr><tr><td>train/train_samples_per_second</td><td>2014.234</td></tr><tr><td>train/train_steps_per_second</td><td>16.114</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">pos syn style 42</strong>: <a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/2d0xdjya\" target=\"_blank\">https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/2d0xdjya</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/scratch/data_jz17d/wandb_tmp/wandb/run-20221114_114002-2d0xdjya/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b38070624194187ae6217aff22d58c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76482d37c88643ef910bb55f4d58be5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"/scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 32,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 128,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 128,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 54\n",
      "}\n",
      "\n",
      "loading weights file /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/ and are newly initialized: ['bert.pooler.dense.weight', 'classifier.weight', 'bert.pooler.dense.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch/data_jz17d/wandb_tmp/wandb/run-20221114_114031-38gyd7bz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/38gyd7bz\" target=\"_blank\">pos syn style 43</a></strong> to <a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2000\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 128\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 320\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='320' max='320' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [320/320 00:19, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.692400</td>\n",
       "      <td>0.690490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.690500</td>\n",
       "      <td>0.688514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.688700</td>\n",
       "      <td>0.686414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.686800</td>\n",
       "      <td>0.684150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.684500</td>\n",
       "      <td>0.681623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.682000</td>\n",
       "      <td>0.678841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.679500</td>\n",
       "      <td>0.675833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.676500</td>\n",
       "      <td>0.672581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.673800</td>\n",
       "      <td>0.669132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.670500</td>\n",
       "      <td>0.665600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.666900</td>\n",
       "      <td>0.662055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.663800</td>\n",
       "      <td>0.658564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.660300</td>\n",
       "      <td>0.655200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.658200</td>\n",
       "      <td>0.652163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.654500</td>\n",
       "      <td>0.649479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.652400</td>\n",
       "      <td>0.647178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.651100</td>\n",
       "      <td>0.645383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.649200</td>\n",
       "      <td>0.644087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.649100</td>\n",
       "      <td>0.643298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.648000</td>\n",
       "      <td>0.643028</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_43/checkpoint-16\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_43/checkpoint-16/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_43/checkpoint-16/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_43/checkpoint-16/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_43/checkpoint-16/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_43/checkpoint-32\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_43/checkpoint-32/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_43/checkpoint-32/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_43/checkpoint-32/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_43/checkpoint-32/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_43/checkpoint-48\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_43/checkpoint-48/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_43/checkpoint-48/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_43/checkpoint-48/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_43/checkpoint-48/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_43/checkpoint-16] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_43/checkpoint-64\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_43/checkpoint-64/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_43/checkpoint-64/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_43/checkpoint-64/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_43/checkpoint-64/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_43/checkpoint-32] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_43/checkpoint-80\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_43/checkpoint-80/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_43/checkpoint-80/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_43/checkpoint-80/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_43/checkpoint-80/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_43/checkpoint-48] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_43/checkpoint-96\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_43/checkpoint-96/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_43/checkpoint-96/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_43/checkpoint-96/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_43/checkpoint-96/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_43/checkpoint-64] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_43/checkpoint-112\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_43/checkpoint-112/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_43/checkpoint-112/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_43/checkpoint-112/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_43/checkpoint-112/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_43/checkpoint-80] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_43/checkpoint-128\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_43/checkpoint-128/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_43/checkpoint-128/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_43/checkpoint-128/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_43/checkpoint-128/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_43/checkpoint-96] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_43/checkpoint-144\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_43/checkpoint-144/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_43/checkpoint-144/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_43/checkpoint-144/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_43/checkpoint-144/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_43/checkpoint-112] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_43/checkpoint-160\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_43/checkpoint-160/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_43/checkpoint-160/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_43/checkpoint-160/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_43/checkpoint-160/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_43/checkpoint-128] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_43/checkpoint-176\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_43/checkpoint-176/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_43/checkpoint-176/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_43/checkpoint-176/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_43/checkpoint-176/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_43/checkpoint-144] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_43/checkpoint-192\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_43/checkpoint-192/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_43/checkpoint-192/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_43/checkpoint-192/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_43/checkpoint-192/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_43/checkpoint-160] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_43/checkpoint-208\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_43/checkpoint-208/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_43/checkpoint-208/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_43/checkpoint-208/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_43/checkpoint-208/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_43/checkpoint-176] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_43/checkpoint-224\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_43/checkpoint-224/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_43/checkpoint-224/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_43/checkpoint-224/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_43/checkpoint-224/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_43/checkpoint-192] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_43/checkpoint-240\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_43/checkpoint-240/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_43/checkpoint-240/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_43/checkpoint-240/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_43/checkpoint-240/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_43/checkpoint-208] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_43/checkpoint-256\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_43/checkpoint-256/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_43/checkpoint-256/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_43/checkpoint-256/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_43/checkpoint-256/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_43/checkpoint-224] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_43/checkpoint-272\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_43/checkpoint-272/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_43/checkpoint-272/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_43/checkpoint-272/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_43/checkpoint-272/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_43/checkpoint-240] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_43/checkpoint-288\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_43/checkpoint-288/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_43/checkpoint-288/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_43/checkpoint-288/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_43/checkpoint-288/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_43/checkpoint-256] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_43/checkpoint-304\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_43/checkpoint-304/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_43/checkpoint-304/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_43/checkpoint-304/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_43/checkpoint-304/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_43/checkpoint-272] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_43/checkpoint-320\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_43/checkpoint-320/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_43/checkpoint-320/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_43/checkpoint-320/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_43/checkpoint-320/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_43/checkpoint-288] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>██▇▇▇▆▆▅▅▄▄▃▃▂▂▂▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▁▅▆▄▆▅▅▂▃▆▄▅█▅▅▅▄▅▅▅</td></tr><tr><td>eval/samples_per_second</td><td>█▄▃▄▃▃▄▇▅▃▅▄▁▄▄▄▅▃▄▄</td></tr><tr><td>eval/steps_per_second</td><td>█▄▃▄▃▃▄▇▅▃▅▄▁▄▄▄▅▃▄▄</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇█████</td></tr><tr><td>train/learning_rate</td><td>██▇▇▇▆▆▅▅▅▄▄▄▃▃▂▂▂▁▁</td></tr><tr><td>train/loss</td><td>██▇▇▇▆▆▅▅▅▄▃▃▃▂▂▁▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.64303</td></tr><tr><td>eval/runtime</td><td>0.0673</td></tr><tr><td>eval/samples_per_second</td><td>2970.018</td></tr><tr><td>eval/steps_per_second</td><td>29.7</td></tr><tr><td>train/epoch</td><td>20.0</td></tr><tr><td>train/global_step</td><td>320</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.648</td></tr><tr><td>train/total_flos</td><td>212168160000.0</td></tr><tr><td>train/train_loss</td><td>0.66893</td></tr><tr><td>train/train_runtime</td><td>19.6037</td></tr><tr><td>train/train_samples_per_second</td><td>2040.435</td></tr><tr><td>train/train_steps_per_second</td><td>16.323</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">pos syn style 43</strong>: <a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/38gyd7bz\" target=\"_blank\">https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/38gyd7bz</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/scratch/data_jz17d/wandb_tmp/wandb/run-20221114_114031-38gyd7bz/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6857232abe0f4e72a94fd90afe35da5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d3d70489d5b4639bdc886ae6a247bfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"/scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 32,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 128,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 128,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 54\n",
      "}\n",
      "\n",
      "loading weights file /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/ and are newly initialized: ['bert.pooler.dense.weight', 'classifier.weight', 'bert.pooler.dense.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch/data_jz17d/wandb_tmp/wandb/run-20221114_114100-145164w2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/145164w2\" target=\"_blank\">pos syn style 44</a></strong> to <a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2000\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 128\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 320\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='320' max='320' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [320/320 00:26, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.691700</td>\n",
       "      <td>0.688911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.687800</td>\n",
       "      <td>0.684634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.683500</td>\n",
       "      <td>0.679291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.677900</td>\n",
       "      <td>0.672564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.671200</td>\n",
       "      <td>0.664016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.662600</td>\n",
       "      <td>0.653934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.653300</td>\n",
       "      <td>0.642663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.643100</td>\n",
       "      <td>0.630621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.632900</td>\n",
       "      <td>0.619012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.622500</td>\n",
       "      <td>0.608028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.612500</td>\n",
       "      <td>0.597677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.603200</td>\n",
       "      <td>0.588199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.594500</td>\n",
       "      <td>0.579726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.587500</td>\n",
       "      <td>0.572347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.579800</td>\n",
       "      <td>0.566157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.575400</td>\n",
       "      <td>0.561186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.570500</td>\n",
       "      <td>0.557174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.568100</td>\n",
       "      <td>0.554434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.565100</td>\n",
       "      <td>0.552783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.564700</td>\n",
       "      <td>0.552236</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_44/checkpoint-16\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_44/checkpoint-16/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_44/checkpoint-16/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_44/checkpoint-16/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_44/checkpoint-16/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_44/checkpoint-32\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_44/checkpoint-32/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_44/checkpoint-32/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_44/checkpoint-32/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_44/checkpoint-32/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_44/checkpoint-48\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_44/checkpoint-48/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_44/checkpoint-48/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_44/checkpoint-48/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_44/checkpoint-48/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_44/checkpoint-16] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_44/checkpoint-64\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_44/checkpoint-64/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_44/checkpoint-64/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_44/checkpoint-64/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_44/checkpoint-64/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_44/checkpoint-32] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_44/checkpoint-80\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_44/checkpoint-80/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_44/checkpoint-80/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_44/checkpoint-80/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_44/checkpoint-80/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_44/checkpoint-48] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_44/checkpoint-96\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_44/checkpoint-96/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_44/checkpoint-96/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_44/checkpoint-96/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_44/checkpoint-96/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_44/checkpoint-64] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_44/checkpoint-112\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_44/checkpoint-112/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_44/checkpoint-112/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_44/checkpoint-112/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_44/checkpoint-112/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_44/checkpoint-80] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_44/checkpoint-128\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_44/checkpoint-128/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_44/checkpoint-128/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_44/checkpoint-128/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_44/checkpoint-128/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_44/checkpoint-96] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_44/checkpoint-144\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_44/checkpoint-144/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_44/checkpoint-144/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_44/checkpoint-144/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_44/checkpoint-144/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_44/checkpoint-112] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_44/checkpoint-160\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_44/checkpoint-160/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_44/checkpoint-160/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_44/checkpoint-160/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_44/checkpoint-160/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_44/checkpoint-128] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_44/checkpoint-176\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_44/checkpoint-176/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_44/checkpoint-176/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_44/checkpoint-176/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_44/checkpoint-176/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_44/checkpoint-144] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_44/checkpoint-192\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_44/checkpoint-192/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_44/checkpoint-192/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_44/checkpoint-192/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_44/checkpoint-192/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_44/checkpoint-160] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_44/checkpoint-208\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_44/checkpoint-208/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_44/checkpoint-208/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_44/checkpoint-208/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_44/checkpoint-208/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_44/checkpoint-176] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_44/checkpoint-224\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_44/checkpoint-224/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_44/checkpoint-224/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_44/checkpoint-224/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_44/checkpoint-224/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_44/checkpoint-192] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_44/checkpoint-240\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_44/checkpoint-240/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_44/checkpoint-240/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_44/checkpoint-240/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_44/checkpoint-240/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_44/checkpoint-208] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_44/checkpoint-256\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_44/checkpoint-256/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_44/checkpoint-256/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_44/checkpoint-256/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_44/checkpoint-256/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_44/checkpoint-224] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_44/checkpoint-272\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_44/checkpoint-272/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_44/checkpoint-272/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_44/checkpoint-272/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_44/checkpoint-272/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_44/checkpoint-240] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_44/checkpoint-288\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_44/checkpoint-288/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_44/checkpoint-288/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_44/checkpoint-288/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_44/checkpoint-288/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_44/checkpoint-256] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_44/checkpoint-304\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_44/checkpoint-304/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_44/checkpoint-304/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_44/checkpoint-304/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_44/checkpoint-304/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_44/checkpoint-272] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_44/checkpoint-320\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_44/checkpoint-320/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_44/checkpoint-320/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_44/checkpoint-320/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_44/checkpoint-320/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_44/checkpoint-288] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>███▇▇▆▆▅▄▄▃▃▂▂▂▁▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▇▇▅▁▅▇█▅▆█▇▅▇█▂▅▆▅▇█</td></tr><tr><td>eval/samples_per_second</td><td>▂▂▄█▄▂▁▄▃▁▂▄▂▁▇▃▂▄▁▁</td></tr><tr><td>eval/steps_per_second</td><td>▂▂▄█▄▂▁▄▃▁▂▄▂▁▇▃▂▄▁▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇█████</td></tr><tr><td>train/learning_rate</td><td>██▇▇▇▆▆▅▅▅▄▄▄▃▃▂▂▂▁▁</td></tr><tr><td>train/loss</td><td>███▇▇▆▆▅▅▄▄▃▃▂▂▂▁▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.55224</td></tr><tr><td>eval/runtime</td><td>0.0686</td></tr><tr><td>eval/samples_per_second</td><td>2913.925</td></tr><tr><td>eval/steps_per_second</td><td>29.139</td></tr><tr><td>train/epoch</td><td>20.0</td></tr><tr><td>train/global_step</td><td>320</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.5647</td></tr><tr><td>train/total_flos</td><td>212168160000.0</td></tr><tr><td>train/train_loss</td><td>0.62239</td></tr><tr><td>train/train_runtime</td><td>26.3338</td></tr><tr><td>train/train_samples_per_second</td><td>1518.962</td></tr><tr><td>train/train_steps_per_second</td><td>12.152</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">pos syn style 44</strong>: <a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/145164w2\" target=\"_blank\">https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/145164w2</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/scratch/data_jz17d/wandb_tmp/wandb/run-20221114_114100-145164w2/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de59e7b810d04c4d943323091ddeefea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f01a3f9ad92425da81b6984796d5652",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"/scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 32,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 128,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 128,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 54\n",
      "}\n",
      "\n",
      "loading weights file /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/ and are newly initialized: ['bert.pooler.dense.weight', 'classifier.weight', 'bert.pooler.dense.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch/data_jz17d/wandb_tmp/wandb/run-20221114_114140-xafo0p2c</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/xafo0p2c\" target=\"_blank\">pos syn style 45</a></strong> to <a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2000\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 128\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 320\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='320' max='320' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [320/320 00:15, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.693200</td>\n",
       "      <td>0.692392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.692600</td>\n",
       "      <td>0.691846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.692100</td>\n",
       "      <td>0.691320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.691600</td>\n",
       "      <td>0.690828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.691300</td>\n",
       "      <td>0.690364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.690800</td>\n",
       "      <td>0.689933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.690400</td>\n",
       "      <td>0.689537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.690100</td>\n",
       "      <td>0.689171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.689800</td>\n",
       "      <td>0.688832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.689500</td>\n",
       "      <td>0.688526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.689100</td>\n",
       "      <td>0.688249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.689000</td>\n",
       "      <td>0.687997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.688700</td>\n",
       "      <td>0.687774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.688600</td>\n",
       "      <td>0.687579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.688300</td>\n",
       "      <td>0.687418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.688200</td>\n",
       "      <td>0.687279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.688100</td>\n",
       "      <td>0.687176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.687900</td>\n",
       "      <td>0.687101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.688000</td>\n",
       "      <td>0.687057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.688100</td>\n",
       "      <td>0.687042</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_45/checkpoint-16\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_45/checkpoint-16/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_45/checkpoint-16/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_45/checkpoint-16/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_45/checkpoint-16/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_45/checkpoint-32\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_45/checkpoint-32/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_45/checkpoint-32/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_45/checkpoint-32/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_45/checkpoint-32/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_45/checkpoint-48\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_45/checkpoint-48/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_45/checkpoint-48/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_45/checkpoint-48/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_45/checkpoint-48/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_45/checkpoint-16] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_45/checkpoint-64\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_45/checkpoint-64/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_45/checkpoint-64/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_45/checkpoint-64/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_45/checkpoint-64/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_45/checkpoint-32] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_45/checkpoint-80\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_45/checkpoint-80/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_45/checkpoint-80/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_45/checkpoint-80/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_45/checkpoint-80/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_45/checkpoint-48] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_45/checkpoint-96\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_45/checkpoint-96/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_45/checkpoint-96/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_45/checkpoint-96/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_45/checkpoint-96/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_45/checkpoint-64] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_45/checkpoint-112\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_45/checkpoint-112/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_45/checkpoint-112/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_45/checkpoint-112/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_45/checkpoint-112/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_45/checkpoint-80] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_45/checkpoint-128\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_45/checkpoint-128/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_45/checkpoint-128/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_45/checkpoint-128/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_45/checkpoint-128/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_45/checkpoint-96] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_45/checkpoint-144\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_45/checkpoint-144/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_45/checkpoint-144/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_45/checkpoint-144/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_45/checkpoint-144/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_45/checkpoint-112] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_45/checkpoint-160\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_45/checkpoint-160/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_45/checkpoint-160/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_45/checkpoint-160/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_45/checkpoint-160/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_45/checkpoint-128] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_45/checkpoint-176\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_45/checkpoint-176/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_45/checkpoint-176/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_45/checkpoint-176/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_45/checkpoint-176/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_45/checkpoint-144] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_45/checkpoint-192\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_45/checkpoint-192/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_45/checkpoint-192/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_45/checkpoint-192/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_45/checkpoint-192/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_45/checkpoint-160] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_45/checkpoint-208\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_45/checkpoint-208/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_45/checkpoint-208/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_45/checkpoint-208/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_45/checkpoint-208/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_45/checkpoint-176] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_45/checkpoint-224\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_45/checkpoint-224/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_45/checkpoint-224/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_45/checkpoint-224/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_45/checkpoint-224/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_45/checkpoint-192] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_45/checkpoint-240\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_45/checkpoint-240/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_45/checkpoint-240/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_45/checkpoint-240/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_45/checkpoint-240/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_45/checkpoint-208] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_45/checkpoint-256\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_45/checkpoint-256/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_45/checkpoint-256/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_45/checkpoint-256/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_45/checkpoint-256/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_45/checkpoint-224] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_45/checkpoint-272\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_45/checkpoint-272/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_45/checkpoint-272/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_45/checkpoint-272/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_45/checkpoint-272/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_45/checkpoint-240] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_45/checkpoint-288\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_45/checkpoint-288/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_45/checkpoint-288/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_45/checkpoint-288/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_45/checkpoint-288/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_45/checkpoint-256] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_45/checkpoint-304\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_45/checkpoint-304/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_45/checkpoint-304/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_45/checkpoint-304/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_45/checkpoint-304/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_45/checkpoint-272] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_45/checkpoint-320\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_45/checkpoint-320/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_45/checkpoint-320/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_45/checkpoint-320/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_45/checkpoint-320/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_45/checkpoint-288] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▇▇▆▅▅▄▄▃▃▃▂▂▂▁▁▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▄▇▅▅▆▅▅▄▆▅▅█▄▅▅▅▅▄▅▁</td></tr><tr><td>eval/samples_per_second</td><td>▅▂▄▄▃▄▄▅▃▃▄▁▅▄▃▄▄▅▃█</td></tr><tr><td>eval/steps_per_second</td><td>▅▂▄▄▃▄▄▅▃▃▄▁▅▄▃▄▄▅▃█</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇█████</td></tr><tr><td>train/learning_rate</td><td>██▇▇▇▆▆▅▅▅▄▄▄▃▃▂▂▂▁▁</td></tr><tr><td>train/loss</td><td>█▇▇▆▅▅▄▄▄▃▃▂▂▂▂▁▁▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.68704</td></tr><tr><td>eval/runtime</td><td>0.0593</td></tr><tr><td>eval/samples_per_second</td><td>3373.471</td></tr><tr><td>eval/steps_per_second</td><td>33.735</td></tr><tr><td>train/epoch</td><td>20.0</td></tr><tr><td>train/global_step</td><td>320</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.6881</td></tr><tr><td>train/total_flos</td><td>212168160000.0</td></tr><tr><td>train/train_loss</td><td>0.68977</td></tr><tr><td>train/train_runtime</td><td>15.4531</td></tr><tr><td>train/train_samples_per_second</td><td>2588.476</td></tr><tr><td>train/train_steps_per_second</td><td>20.708</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">pos syn style 45</strong>: <a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/xafo0p2c\" target=\"_blank\">https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/xafo0p2c</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/scratch/data_jz17d/wandb_tmp/wandb/run-20221114_114140-xafo0p2c/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f937e13a56c440619a3ec3a042ab5861",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70961dad6fd24880a5171f5be2e5ae26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"/scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 32,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 128,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 128,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 54\n",
      "}\n",
      "\n",
      "loading weights file /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/ and are newly initialized: ['bert.pooler.dense.weight', 'classifier.weight', 'bert.pooler.dense.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch/data_jz17d/wandb_tmp/wandb/run-20221114_114205-1l723qjf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/1l723qjf\" target=\"_blank\">pos syn style 46</a></strong> to <a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2000\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 128\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 320\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='320' max='320' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [320/320 00:20, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.692100</td>\n",
       "      <td>0.689766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.689000</td>\n",
       "      <td>0.686538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.686000</td>\n",
       "      <td>0.682857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.682200</td>\n",
       "      <td>0.678597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.678100</td>\n",
       "      <td>0.673559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.673000</td>\n",
       "      <td>0.667761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.667300</td>\n",
       "      <td>0.661230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.661400</td>\n",
       "      <td>0.654160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.654600</td>\n",
       "      <td>0.646626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.647900</td>\n",
       "      <td>0.638902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.640700</td>\n",
       "      <td>0.631328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.634200</td>\n",
       "      <td>0.623984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.627100</td>\n",
       "      <td>0.617166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.621600</td>\n",
       "      <td>0.611018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.615200</td>\n",
       "      <td>0.605610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.611600</td>\n",
       "      <td>0.601145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.607900</td>\n",
       "      <td>0.597636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.604200</td>\n",
       "      <td>0.595102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.603400</td>\n",
       "      <td>0.593599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.601900</td>\n",
       "      <td>0.593080</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_46/checkpoint-16\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_46/checkpoint-16/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_46/checkpoint-16/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_46/checkpoint-16/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_46/checkpoint-16/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_46/checkpoint-32\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_46/checkpoint-32/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_46/checkpoint-32/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_46/checkpoint-32/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_46/checkpoint-32/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_46/checkpoint-48\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_46/checkpoint-48/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_46/checkpoint-48/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_46/checkpoint-48/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_46/checkpoint-48/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_46/checkpoint-16] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_46/checkpoint-64\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_46/checkpoint-64/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_46/checkpoint-64/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_46/checkpoint-64/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_46/checkpoint-64/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_46/checkpoint-32] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_46/checkpoint-80\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_46/checkpoint-80/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_46/checkpoint-80/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_46/checkpoint-80/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_46/checkpoint-80/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_46/checkpoint-48] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_46/checkpoint-96\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_46/checkpoint-96/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_46/checkpoint-96/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_46/checkpoint-96/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_46/checkpoint-96/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_46/checkpoint-64] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_46/checkpoint-112\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_46/checkpoint-112/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_46/checkpoint-112/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_46/checkpoint-112/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_46/checkpoint-112/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_46/checkpoint-80] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_46/checkpoint-128\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_46/checkpoint-128/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_46/checkpoint-128/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_46/checkpoint-128/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_46/checkpoint-128/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_46/checkpoint-96] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_46/checkpoint-144\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_46/checkpoint-144/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_46/checkpoint-144/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_46/checkpoint-144/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_46/checkpoint-144/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_46/checkpoint-112] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_46/checkpoint-160\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_46/checkpoint-160/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_46/checkpoint-160/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_46/checkpoint-160/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_46/checkpoint-160/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_46/checkpoint-128] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_46/checkpoint-176\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_46/checkpoint-176/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_46/checkpoint-176/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_46/checkpoint-176/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_46/checkpoint-176/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_46/checkpoint-144] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_46/checkpoint-192\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_46/checkpoint-192/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_46/checkpoint-192/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_46/checkpoint-192/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_46/checkpoint-192/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_46/checkpoint-160] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_46/checkpoint-208\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_46/checkpoint-208/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_46/checkpoint-208/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_46/checkpoint-208/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_46/checkpoint-208/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_46/checkpoint-176] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_46/checkpoint-224\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_46/checkpoint-224/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_46/checkpoint-224/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_46/checkpoint-224/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_46/checkpoint-224/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_46/checkpoint-192] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_46/checkpoint-240\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_46/checkpoint-240/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_46/checkpoint-240/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_46/checkpoint-240/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_46/checkpoint-240/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_46/checkpoint-208] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_46/checkpoint-256\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_46/checkpoint-256/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_46/checkpoint-256/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_46/checkpoint-256/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_46/checkpoint-256/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_46/checkpoint-224] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_46/checkpoint-272\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_46/checkpoint-272/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_46/checkpoint-272/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_46/checkpoint-272/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_46/checkpoint-272/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_46/checkpoint-240] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_46/checkpoint-288\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_46/checkpoint-288/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_46/checkpoint-288/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_46/checkpoint-288/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_46/checkpoint-288/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_46/checkpoint-256] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_46/checkpoint-304\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_46/checkpoint-304/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_46/checkpoint-304/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_46/checkpoint-304/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_46/checkpoint-304/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_46/checkpoint-272] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_46/checkpoint-320\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_46/checkpoint-320/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_46/checkpoint-320/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_46/checkpoint-320/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_46/checkpoint-320/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_46/checkpoint-288] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>██▇▇▇▆▆▅▅▄▄▃▃▂▂▂▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▄▂▁▄▄█▄▅▄▅▅▄▄▅▄▆▅▄▃▄</td></tr><tr><td>eval/samples_per_second</td><td>▅▇█▄▄▁▅▄▅▄▄▅▄▄▅▃▃▄▅▄</td></tr><tr><td>eval/steps_per_second</td><td>▅▇█▄▄▁▅▄▅▄▄▅▄▄▅▃▃▄▅▄</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇█████</td></tr><tr><td>train/learning_rate</td><td>██▇▇▇▆▆▅▅▅▄▄▄▃▃▂▂▂▁▁</td></tr><tr><td>train/loss</td><td>███▇▇▇▆▆▅▅▄▄▃▃▂▂▁▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.59308</td></tr><tr><td>eval/runtime</td><td>0.0672</td></tr><tr><td>eval/samples_per_second</td><td>2976.489</td></tr><tr><td>eval/steps_per_second</td><td>29.765</td></tr><tr><td>train/epoch</td><td>20.0</td></tr><tr><td>train/global_step</td><td>320</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.6019</td></tr><tr><td>train/total_flos</td><td>212168160000.0</td></tr><tr><td>train/train_loss</td><td>0.64496</td></tr><tr><td>train/train_runtime</td><td>20.1428</td></tr><tr><td>train/train_samples_per_second</td><td>1985.816</td></tr><tr><td>train/train_steps_per_second</td><td>15.887</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">pos syn style 46</strong>: <a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/1l723qjf\" target=\"_blank\">https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/1l723qjf</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/scratch/data_jz17d/wandb_tmp/wandb/run-20221114_114205-1l723qjf/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc1c29a4628a47cab9a75405df91a294",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80639af1cd3b4bcc963feef0776fea77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"/scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 32,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 128,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 128,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 54\n",
      "}\n",
      "\n",
      "loading weights file /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/ and are newly initialized: ['bert.pooler.dense.weight', 'classifier.weight', 'bert.pooler.dense.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch/data_jz17d/wandb_tmp/wandb/run-20221114_114234-uy5ffyf6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/uy5ffyf6\" target=\"_blank\">pos syn style 47</a></strong> to <a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2000\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 128\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 320\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='320' max='320' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [320/320 00:17, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.692300</td>\n",
       "      <td>0.690286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.689800</td>\n",
       "      <td>0.687695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.687400</td>\n",
       "      <td>0.684954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.684600</td>\n",
       "      <td>0.682012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.681900</td>\n",
       "      <td>0.678741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.678600</td>\n",
       "      <td>0.675167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.675100</td>\n",
       "      <td>0.671259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.671600</td>\n",
       "      <td>0.667084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.667700</td>\n",
       "      <td>0.662694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.663500</td>\n",
       "      <td>0.658195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.659100</td>\n",
       "      <td>0.653708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.655200</td>\n",
       "      <td>0.649308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.650900</td>\n",
       "      <td>0.645133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.647500</td>\n",
       "      <td>0.641319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.643100</td>\n",
       "      <td>0.637963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.640900</td>\n",
       "      <td>0.635105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.638800</td>\n",
       "      <td>0.632885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.636100</td>\n",
       "      <td>0.631271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.636100</td>\n",
       "      <td>0.630303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.635200</td>\n",
       "      <td>0.629970</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_47/checkpoint-16\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_47/checkpoint-16/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_47/checkpoint-16/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_47/checkpoint-16/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_47/checkpoint-16/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_47/checkpoint-32\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_47/checkpoint-32/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_47/checkpoint-32/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_47/checkpoint-32/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_47/checkpoint-32/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_47/checkpoint-48\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_47/checkpoint-48/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_47/checkpoint-48/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_47/checkpoint-48/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_47/checkpoint-48/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_47/checkpoint-16] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_47/checkpoint-64\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_47/checkpoint-64/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_47/checkpoint-64/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_47/checkpoint-64/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_47/checkpoint-64/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_47/checkpoint-32] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_47/checkpoint-80\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_47/checkpoint-80/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_47/checkpoint-80/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_47/checkpoint-80/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_47/checkpoint-80/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_47/checkpoint-48] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_47/checkpoint-96\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_47/checkpoint-96/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_47/checkpoint-96/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_47/checkpoint-96/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_47/checkpoint-96/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_47/checkpoint-64] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_47/checkpoint-112\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_47/checkpoint-112/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_47/checkpoint-112/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_47/checkpoint-112/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_47/checkpoint-112/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_47/checkpoint-80] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_47/checkpoint-128\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_47/checkpoint-128/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_47/checkpoint-128/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_47/checkpoint-128/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_47/checkpoint-128/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_47/checkpoint-96] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_47/checkpoint-144\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_47/checkpoint-144/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_47/checkpoint-144/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_47/checkpoint-144/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_47/checkpoint-144/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_47/checkpoint-112] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_47/checkpoint-160\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_47/checkpoint-160/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_47/checkpoint-160/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_47/checkpoint-160/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_47/checkpoint-160/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_47/checkpoint-128] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_47/checkpoint-176\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_47/checkpoint-176/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_47/checkpoint-176/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_47/checkpoint-176/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_47/checkpoint-176/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_47/checkpoint-144] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_47/checkpoint-192\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_47/checkpoint-192/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_47/checkpoint-192/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_47/checkpoint-192/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_47/checkpoint-192/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_47/checkpoint-160] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_47/checkpoint-208\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_47/checkpoint-208/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_47/checkpoint-208/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_47/checkpoint-208/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_47/checkpoint-208/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_47/checkpoint-176] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_47/checkpoint-224\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_47/checkpoint-224/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_47/checkpoint-224/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_47/checkpoint-224/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_47/checkpoint-224/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_47/checkpoint-192] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_47/checkpoint-240\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_47/checkpoint-240/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_47/checkpoint-240/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_47/checkpoint-240/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_47/checkpoint-240/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_47/checkpoint-208] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_47/checkpoint-256\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_47/checkpoint-256/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_47/checkpoint-256/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_47/checkpoint-256/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_47/checkpoint-256/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_47/checkpoint-224] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_47/checkpoint-272\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_47/checkpoint-272/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_47/checkpoint-272/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_47/checkpoint-272/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_47/checkpoint-272/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_47/checkpoint-240] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_47/checkpoint-288\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_47/checkpoint-288/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_47/checkpoint-288/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_47/checkpoint-288/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_47/checkpoint-288/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_47/checkpoint-256] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_47/checkpoint-304\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_47/checkpoint-304/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_47/checkpoint-304/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_47/checkpoint-304/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_47/checkpoint-304/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_47/checkpoint-272] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_47/checkpoint-320\n",
      "Configuration saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_47/checkpoint-320/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_47/checkpoint-320/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_47/checkpoint-320/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_47/checkpoint-320/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_47/checkpoint-288] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>██▇▇▇▆▆▅▅▄▄▃▃▂▂▂▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▂█▇▁▇▆▇▇▆▆▂▅▁▆▆▇█▇▇▆</td></tr><tr><td>eval/samples_per_second</td><td>▇▁▂█▂▃▂▂▃▃▇▄█▂▃▂▁▂▂▃</td></tr><tr><td>eval/steps_per_second</td><td>▇▁▂█▂▃▂▂▃▃▇▄█▂▃▂▁▂▂▃</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇█████</td></tr><tr><td>train/learning_rate</td><td>██▇▇▇▆▆▅▅▅▄▄▄▃▃▂▂▂▁▁</td></tr><tr><td>train/loss</td><td>██▇▇▇▆▆▅▅▄▄▃▃▃▂▂▁▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.62997</td></tr><tr><td>eval/runtime</td><td>0.0657</td></tr><tr><td>eval/samples_per_second</td><td>3045.132</td></tr><tr><td>eval/steps_per_second</td><td>30.451</td></tr><tr><td>train/epoch</td><td>20.0</td></tr><tr><td>train/global_step</td><td>320</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.6352</td></tr><tr><td>train/total_flos</td><td>212168160000.0</td></tr><tr><td>train/train_loss</td><td>0.66176</td></tr><tr><td>train/train_runtime</td><td>17.8615</td></tr><tr><td>train/train_samples_per_second</td><td>2239.448</td></tr><tr><td>train/train_steps_per_second</td><td>17.916</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">pos syn style 47</strong>: <a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/uy5ffyf6\" target=\"_blank\">https://wandb.ai/fsu-dsc-cil/POS%20MLM%20syn%20style/runs/uy5ffyf6</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/scratch/data_jz17d/wandb_tmp/wandb/run-20221114_114234-uy5ffyf6/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = 20\n",
    "batchsize = 128\n",
    "num_styles = 2\n",
    "sentence_length = 15\n",
    "num_trainset = 2000\n",
    "num_testset = 200\n",
    "\n",
    "common_tokens = ['NN', 'VB', 'DT', 'IN', 'JJ', 'PRP', 'RB',]\n",
    "\n",
    "DIFF = [0.1,0.2,0.3,0.4]\n",
    "NUM_SENT = [10, 15, 25]\n",
    "FREEZE_BERT = [0,1,2,3]\n",
    "\n",
    "DIFF, NUM_SENT, FREEZE_BERT = np.meshgrid(DIFF, NUM_SENT, FREEZE_BERT)\n",
    "DIFF, NUM_SENT, FREEZE_BERT = DIFF.flatten(), NUM_SENT.flatten(), FREEZE_BERT.flatten()\n",
    "\n",
    "num_runs = len(DIFF)\n",
    "\n",
    "for i_run in trange(num_runs):\n",
    "    \n",
    "    diff = DIFF[i_run]\n",
    "    sentence_length = NUM_SENT[i_run]\n",
    "    freeze_bert = FREEZE_BERT[i_run]\n",
    "    \n",
    "    # create synthetic POS sequences\n",
    "    dists = get_style_dists(num_styles, diff, tokens=common_tokens)\n",
    "\n",
    "    train_sentences, train_labels = gene_sentences(num_trainset, sentence_length, common_tokens, dists)\n",
    "    test_sentences, test_labels = gene_sentences(num_testset, sentence_length, common_tokens, dists)\n",
    "\n",
    "    train_dataset = Dataset.from_dict({\"text\": train_sentences, 'labels':train_labels})\n",
    "    train_dataset = train_dataset.map(tokenize, batched=True)\n",
    "    train_dataset = train_dataset.remove_columns(['text'])\n",
    "\n",
    "    test_dataset = Dataset.from_dict({\"text\": test_sentences, 'labels':test_labels})\n",
    "    test_dataset = test_dataset.map(tokenize, batched=True)\n",
    "    test_dataset = test_dataset.remove_columns(['text'])\n",
    "    \n",
    "    # init models\n",
    "    model = AutoModelForSequenceClassification.from_pretrained('/scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/', local_files_only=True, num_labels=num_styles)\n",
    "    model = freeze_model(model, freeze_bert)\n",
    "    \n",
    "    # trainer config\n",
    "    training_args = TrainingArguments(\n",
    "#         learning_rate=lr,\n",
    "        output_dir= f\"/scratch/data_jz17d/result/pos_mlm_syn_style/syn_style_{i_run}\",\n",
    "        overwrite_output_dir=True,\n",
    "        num_train_epochs=epochs,\n",
    "        per_device_train_batch_size=batchsize,\n",
    "        per_device_eval_batch_size=batchsize,\n",
    "        evaluation_strategy='epoch',\n",
    "        logging_strategy='epoch',\n",
    "        save_strategy='epoch',\n",
    "#         save_steps=control_steps,\n",
    "#         logging_steps=control_steps,\n",
    "#         eval_steps=control_steps,\n",
    "#         metric_for_best_model='eval_accuracy',\n",
    "        save_total_limit=2,\n",
    "        prediction_loss_only=True,\n",
    "        remove_unused_columns=False,\n",
    "#         report_to='wandb',\n",
    "        )\n",
    "    \n",
    "    # wandb config\n",
    "    wconfig = {}\n",
    "    wconfig['diff'] = diff\n",
    "    wconfig['sentence_length'] = sentence_length\n",
    "    wconfig['freeze_bert'] = freeze_bert\n",
    "    \n",
    "    \n",
    "    run = wandb.init(project=\"POS MLM syn style\", \n",
    "                     entity=\"fsu-dsc-cil\", \n",
    "                     dir='/scratch/data_jz17d/wandb_tmp/', \n",
    "                     config=wconfig,\n",
    "                     name=f'pos syn style {i_run}',\n",
    "                     reinit=True)\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=test_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    trainer.train()\n",
    "        \n",
    "    metric = datasets.load_metric('accuracy')\n",
    "    for x in trainer.get_eval_dataloader(test_dataset):\n",
    "        labels = x['labels']\n",
    "        x = nested_to(x, device)\n",
    "        model_predictions = model(**x)\n",
    "        metric.add_batch(predictions=model_predictions.logits.argmax(axis=-1).cpu().detach().numpy(), references=labels)\n",
    "\n",
    "    accuracy = metric.compute()\n",
    "    wandb.log({'accuracy':accuracy})\n",
    "    run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60298016",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
