{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c714154d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import trange, tqdm\n",
    "import pandas as pd\n",
    "import ast\n",
    "import itertools\n",
    "from itertools import groupby\n",
    "\n",
    "import wandb\n",
    "import evaluate\n",
    "from itertools import cycle\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "\n",
    "from datetime import datetime\n",
    "import collections\n",
    "from sklearn.metrics import top_k_accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2fd70e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GATConv, GATv2Conv, TransformerConv, PDNConv, global_mean_pool, global_max_pool\n",
    "import torch_geometric as pyg\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "import transformers\n",
    "from transformers import get_scheduler\n",
    "from transformers import AutoTokenizer\n",
    "from transformers.models.bert.modeling_bert import BertModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "305b0a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "from torch import Tensor\n",
    "\n",
    "from torch_geometric.utils import scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b05aad7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('4.26.0', '2.2.0')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformers.__version__, pyg.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad6aa276",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "612c538f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "038ca9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "@dataclass\n",
    "class myGNNoutput:\n",
    "    loss: None\n",
    "    logit: None\n",
    "    emb: None\n",
    "    doc_y: None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2c8d880",
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_min_pool(x: Tensor, batch: Optional[Tensor],\n",
    "                    size: Optional[int] = None) -> Tensor:\n",
    "    r\"\"\"Returns batch-wise graph-level-outputs by taking the channel-wise\n",
    "    maximum across the node dimension, so that for a single graph\n",
    "    :math:`\\mathcal{G}_i` its output is computed by\n",
    "\n",
    "    .. math::\n",
    "        \\mathbf{r}_i = \\mathrm{max}_{n=1}^{N_i} \\, \\mathbf{x}_n.\n",
    "\n",
    "    Functional method of the\n",
    "    :class:`~torch_geometric.nn.aggr.MaxAggregation` module.\n",
    "\n",
    "    Args:\n",
    "        x (torch.Tensor): Node feature matrix\n",
    "            :math:`\\mathbf{X} \\in \\mathbb{R}^{(N_1 + \\ldots + N_B) \\times F}`.\n",
    "        batch (torch.Tensor, optional): The batch vector\n",
    "            :math:`\\mathbf{b} \\in {\\{ 0, \\ldots, B-1\\}}^N`, which assigns\n",
    "            each element to a specific example.\n",
    "        size (int, optional): The number of examples :math:`B`.\n",
    "            Automatically calculated if not given. (default: :obj:`None`)\n",
    "    \"\"\"\n",
    "    dim = -1 if x.dim() == 1 else -2\n",
    "\n",
    "    if batch is None:\n",
    "        return x.max(dim=dim, keepdim=x.dim() <= 2)[0]\n",
    "    size = int(batch.max().item() + 1) if size is None else size\n",
    "    return scatter(x, batch, dim=dim, dim_size=size, reduce='min')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7a9f72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# English specific denpendency relations: https://universaldependencies.org/en/dep/\n",
    "s = '''nsubj \tcsubj\n",
    "↳nsubj:pass \t↳csubj:pass\n",
    "↳nsubj:outer \t↳csubj:outer\n",
    "obj \tccomp \txcomp\n",
    "iobj\n",
    "obl \tadvcl \tadvmod\n",
    "↳obl:npmod \t↳advcl:relcl\n",
    "↳obl:tmod\n",
    "vocative \taux \tmark\n",
    "discourse \t↳aux:pass\n",
    "expl \tcop\n",
    "nummod \tacl \tamod\n",
    "  \t↳acl:relcl\n",
    "appos \t  \tdet\n",
    "  \t  \t↳det:predet\n",
    "nmod \t  \t \n",
    "↳nmod:npmod\n",
    "↳nmod:tmod\n",
    "↳nmod:poss\n",
    "compound \tflat\n",
    "↳compound:prt \t↳flat:foreign\n",
    "fixed \tgoeswith\n",
    "conj \tcc\n",
    "  \t↳cc:preconj\n",
    "list \tparataxis \torphan\n",
    "dislocated \t\treparandum\n",
    "root \tpunct \tdep'''\n",
    "all_relations = []\n",
    "s = s.split('\\n')\n",
    "for line in s:\n",
    "    if '↳' in line:\n",
    "        continue\n",
    "    line = line.split('\\t')\n",
    "    for r in line:\n",
    "        if r.strip() == '':\n",
    "            continue\n",
    "        all_relations.append(r.split(':')[0].strip())\n",
    "if 'root' in all_relations:\n",
    "    all_relations.remove('root')\n",
    "    all_relations.append('ROOT')\n",
    "    all_relations.append('case')      # manually add relation not in list\n",
    "    all_relations.append('discourse')    # manually add relation not in list\n",
    "all_relations = sorted(all_relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6aada0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "relation2id = {all_relations[i]:i for i in range(len(all_relations))}\n",
    "relation2id['self'] = 36 # add self loop type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5df71f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_eval = ['edge_indexs', 'hetoro_edges', 'pos_seqs', 'upos_seqs', 'num_syllables', 'alignments']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "33d4f908",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Mapping\n",
    "from typing import List, Optional, Sequence, Union\n",
    "\n",
    "import torch.utils.data\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "\n",
    "from torch_geometric.data import Batch, Dataset\n",
    "from torch_geometric.data.data import BaseData\n",
    "\n",
    "class ParagraphBatchCollater:\n",
    "    def __init__(self, follow_batch, exclude_keys):\n",
    "        self.follow_batch = follow_batch\n",
    "        self.exclude_keys = exclude_keys\n",
    "\n",
    "    def __call__(self, batch): \n",
    "        # batch is a list of lists of BaseData. E.g.\n",
    "        # [[Data(), # para 1\n",
    "        #   Data(),\n",
    "        #   Data()],\n",
    "        #  [Data(), # para 2\n",
    "        #   Data()]]\n",
    "        \n",
    "        # so it needs only one extra step, flatten the list\n",
    "        batch = [item for sublist in batch for item in sublist]\n",
    "        \n",
    "        elem = batch[0]\n",
    "        if isinstance(elem, BaseData):\n",
    "            return Batch.from_data_list(batch, self.follow_batch,\n",
    "                                        self.exclude_keys)\n",
    "        elif isinstance(elem, torch.Tensor):\n",
    "            return default_collate(batch)\n",
    "        elif isinstance(elem, float):\n",
    "            return torch.tensor(batch, dtype=torch.float)\n",
    "        elif isinstance(elem, int):\n",
    "            return torch.tensor(batch)\n",
    "        elif isinstance(elem, str):\n",
    "            return batch\n",
    "        elif isinstance(elem, Mapping):\n",
    "            return {key: self([data[key] for data in batch]) for key in elem}\n",
    "        elif isinstance(elem, tuple) and hasattr(elem, '_fields'):\n",
    "            return type(elem)(*(self(s) for s in zip(*batch)))\n",
    "        elif isinstance(elem, Sequence) and not isinstance(elem, str):\n",
    "            return [self(s) for s in zip(*batch)]\n",
    "\n",
    "        raise TypeError(f'DataLoader found invalid type: {type(elem)}')\n",
    "\n",
    "    def collate(self, batch):  # pragma: no cover\n",
    "        # TODO Deprecated, remove soon.\n",
    "        return self(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99645ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyg source code\n",
    "class ParagraphDataLoader(torch.utils.data.DataLoader):\n",
    "    r\"\"\"A data loader which merges data objects from a\n",
    "    :class:`torch_geometric.data.Dataset` to a mini-batch.\n",
    "    Data objects can be either of type :class:`~torch_geometric.data.Data` or\n",
    "    :class:`~torch_geometric.data.HeteroData`.\n",
    "\n",
    "    Args:\n",
    "        dataset (Dataset): The dataset from which to load the data.\n",
    "        batch_size (int, optional): How many samples per batch to load.\n",
    "            (default: :obj:`1`)\n",
    "        shuffle (bool, optional): If set to :obj:`True`, the data will be\n",
    "            reshuffled at every epoch. (default: :obj:`False`)\n",
    "        follow_batch (List[str], optional): Creates assignment batch\n",
    "            vectors for each key in the list. (default: :obj:`None`)\n",
    "        exclude_keys (List[str], optional): Will exclude each key in the\n",
    "            list. (default: :obj:`None`)\n",
    "        **kwargs (optional): Additional arguments of\n",
    "            :class:`torch.utils.data.DataLoader`.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset: Union[Dataset, Sequence[BaseData]],\n",
    "        batch_size: int = 1,\n",
    "        shuffle: bool = False,\n",
    "        follow_batch: Optional[List[str]] = None,\n",
    "        exclude_keys: Optional[List[str]] = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "\n",
    "        if 'collate_fn' in kwargs:\n",
    "            del kwargs['collate_fn']\n",
    "\n",
    "        # Save for PyTorch Lightning < 1.6:\n",
    "        self.follow_batch = follow_batch\n",
    "        self.exclude_keys = exclude_keys\n",
    "\n",
    "        super().__init__(\n",
    "            dataset,\n",
    "            batch_size,\n",
    "            shuffle,\n",
    "            collate_fn=ParagraphBatchCollater(follow_batch, exclude_keys),\n",
    "            **kwargs,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a90c65c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loader(df, add_syllables=False, col='pos_seqs', limit=None, batch_size=32, shuffle=True, max_length=128):\n",
    "    data_list = []\n",
    "    if limit is not None:\n",
    "        dfnew = df.sample(frac=1).reset_index(drop=True)[:limit]\n",
    "    else:\n",
    "        dfnew = df\n",
    "    data_lists = []\n",
    "    count = 0\n",
    "    \n",
    "    for i in trange(len(dfnew), leave=False):\n",
    "        curr = df.iloc[i]\n",
    "        data_list = []\n",
    "        for j in range(len(curr['edge_indexs'])):\n",
    "            data = Data()\n",
    "            data.edge_index = torch.cat([torch.tensor([[0],[0]]),  # for self loop of CLS token\n",
    "                                         torch.tensor(curr['edge_indexs'][j]).T, \n",
    "                                         # for batching purpose, if data.x is missing, edge_index is used to inference batch\n",
    "                                         # an isolated node (the SEP in this case) will mess all up\n",
    "                                         torch.tensor([[len(curr['edge_indexs'][j])+1],[len(curr['edge_indexs'][j])+1]])], \n",
    "                                        axis=1)\n",
    "            data.edge_type_ids = torch.tensor([36]+[relation2id[t.split(':')[0]] for t in curr['hetoro_edges'][j]]+[36])\n",
    "            if data.edge_index.shape[1] >= max_length-1:\n",
    "                count += 1\n",
    "                continue\n",
    "        \n",
    "            data.text = ' '.join(curr[col][j])\n",
    "            data.y = torch.tensor([curr['author']])\n",
    "            if add_syllables:\n",
    "                data.num_syllables = torch.tensor([17]+curr['num_syllables'][j]+[17])\n",
    "            \n",
    "            if 'doc_id' in curr:\n",
    "                data.doc_id = torch.tensor([curr['doc_id']])\n",
    "\n",
    "            data.num_nodes = len(data.edge_type_ids)\n",
    "            data_list.append(data)\n",
    "        data_lists.append(data_list)\n",
    "        \n",
    "    print(f'{count} data dropped because of exceeding max_length {max_length}')\n",
    "    loader = ParagraphDataLoader(data_lists, batch_size=batch_size, shuffle=shuffle)\n",
    "    return loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9842321c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv, GATv2Conv, TransformerConv, PDNConv, global_mean_pool\n",
    "\n",
    "GNNtype2layer = {'GATConv':GATConv, 'GATv2Conv':GATv2Conv, 'TransformerConv':TransformerConv, 'PDNConv':PDNConv}\n",
    "\n",
    "class HierGNN(torch.nn.Module):\n",
    "    def __init__(self, \n",
    "                 num_layers, \n",
    "                 num_classes, \n",
    "                 num_dep_type, \n",
    "                 heads, \n",
    "                 hidden_dim, \n",
    "                 num_hier_layers=1, \n",
    "                 dep_emb_dim=32, \n",
    "                 add_self_loops=False, \n",
    "                 gnntype='GATConv', \n",
    "                 add_syllables=None,\n",
    "                 checkpoint='/scratch/data_jz17d/result/pos_mlm_corenlp/retrained_256_pos_mlm_0_recovered/',\n",
    "                 max_length=256,\n",
    "                 dropout=0.1):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.checkpoint = checkpoint\n",
    "        self.max_length = max_length\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.checkpoint, local_files_only=True)\n",
    "        self.bert = BertModel.from_pretrained(self.checkpoint, local_files_only=True, add_pooling_layer = False).to(device)\n",
    "        self.num_layers = num_layers\n",
    "        self.num_hier_layers = num_hier_layers\n",
    "        self.num_classes = num_classes\n",
    "        self.pos_emb_dim = 64 # this is determined by POS Bert\n",
    "        self.heads = heads\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dep_emb_dim = dep_emb_dim\n",
    "        self.add_syllables = add_syllables\n",
    "        self.dropout=dropout\n",
    "        \n",
    "        if add_syllables:\n",
    "            self.num_syllables = 18 # the longest word has 17 syllables\n",
    "            self.syllable_emb_layer = nn.Embedding(self.num_syllables, self.pos_emb_dim)\n",
    "            \n",
    "        self.GNNlayer = GNNtype2layer[gnntype]\n",
    "        \n",
    "        self.add_self_loops = add_self_loops\n",
    "        self.dep_emb_layer = nn.Embedding(num_dep_type, self.dep_emb_dim)\n",
    "        \n",
    "        self.gnns = nn.ModuleList()\n",
    "        for i in range(self.num_layers):\n",
    "            self.gnns.append(self.GNNlayer(self.pos_emb_dim, self.pos_emb_dim//self.heads, heads = self.heads, add_self_loops=self.add_self_loops, edge_dim=self.dep_emb_dim, beta=True))\n",
    "            \n",
    "        \n",
    "        # layernorms for above gnn \n",
    "        self.layernorms = nn.ModuleList()\n",
    "        for i in range(self.num_layers):\n",
    "            self.layernorms.append(nn.LayerNorm(self.pos_emb_dim))\n",
    "        \n",
    "        # hierarchical layer\n",
    "        self.transformer_layer1 = TransformerConv(3*self.pos_emb_dim, 3*self.pos_emb_dim//self.heads, heads=self.heads, beta=True)\n",
    "        self.layer_norm1 = nn.LayerNorm(3*self.pos_emb_dim)\n",
    "        \n",
    "        self.transformer_layer2 = TransformerConv(3*self.pos_emb_dim, 3*self.pos_emb_dim//self.heads, heads=self.heads, beta=True)\n",
    "        self.layer_norm2 = nn.LayerNorm(3*self.pos_emb_dim)\n",
    "        \n",
    "        self.classifier = nn.Linear(9*self.pos_emb_dim, self.num_classes)\n",
    "        self.lossfn = nn.CrossEntropyLoss()\n",
    "        \n",
    "    def forward(self, text, edge_index, edge_type_ids, batch, y, doc_id, ptr, num_syllable=None, readout='pool'):\n",
    "        tokens = self.tokenizer(text, padding=True, truncation=True, max_length=self.max_length, return_tensors='pt').to(device)\n",
    "        x = self.bert(**tokens).last_hidden_state\n",
    "        # reshape! drop padded tokens!\n",
    "        x = x.masked_select(tokens.attention_mask.ge(0.5).unsqueeze(2)).reshape((-1,self.pos_emb_dim))\n",
    "        \n",
    "        if self.add_syllables:\n",
    "            syllable_emb = self.syllable_emb_layer(num_syllable)\n",
    "            x = x + syllable_emb\n",
    "            \n",
    "        edge_attr = self.dep_emb_layer(edge_type_ids)\n",
    "        for i in range(self.num_layers):\n",
    "            if i == 0:\n",
    "                x = self.gnns[i](self.layernorms[i](x), edge_index, edge_attr=edge_attr).relu()\n",
    "            else:\n",
    "                x = self.gnns[i](self.layernorms[i](x), edge_index, edge_attr=edge_attr).relu() + x\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        \n",
    "        # readout to get sentence embeddings\n",
    "        if readout == 'pool':\n",
    "            x = torch.cat([global_mean_pool(x, batch), global_max_pool(x, batch), global_min_pool(x, batch)], axis=1)\n",
    "        elif readout == 'cls':\n",
    "            x = x[ptr[:-1],:]\n",
    "        \n",
    "        # calculate edge_index between sentences from the same paragraph\n",
    "        edges_among_sentences = torch.LongTensor().to(device)\n",
    "        doc_y = torch.zeros(doc_id.unique().shape[0], device=device).long()\n",
    "        batch_doc = torch.LongTensor()\n",
    "        for ii, i in enumerate([i for i, j in groupby(doc_id.tolist())]):\n",
    "            idx = (doc_id==i).nonzero().long().squeeze(1) \n",
    "            doc_y[ii] = y[idx[0]]\n",
    "            batch_doc = torch.cat([batch_doc, torch.as_tensor([ii]*len(idx), dtype=torch.long)])\n",
    "            edge_x, edge_y = torch.meshgrid(idx, idx)\n",
    "            edge = torch.vstack([edge_x.flatten(), edge_y.flatten()])\n",
    "            edges_among_sentences = torch.cat([edges_among_sentences, edge], axis = 1)\n",
    "        \n",
    "        x = self.transformer_layer1(self.layer_norm1(x), edges_among_sentences).relu() + x\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.transformer_layer2(self.layer_norm2(x), edges_among_sentences).relu() + x\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        \n",
    "        # readout to get doc embeddings\n",
    "        batch_doc = batch_doc.to(device)\n",
    "        x = torch.cat([global_mean_pool(x, batch_doc), global_max_pool(x, batch_doc), global_min_pool(x, batch_doc)], axis=1)\n",
    "        \n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "        logit = self.classifier(x)\n",
    "        loss = self.lossfn(logit, doc_y)\n",
    "        return myGNNoutput(loss=loss, logit=logit, emb=x, doc_y=doc_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "77062b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hiddensize2checkpoint = {64: \"/scratch/data_jz17d/result/pos_mlm_corenlp/retrained_all_pos_mlm_22/checkpoint-95000/\",\n",
    "                         48: \"/scratch/data_jz17d/result/pos_mlm_corenlp/retrained_all_pos_mlm_16/checkpoint-95000/\",\n",
    "                         32: \"/scratch/data_jz17d/result/pos_mlm_corenlp/retrained_all_pos_mlm_10/checkpoint-145000/\",\n",
    "                         16: \"/scratch/data_jz17d/result/pos_mlm_corenlp/retrained_all_pos_mlm_5/checkpoint-95000/\",}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe50c1d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6c6241b8184435cbd96fd3e9d8b82db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_length = 256\n",
    "\n",
    "epochs = 200\n",
    "warmup_ratio = 0.15\n",
    "monitering_metric = 'accuracy'\n",
    "\n",
    "\n",
    "pos_hidden_dim = 64\n",
    "checkpoint = hiddensize2checkpoint[pos_hidden_dim]\n",
    "\n",
    "LIMIT = [None]\n",
    "NUM_LAYERS = [4]\n",
    "LR = [1e-3]\n",
    "HEADS = [4]\n",
    "READOUT = ['pool']\n",
    "GNNTYPE = ['TransformerConv'] # 'GATConv', 'GATv2Conv', \n",
    "HIDDEN_DIM = [64] # not used\n",
    "DEP_EMB_DIM = [64]\n",
    "NUM_SENT = [3,2,1]\n",
    "ADD_SELF_LOOPS = [False]\n",
    "ADD_SYLLABLES = [True, False]\n",
    "REPEAT = list(range(1))\n",
    "\n",
    "ARGS = itertools.product(LIMIT, NUM_LAYERS, LR, HEADS, READOUT, GNNTYPE, HIDDEN_DIM, DEP_EMB_DIM, NUM_SENT, ADD_SELF_LOOPS, ADD_SYLLABLES, REPEAT)\n",
    "num_runs = len(list(ARGS))\n",
    "run_pbar = trange(num_runs, leave=False)\n",
    "\n",
    "skip_runs = -1\n",
    "ARGS = itertools.product(LIMIT, NUM_LAYERS, LR, HEADS, READOUT, GNNTYPE, HIDDEN_DIM, DEP_EMB_DIM, NUM_SENT, ADD_SELF_LOOPS, ADD_SYLLABLES, REPEAT)\n",
    "for i_run, args in enumerate(ARGS):\n",
    "\n",
    "    if i_run <= skip_runs:\n",
    "        run_pbar.update(1)\n",
    "        continue\n",
    "    limit, num_layers, lr, heads, readout, gnntype, hidden_dim, dep_emb_dim, num_sent_per_text, add_self_loops, add_syllables, repeat = args\n",
    "    \n",
    "    seed = int(datetime.now().timestamp())\n",
    "    set_seed(seed)\n",
    "    \n",
    "    file = f'../../data/CCAT50/processed/author_all_sent_{num_sent_per_text}_0.csv'\n",
    "    df = pd.read_csv(file)\n",
    "    for col in cols_to_eval:\n",
    "        df[col] = df[col].apply(ast.literal_eval)\n",
    "    gb = df.groupby('doc_id')\n",
    "    df_agg = gb.agg({'text': list, 'author':lambda x: x.iloc[0] , 'edge_indexs':list, 'hetoro_edges':list, 'pos_seqs':list, 'upos_seqs':list, 'num_syllables':list})\n",
    "    df_doc_train = df_agg.reset_index()\n",
    "    \n",
    "    file = f'../../data/CCAT50/processed/author_all_sent_{num_sent_per_text}_1.csv'\n",
    "    df_val = pd.read_csv(file)\n",
    "    for col in cols_to_eval:\n",
    "        df_val[col] = df_val[col].apply(ast.literal_eval)\n",
    "    gb = df_val.groupby('doc_id')\n",
    "    df_agg = gb.agg({'text': list, 'author':lambda x: x.iloc[0] , 'edge_indexs':list, 'hetoro_edges':list, 'pos_seqs':list, 'upos_seqs':list, 'num_syllables':list})\n",
    "    df_doc_val = df_agg.reset_index()\n",
    "    \n",
    "    val_docid2index = {doc_id:i for i,doc_id in enumerate(df_val['doc_id'].unique())}\n",
    "    \n",
    "    train_loader = get_loader(df_doc_train, add_syllables=add_syllables, batch_size=4, shuffle=True, max_length=max_length)\n",
    "    num_training_steps = len(train_loader)\n",
    "    valid_loader = get_loader(df_doc_val, add_syllables=add_syllables, batch_size=4, shuffle=True, max_length=max_length)\n",
    "    num_valid_steps = len(valid_loader)\n",
    "    \n",
    "    model = HierGNN(num_layers=num_layers,\n",
    "                    num_classes=50, \n",
    "                    num_dep_type=len(relation2id), \n",
    "                    heads=heads,\n",
    "                    hidden_dim=hidden_dim,\n",
    "                    dep_emb_dim=dep_emb_dim, \n",
    "                    add_self_loops=add_self_loops,\n",
    "                    gnntype=gnntype,\n",
    "                    add_syllables=add_syllables,\n",
    "                    checkpoint=checkpoint,\n",
    "                   )\n",
    "    \n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    scheduler = get_scheduler(\"linear\",\n",
    "                            optimizer=optimizer,\n",
    "                            num_warmup_steps=int(warmup_ratio*epochs*num_training_steps),\n",
    "                            num_training_steps=epochs*num_training_steps)\n",
    "    \n",
    "    wconfig = {}\n",
    "    wconfig['seed'] = seed\n",
    "    wconfig['num_sent_per_text'] = num_sent_per_text\n",
    "    wconfig['limit'] = limit\n",
    "    wconfig['num_layers'] = num_layers\n",
    "    wconfig['lr'] = lr\n",
    "    wconfig['heads'] = heads\n",
    "    wconfig['readout'] = readout\n",
    "    wconfig['GNNtype'] = gnntype\n",
    "    wconfig['add_self_loops'] = add_self_loops\n",
    "    wconfig['add_syllables'] = add_syllables\n",
    "    wconfig['pooling_method'] = 'mean+max+min'\n",
    "    wconfig['checkpoint'] = checkpoint\n",
    "    \n",
    "    \n",
    "    \n",
    "    run = wandb.init(project=\"hierarchical POS GNN\", \n",
    "                     entity=\"fsu-dsc-cil\", \n",
    "                     dir='/scratch/data_jz17d/wandb_tmp/', \n",
    "                     config=wconfig,\n",
    "                     name=f'run_{i_run}',\n",
    "                     reinit=True,\n",
    "                     settings=wandb.Settings(start_method='thread'))\n",
    "    \n",
    "    best_evaluation = collections.defaultdict(float)\n",
    "    pbar = trange(epochs*num_training_steps, leave=False)\n",
    "    for i_epoch in range(epochs):\n",
    "        model.train()\n",
    "        for data in train_loader:\n",
    "            data.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            if add_syllables:\n",
    "                output = model(data.text, data.edge_index, data.edge_type_ids, data.batch, data.y, data.doc_id, data.ptr, data.num_syllables, readout=readout)\n",
    "            else:\n",
    "                output = model(data.text, data.edge_index, data.edge_type_ids, data.batch, data.y, data.doc_id, data.ptr, readout=readout)\n",
    "            output.loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            pbar.update(1)\n",
    "\n",
    "        model.eval()\n",
    "        doc_score = 1e-8*np.ones((len(val_docid2index),50))\n",
    "        doc_true = np.zeros(len(val_docid2index))\n",
    "        metric = evaluate.load('/home/jz17d/Desktop/metrics/accuracy')\n",
    "        for data in valid_loader:\n",
    "            data.to(device)\n",
    "            if add_syllables:\n",
    "                output = model(data.text, data.edge_index, data.edge_type_ids, data.batch, data.y, data.doc_id, data.ptr, data.num_syllables, readout=readout)\n",
    "            else:\n",
    "                output = model(data.text, data.edge_index, data.edge_type_ids, data.batch, data.y, data.doc_id, data.ptr, readout=readout)\n",
    "            metric.add_batch(predictions=output.logit.argmax(axis=-1).cpu().detach().numpy(), references=output.doc_y.cpu().numpy())\n",
    "            \n",
    "#             pred = output.logit.argmax(axis=-1).cpu().detach().numpy()\n",
    "#             doc_id = np.vectorize(val_docid2index.get)(data.doc_id.cpu().detach().numpy()) \n",
    "#             doc_score[doc_id,pred] += 1\n",
    "#             doc_true[doc_id] = data.y.cpu().numpy()\n",
    "        \n",
    "        # logging\n",
    "        evaluation = metric.compute()\n",
    "#         for k in range(1, 6):\n",
    "#             evaluation.update({f'doc_acc@{k}': top_k_accuracy_score(doc_true, doc_score, k=k)})\n",
    "        wandb.log(evaluation, step=pbar.n)\n",
    "        \n",
    "        # logging best\n",
    "        for key in evaluation:\n",
    "            best_evaluation[f'best_{key}'] = max(best_evaluation[f'best_{key}'], evaluation[key])\n",
    "        wandb.log(best_evaluation, step=pbar.n)\n",
    "    \n",
    "    run.finish()\n",
    "    run_pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e5aa6777",
   "metadata": {},
   "outputs": [],
   "source": [
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e32122",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdbd5d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
