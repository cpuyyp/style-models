{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88d2bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from transformers import PreTrainedTokenizer, PreTrainedTokenizerFast\n",
    "from transformers import BertForMaskedLM, BertConfig\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import torch\n",
    "from nltk.parse.corenlp import CoreNLPParser, CoreNLPDependencyParser \n",
    "from nltk.tag.hunpos import HunposTagger\n",
    "from nltk.tokenize import word_tokenize\n",
    "import stanza\n",
    "import nltk\n",
    "import numpy as np\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from numerize import numerize\n",
    "import wandb\n",
    "import os \n",
    "import typing\n",
    "import tokenizers\n",
    "from tqdm.auto import trange, tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e957f02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get tagset \n",
    "def get_pos_vocab(tagger, tagset='xpos'):\n",
    "#     upos_vocab = ['ADJ',\n",
    "#                 'ADP',\n",
    "#                 'ADV',\n",
    "#                 'AUX',\n",
    "#                 'CCONJ',\n",
    "#                 'DET',\n",
    "#                 'INTJ',\n",
    "#                 'NOUN',\n",
    "#                 'NUM',\n",
    "#                 'PART',\n",
    "#                 'PRON',\n",
    "#                 'PROPN',\n",
    "#                 'PUNCT',\n",
    "#                 'SCONJ',\n",
    "#                 'SYM',\n",
    "#                 'VERB',\n",
    "#                 'X']\n",
    "    upos_vocab = ['ADJ',\n",
    "                 'ADP',\n",
    "                 'ADV',\n",
    "                 'CCONJ',\n",
    "                 'DET',\n",
    "                 'INTJ',\n",
    "                 'NOUN',\n",
    "                 'NUM',\n",
    "                 'PART',\n",
    "                 'PRON',\n",
    "                 'PROPN',\n",
    "                 'PUNCT',\n",
    "                 'SYM',\n",
    "                 'VERB',\n",
    "                 'X']\n",
    "    from nltk.data import load\n",
    "    tagdict = load('help/tagsets/upenn_tagset.pickle')\n",
    "    xpos_vocab = list(tagdict.keys())\n",
    "    xpos_vocab = sorted(xpos_vocab)\n",
    "    if tagset=='xpos':\n",
    "        return xpos_vocab\n",
    "    else:\n",
    "        return upos_vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7c0529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this conversion comes from https://universaldependencies.org/tagset-conversion/en-penn-uposf.html\n",
    "# however, it's just impossible to convert to upos without knowing the context. \n",
    "# Manning's word here: https://github.com/UniversalDependencies/docs/issues/212#issuecomment-148846154\n",
    "# how to use Manning's converter: https://github.com/clulab/processors/wiki/Converting-from-Penn-Treebank-to-Basic-Stanford-Dependencies\n",
    "xpos2upos = {'#': 'SYM',\n",
    "             '$': 'SYM',\n",
    "             \"''\": 'PUNCT',\n",
    "             ',': 'PUNCT',\n",
    "             '-LRB-': 'PUNCT',\n",
    "             '-RRB-': 'PUNCT',\n",
    "             '.': 'PUNCT',\n",
    "             ':': 'PUNCT',\n",
    "             'AFX': 'ADJ',\n",
    "             'CC': 'CCONJ',\n",
    "             'CD': 'NUM',\n",
    "             'DT': 'DET',\n",
    "             'EX': 'PRON',\n",
    "             'FW': 'X',\n",
    "             'HYPH': 'PUNCT',\n",
    "             'IN': 'ADP',\n",
    "             'JJ': 'ADJ',\n",
    "             'JJR': 'ADJ',\n",
    "             'JJS': 'ADJ',\n",
    "             'LS': 'X',\n",
    "             'MD': 'VERB',\n",
    "             'NFP': 'PUNCT', # manually added. \n",
    "             'NIL': 'X',\n",
    "             'NN': 'NOUN',\n",
    "             'NNP': 'PROPN',\n",
    "             'NNPS': 'PROPN',\n",
    "             'NNS': 'NOUN',\n",
    "             'PDT': 'DET',\n",
    "             'POS': 'PART',\n",
    "             'PRP': 'PRON',\n",
    "             'PRP$': 'DET',\n",
    "             'RB': 'ADV',\n",
    "             'RBR': 'ADV',\n",
    "             'RBS': 'ADV',\n",
    "             'RP': 'ADP',\n",
    "             'SYM': 'SYM',\n",
    "             'TO': 'PART',\n",
    "             'UH': 'INTJ',\n",
    "             'VB': 'VERB',\n",
    "             'VBD': 'VERB',\n",
    "             'VBG': 'VERB',\n",
    "             'VBN': 'VERB',\n",
    "             'VBP': 'VERB',\n",
    "             'VBZ': 'VERB',\n",
    "             'WDT': 'DET',\n",
    "             'WP': 'PRON',\n",
    "             'WP$': 'DET',\n",
    "             'WRB': 'ADV',\n",
    "             '``': 'PUNCT'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd15d2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenizer(vocab, model_max_length = 128):\n",
    "    # Tokenizer is from tokenizers package. PreTrainedTokenizerFast is from tranformers package.\n",
    "    # PreTrainedTokenizerFast can load vocab saved/trained by Tokenizer\n",
    "    t = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
    "    t.pre_tokenizer = Whitespace()\n",
    "    t.add_special_tokens([\"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\", \"[UNK]\",])\n",
    "\n",
    "    # t.add_tokens(vocab) # this does not work. tokens are not added to t.model, but just t.\n",
    "    trainer = WordLevelTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
    "    data = [' '.join(vocab)]\n",
    "    t.train_from_iterator(data, trainer=trainer)\n",
    "\n",
    "    t.post_processor = TemplateProcessing(\n",
    "        single=\"[CLS] $A [SEP]\",\n",
    "        pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
    "        special_tokens=[\n",
    "            (\"[CLS]\", t.token_to_id(\"[CLS]\")),\n",
    "            (\"[SEP]\", t.token_to_id(\"[SEP]\")),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    t.enable_padding(pad_id=t.token_to_id(\"[PAD]\"), pad_token=\"[PAD]\")\n",
    "    t.enable_truncation(max_length=model_max_length)\n",
    "    t.save('/home/jz17d/Desktop/upos_tokenizer.json')\n",
    "\n",
    "    tokenizer = PreTrainedTokenizerFast(tokenizer_file=\"/home/jz17d/Desktop/upos_tokenizer.json\", unk_token=\"[UNK]\")\n",
    "    tokenizer.pad_token = '[PAD]'\n",
    "    tokenizer.mask_token = '[MASK]'\n",
    "    tokenizer.unk_token = '[UNK]'\n",
    "    special_tokens = {\n",
    "         \"unk_token\": \"[UNK]\",\n",
    "         \"sep_token\": \"[SEP]\",\n",
    "         \"pad_token\": \"[PAD]\",\n",
    "         \"cls_token\": \"[CLS]\",\n",
    "         \"mask_token\": \"[MASK]\" }\n",
    "    tokenizer.add_special_tokens(special_tokens)\n",
    "    # tokenizer.add_special_tokens({'unk_token':'[UNK]'})\n",
    "    tokenizer.model_max_length=model_max_length\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882f7a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load xpos tokenizer, use its convert_ids_to_tokens function later\n",
    "xpos_tokenizer = PreTrainedTokenizerFast(tokenizer_file=\"/home/jz17d/Desktop/my_tokenizer.json\", unk_token=\"[UNK]\")\n",
    "xpos_tokenizer.pad_token = '[PAD]'\n",
    "xpos_tokenizer.mask_token = '[MASK]'\n",
    "xpos_tokenizer.unk_token = '[UNK]'\n",
    "special_tokens = {\n",
    "     \"unk_token\": \"[UNK]\",\n",
    "     \"sep_token\": \"[SEP]\",\n",
    "     \"pad_token\": \"[PAD]\",\n",
    "     \"cls_token\": \"[CLS]\",\n",
    "     \"mask_token\": \"[MASK]\" }\n",
    "xpos_tokenizer.add_special_tokens(special_tokens)\n",
    "# tokenizer.add_special_tokens({'unk_token':'[UNK]'})\n",
    "xpos_tokenizer.model_max_length=model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7a55d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xpos2upos_batch(samples):\n",
    "    new = []\n",
    "    for input_ids in samples['input_ids']:\n",
    "        seq = xpos_tokenizer.convert_ids_to_tokens(input_ids)\n",
    "        for i in range(len(seq)):\n",
    "            if seq[i] in xpos2upos:\n",
    "                seq[i] = xpos2upos[seq[i]]\n",
    "        new.append(tokenizer.convert_tokens_to_ids(seq))\n",
    "    samples['input_ids'] = new\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91752333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect tagset and create tokenizer\n",
    "tagger = 'corenlp'\n",
    "tagset = 'upos'\n",
    "select = 1000000\n",
    "model_max_length = 128\n",
    "\n",
    "vocab = get_pos_vocab(tagger, tagset=tagset)\n",
    "tokenizer = get_tokenizer(vocab, model_max_length = model_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f5a435",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30bee6ee30ad47ddbab89b8d140c3e7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load processed xpos corpus, convert it to upos corpus\n",
    "xpos_cache_location = f'/scratch/data_jz17d/data/bookcorpus/bookcorpus_{numerize.numerize(select).lower()}_{tagger}_transformed.hf'\n",
    "trainset = load_from_disk(xpos_cache_location)\n",
    "\n",
    "trainset = trainset.map(xpos2upos_batch, batched=True)\n",
    "upos_cache_location = f'/scratch/data_jz17d/data/bookcorpus/bookcorpus_{numerize.numerize(select).lower()}_{tagger}_upos_transformed.hf'\n",
    "trainset.save_to_disk(upos_cache_location)\n",
    "\n",
    "trainset = trainset.remove_columns(['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff6c572",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "271a94b147da4c5d8a18556241b43d5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# same for test set\n",
    "xpos_cache_location = f'/scratch/data_jz17d/data/bookcorpus/bookcorpus_50k_{tagger}_transformed_test.hf'\n",
    "testset = load_from_disk(xpos_cache_location)\n",
    "\n",
    "testset = testset.map(xpos2upos_batch, batched=True)\n",
    "upos_cache_location = f'/scratch/data_jz17d/data/bookcorpus/bookcorpus_50k_{tagger}_upos_transformed_test.hf'\n",
    "testset.save_to_disk(upos_cache_location)\n",
    "\n",
    "testset = testset.remove_columns(['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bed9bd",
   "metadata": {},
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a211a20e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2ce33fb8fea474a98994be15f7c25eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcpuyyp\u001b[0m (\u001b[33mfsu-dsc-cil\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.8 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch/data_jz17d/wandb_tmp/wandb/run-20230112_135719-nnluog3g</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20UPOS%20%28simple%20conversion%29/runs/nnluog3g\" target=\"_blank\">run 0</a></strong> to <a href=\"https://wandb.ai/fsu-dsc-cil/POS%20MLM%20UPOS%20%28simple%20conversion%29\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1000000\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 128\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 156260\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7318' max='156260' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  7318/156260 09:58 < 3:23:12, 12.22 it/s, Epoch 0.94/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>1.697400</td>\n",
       "      <td>1.271106</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 50000\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/upos_mlm_corenlp/run_0/checkpoint-5000\n",
      "Configuration saved in /scratch/data_jz17d/result/upos_mlm_corenlp/run_0/checkpoint-5000/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/upos_mlm_corenlp/run_0/checkpoint-5000/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/upos_mlm_corenlp/run_0/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/upos_mlm_corenlp/run_0/checkpoint-5000/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# bert related args\n",
    "model_max_length = 128\n",
    "hidden_size = 32\n",
    "num_attention_heads = 4\n",
    "intermediate_size = 128\n",
    "\n",
    "# training related args\n",
    "control_steps = 5000 # num_steps to log and save\n",
    "num_train_epochs = 20\n",
    "batchsize = 128\n",
    "dropout_prob = 0.1\n",
    "tagger = 'corenlp'\n",
    "\n",
    "NUM_LAYERS = [4]\n",
    "MLM_P = [0.15]\n",
    "LR = [5e-4, 7e-4, 9e-4]\n",
    "\n",
    "NUM_LAYERS, MLM_P, LR = np.meshgrid(NUM_LAYERS, MLM_P, LR)\n",
    "NUM_LAYERS, MLM_P, LR = NUM_LAYERS.flatten(), MLM_P.flatten(), LR.flatten()\n",
    "num_runs = len(LR)\n",
    "\n",
    "for i_run in trange(num_runs):\n",
    "    \n",
    "    num_hidden_layers = int(NUM_LAYERS[i_run])\n",
    "    mlm_probability = float(MLM_P[i_run])\n",
    "    lr = float(LR[i_run])\n",
    "    \n",
    "    # mlm data collater\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer, \n",
    "        mlm=True,\n",
    "        mlm_probability=mlm_probability\n",
    "    )\n",
    "\n",
    "    # model config\n",
    "    config = BertConfig(vocab_size = len(tokenizer.get_vocab()),\n",
    "                        hidden_size = hidden_size,\n",
    "                        num_hidden_layers = num_hidden_layers,\n",
    "                        num_attention_heads = num_attention_heads,\n",
    "                        intermediate_size = intermediate_size,\n",
    "                        hidden_act = 'gelu',\n",
    "                        hidden_dropout_prob = dropout_prob,\n",
    "                        attention_probs_dropout_prob = dropout_prob,\n",
    "                        max_position_embeddings = model_max_length,\n",
    "                        type_vocab_size = 2,\n",
    "                        initializer_range = 0.02,\n",
    "                        layer_norm_eps = 1e-12,\n",
    "                        pad_token_id = tokenizer.pad_token_id)\n",
    "    # init model\n",
    "    bert = BertForMaskedLM(config)\n",
    "\n",
    "    # trainer config\n",
    "    training_args = TrainingArguments(\n",
    "        learning_rate=lr,\n",
    "        output_dir= f\"/scratch/data_jz17d/result/upos_mlm_corenlp/run_{i_run}\",\n",
    "        overwrite_output_dir=True,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        per_device_train_batch_size=batchsize,\n",
    "        per_device_eval_batch_size=batchsize,\n",
    "        evaluation_strategy='steps',\n",
    "        save_steps=control_steps,\n",
    "        logging_steps=control_steps,\n",
    "        eval_steps=control_steps,\n",
    "        save_total_limit=2,\n",
    "        prediction_loss_only=True,\n",
    "        remove_unused_columns=False,\n",
    "#         report_to='wandb',\n",
    "        )\n",
    "    \n",
    "    # wandb config\n",
    "    wconfig = {}\n",
    "    wconfig['num_hidden_layers'] = num_hidden_layers\n",
    "    wconfig['mlm_probability'] = mlm_probability\n",
    "    wconfig['lr'] = lr\n",
    "    run = wandb.init(project=\"POS MLM UPOS (simple conversion)\", \n",
    "                     entity=\"fsu-dsc-cil\", \n",
    "                     dir='/scratch/data_jz17d/wandb_tmp/', \n",
    "                     config=wconfig,\n",
    "                     name=f'run {i_run}',\n",
    "                     reinit=True)\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=bert,\n",
    "        args=training_args,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=trainset,\n",
    "        eval_dataset=testset,\n",
    "    )\n",
    "    trainer.train()\n",
    "    run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3bd8f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
