{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e331d0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import TUDataset, OGB_MAG\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "from nltk.parse.corenlp import CoreNLPParser,CoreNLPDependencyParser\n",
    "from tqdm.auto import trange, tqdm\n",
    "import pandas as pd\n",
    "import ast\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35847d26",
   "metadata": {},
   "source": [
    "# preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e972e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(df_ccat, num_authors_to_pick = None, picked_author_ids = None, num_sent_per_text = None, save_folder = None, train=True):\n",
    "    unique_authors = list(df_ccat['author_id'].unique())\n",
    "    if not picked_author_ids:\n",
    "        picked_author_ids = sorted(np.random.choice(unique_authors, replace=False, size=num_authors_to_pick).tolist())\n",
    "    authors = []\n",
    "    texts = []\n",
    "    for author in picked_author_ids:\n",
    "        df_temp = df_ccat[df_ccat['author_id'] == author]\n",
    "        for i_doc in range(len(df_temp)):\n",
    "            doc = df_temp['text'].iloc[i_doc].split('\\n')\n",
    "            for i in range(len(doc)):\n",
    "                doc[i] = doc[i].strip()\n",
    "            doc.remove('')\n",
    "            for i in range(len(doc)-num_sent_per_text):\n",
    "                authors.append(author)\n",
    "                texts.append(' '.join(doc[i:i+num_sent_per_text]))\n",
    "    df = pd.DataFrame({'author':authors, 'text':texts})\n",
    "    if save_folder:\n",
    "        str_author = ','.join(map(str, picked_author_ids))\n",
    "        file_name = f\"author_{str_author}_sent_{num_sent_per_text}_{'train' if train else 'val'}.csv\"\n",
    "        df.to_csv(f\"{save_folder}/{file_name}\", index=False)\n",
    "        return df, file_name\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f928665d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ccat = pd.read_csv('../../data/CCAT50/processed/CCAT50_train.csv')\n",
    "picked_author_ids = [0,1]\n",
    "num_sent_per_text = 2\n",
    "save_folder = '../../data/CCAT50/processed/'\n",
    "df, file_name = create_dataset(df_ccat, picked_author_ids = picked_author_ids, num_sent_per_text = num_sent_per_text, save_folder = save_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720ef4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ccat = pd.read_csv('../../data/CCAT50/processed/CCAT50_AA_val.csv')\n",
    "picked_author_ids = [0,1]\n",
    "num_sent_per_text = 2\n",
    "save_folder = '../../data/CCAT50/processed/'\n",
    "df, file_name = create_dataset(df_ccat, picked_author_ids = picked_author_ids, num_sent_per_text = num_sent_per_text, save_folder = save_folder, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5986ade3",
   "metadata": {},
   "outputs": [],
   "source": [
    "depparser = CoreNLPDependencyParser(url='http://localhost:9000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0c2889",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dep_edges(texts):\n",
    "    homo_edges = []\n",
    "    hetoro_edges = []\n",
    "    pos_seqs = []\n",
    "    for text in tqdm(texts):\n",
    "        parsed = depparser.raw_parse(text)\n",
    "        conll_dep = next(parsed).to_conll(4)\n",
    "        lines = conll_dep.split('\\n')\n",
    "        homo_edge = []\n",
    "        hetoro_edge = []\n",
    "        pos_seq = []\n",
    "        for i,line in enumerate(lines[:-1]):\n",
    "            l = line.split('\\t')\n",
    "            homo_edge.append([i+1, int(l[2])])\n",
    "            hetoro_edge.append([i+1, int(l[2]), l[3]])\n",
    "            pos_seq.append(l[1])\n",
    "        homo_edges.append(homo_edge)\n",
    "        hetoro_edges.append(hetoro_edge)\n",
    "        pos_seqs.append(pos_seq)\n",
    "    return homo_edges, hetoro_edges, pos_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b5a575",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07eb7bdb30f54816af6d7cfa1f2c82c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1299 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # processing train set\n",
    "# file = '../../data/CCAT50/processed/author_0,1_sent_2.csv'\n",
    "# df = pd.read_csv(file)\n",
    "# homo_edges, hetoro_edges, pos_seqs = get_dep_edges(df['text'])\n",
    "# df['homo_edges'] = homo_edges\n",
    "# df['hetoro_edges'] = hetoro_edges\n",
    "# df['pos_seqs'] = pos_seqs\n",
    "# df.to_csv(file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104bab07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82cdd8db878c4af0a7f83aef81efadd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/324 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # processing val set\n",
    "# file = '../../data/CCAT50/processed/author_0,1_sent_2_val.csv'\n",
    "# df_val = pd.read_csv(file)\n",
    "# homo_edges, hetoro_edges, pos_seqs = get_dep_edges(df_val['text'])\n",
    "# df_val['homo_edges'] = homo_edges\n",
    "# df_val['hetoro_edges'] = hetoro_edges\n",
    "# df_val['pos_seqs'] = pos_seqs\n",
    "# df_val.to_csv(file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1128324c",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = '../../data/CCAT50/processed/author_0,1_sent_2.csv'\n",
    "df = pd.read_csv(file)\n",
    "df['homo_edges'] = df['homo_edges'].apply(ast.literal_eval)\n",
    "df['hetoro_edges'] = df['hetoro_edges'].apply(ast.literal_eval)\n",
    "df['pos_seqs'] = df['pos_seqs'].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e505940",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = '../../data/CCAT50/processed/author_0,1_sent_2_val.csv'\n",
    "df_val = pd.read_csv(file)\n",
    "df_val['homo_edges'] = df_val['homo_edges'].apply(ast.literal_eval)\n",
    "df_val['hetoro_edges'] = df_val['hetoro_edges'].apply(ast.literal_eval)\n",
    "df_val['pos_seqs'] = df_val['pos_seqs'].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddedf44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_model(model, freeze_bert):\n",
    "    '''\n",
    "    if freeze_bert == True, freeze all layer. \n",
    "    if freeze_bert is a positive integer, freeze the bottom {freeze_bert} attention layers\n",
    "    negative integer should also work\n",
    "    '''\n",
    "    if freeze_bert==True:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "    elif isinstance(freeze_bert, (int, np.int32, np.int64, torch.int32, torch.int64)):\n",
    "        for param in model.embeddings.parameters():\n",
    "            param.requires_grad = False  \n",
    "        for layer in model.encoder.layer[:freeze_bert]: \n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = False  \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e4e7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from transformers.models.bert.modeling_bert import BertModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577e472e",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = '/scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint, local_files_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3a5a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /scratch/data_jz17d/result/pos_mlm_corenlp/pos_mlm_8/checkpoint-155000/ were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "bert = BertModel.from_pretrained(checkpoint, local_files_only=True, add_pooling_layer = False)\n",
    "# bert = freeze_model(bert, True)\n",
    "bert = bert.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e4e1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = []\n",
    "for i in range(len(df)):\n",
    "    curr = df.iloc[i]\n",
    "    data = Data()\n",
    "    data.edge_index = torch.tensor(curr['homo_edges']).T\n",
    "    tokens = tokenizer(' '.join(curr['pos_seqs']), padding=True, truncation=True, max_length=128, return_tensors='pt')\n",
    "    data.x = bert(**tokens).last_hidden_state.squeeze(0)\n",
    "    data.y = torch.tensor([curr['author']])\n",
    "    data_list.append(data)\n",
    "    \n",
    "train_loader = DataLoader(data_list, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce3d46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = []\n",
    "for i in range(len(df_val)):\n",
    "    curr = df_val.iloc[i]\n",
    "    data = Data()\n",
    "    data.edge_index = torch.tensor(curr['homo_edges']).T\n",
    "    tokens = tokenizer(' '.join(curr['pos_seqs']), padding=True, truncation=True, max_length=128, return_tensors='pt')\n",
    "    data.x = bert(**tokens).last_hidden_state.squeeze(0)\n",
    "    data.y = torch.tensor([curr['author']])\n",
    "    data_list.append(data)\n",
    "    \n",
    "test_loader = DataLoader(data_list, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9195629f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "@dataclass\n",
    "class myGCNoutput:\n",
    "    loss: None\n",
    "    logit: None\n",
    "    emb: None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6156959a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "\n",
    "pos_emb_dim = 32\n",
    "class myGCN(torch.nn.Module):\n",
    "    def __init__(self, num_gcn, num_class):\n",
    "        super().__init__()\n",
    "        self.num_gcn = num_gcn\n",
    "        self.num_class = num_class\n",
    "        \n",
    "        self.gcns = nn.ModuleList()\n",
    "        for i in range(num_gcn):\n",
    "            self.gcns.append(GCNConv(pos_emb_dim, pos_emb_dim))\n",
    "        \n",
    "        self.classifier = nn.Linear(pos_emb_dim, num_class)\n",
    "        self.lossfn = nn.CrossEntropyLoss()\n",
    "        \n",
    "    def forward(self, x, edge_index, batch, y, ptr, readout='pool'):\n",
    "        for i in range(self.num_gcn):\n",
    "            x = self.gcns[i](x, edge_index)\n",
    "            x = F.relu(x)\n",
    "        \n",
    "        if readout == 'pool':\n",
    "            x = global_mean_pool(x, batch) \n",
    "        elif readout == 'cls':\n",
    "            x = x[ptr[:-1],:]\n",
    "        \n",
    "        x = F.dropout(x, training=self.training)\n",
    "        logit = self.classifier(x)\n",
    "        loss = self.lossfn(logit, y)\n",
    "        return myGCNoutput(loss=loss, logit=logit, emb=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2acdcb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_GCN = [1,2,3,4]\n",
    "LR = [1e-4, 5e-4, 1e-5]\n",
    "for num_gcn, lr in itertools.product():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd56076",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = myGCN(2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5169552",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "myGCNoutput(loss=tensor(0.6419, grad_fn=<NllLossBackward0>), logit=tensor([[ 0.1829,  0.0710],\n",
       "        [ 0.1230, -0.0620],\n",
       "        [ 0.0836,  0.0427],\n",
       "        [ 0.4229,  0.0772],\n",
       "        [ 0.1335,  0.1438],\n",
       "        [ 0.3159,  0.3549],\n",
       "        [ 0.2732,  0.0921],\n",
       "        [ 0.4138,  0.0917],\n",
       "        [ 0.4260,  0.0989],\n",
       "        [ 0.1684,  0.2587],\n",
       "        [ 0.3383,  0.1021],\n",
       "        [ 0.3610,  0.1949],\n",
       "        [ 0.1934,  0.3034],\n",
       "        [ 0.2862,  0.0408],\n",
       "        [ 0.4418,  0.1869],\n",
       "        [ 0.3754, -0.0122],\n",
       "        [ 0.4137,  0.4449],\n",
       "        [ 0.2167,  0.0185],\n",
       "        [ 0.3180,  0.3312],\n",
       "        [ 0.0681,  0.0132],\n",
       "        [ 0.4914,  0.1193],\n",
       "        [ 0.0255,  0.0149],\n",
       "        [ 0.0727,  0.0960],\n",
       "        [ 0.0630,  0.2063],\n",
       "        [ 0.1948,  0.0625],\n",
       "        [ 0.1778, -0.0577],\n",
       "        [ 0.1658, -0.0850],\n",
       "        [ 0.2723,  0.2755],\n",
       "        [ 0.3028,  0.2816],\n",
       "        [ 0.1967,  0.3544],\n",
       "        [ 0.1140,  0.0819],\n",
       "        [ 0.1865,  0.1038]], grad_fn=<AddmmBackward0>), emb=tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0040, 0.0000, 0.4483],\n",
       "        [0.0000, 0.0000, 0.0979,  ..., 0.0130, 0.0000, 0.4053],\n",
       "        [0.2689, 0.8213, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        ...,\n",
       "        [0.0000, 1.3723, 0.0000,  ..., 0.0054, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0168, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.8272]],\n",
       "       grad_fn=<MulBackward0>))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model(data.x, data.edge_index, data.batch, data.y, data.ptr, readout='pool')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0bcfb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
