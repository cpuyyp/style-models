{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "567d35eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# self defined library\n",
    "from data_utils import keystoint, get_seg_loader # other unneeded definitions: MyData, MyDataset, SegmentBatchCollater, SegmentDataLoader, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c714154d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import trange, tqdm\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Optional, Sequence, Union\n",
    "\n",
    "import collections\n",
    "import itertools\n",
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import wandb\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch_geometric as pyg\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GATConv, GATv2Conv, TransformerConv, PDNConv, global_mean_pool, global_max_pool\n",
    "from torch_geometric.utils import scatter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "\n",
    "import transformers\n",
    "from transformers import get_scheduler, AutoTokenizer\n",
    "from transformers.models.bert.modeling_bert import BertModel\n",
    "\n",
    "import evaluate\n",
    "from sklearn.metrics import top_k_accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f4aa9c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('4.26.0', '2.2.0', '1.13.1', device(type='cuda'))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "# device = torch.device(\"cpu\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "transformers.__version__, pyg.__version__, torch.__version__,device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "70da8683",
   "metadata": {},
   "source": [
    "# definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "612c538f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7a9f72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class MyConfig:\n",
    "    # dataset configs\n",
    "    modelname: str\n",
    "    dataset: str\n",
    "    num_classes: int \n",
    "    segment_length: int = 'doc'\n",
    "\n",
    "    # model archtectures\n",
    "    bert_emb_dim: int = 768\n",
    "    pos_emb_dim: int = 64 # this is determined by the pretrained pos bert\n",
    "    dep_emb_dim: int = 32 # edge attribute dim\n",
    "    bert_pos_fuse: str = 'after hier' # how embeddings fuse togethor. 'token', 'before hier', or 'after hier'\n",
    "    hidden_dim: int = field(init=False)\n",
    "    # these 4 parameters below are not changable\n",
    "    num_dep_type: int = 37 # this is determined by dependency2id\n",
    "    # max syllable for common word is 17. However, the longest word in English is a protein that has 189819 letters! \n",
    "    # That must have much more syllables. Truncate to 32 to simplify.\n",
    "    max_num_syllables: int = 32  \n",
    "    max_sentence_num: int = 64 \n",
    "    # zipf frequency bins. 0-8 stands for its frequency between 10**(x-1) and 10**x. 9 is for punctuations. 10 is for CLS and SEP\n",
    "    num_freq_type: int = 11\n",
    "\n",
    "    num_layers: int = 4\n",
    "    heads: int = 4\n",
    "    num_hierarchy: int = 1\n",
    "\n",
    "    add_self_loops: bool = False\n",
    "    add_syllables: bool = True\n",
    "    add_word_freq: bool = True\n",
    "    add_dep: bool = True\n",
    "    add_sentence_order: bool = True # only if num_hierarchy > 0\n",
    "    \n",
    "    # training configs\n",
    "    max_length: int = 256 # this is only for pos tokenizer, bert tokenizer use default 512\n",
    "    dropout: float = 0.1\n",
    "    batch_size: int = 16\n",
    "    epochs: int = 100\n",
    "    warmup_ratio: float = 0.15\n",
    "    lr: float = 2e-3\n",
    "    save: bool = False\n",
    "    save_location: str = field(init=False)\n",
    "\n",
    "    # pretrained checkpoints\n",
    "    pos_checkpoint: str = field(init=False)\n",
    "    bert_checkpoint: str = 'bert-base-uncased'\n",
    "    freeze_bert: int = 10 # how many bert layers to freeze\n",
    "\n",
    "    def __post_init__(self):\n",
    "        assert self.pos_emb_dim % self.heads==0, 'make sure pos_emb_dim is dividable to heads'\n",
    "        self.hidden_dim = self.pos_emb_dim//self.heads\n",
    "\n",
    "        pos_emb_dim2pos_checkpoint = {64: \"/scratch/data_jz17d/result/pos_mlm_corenlp/retrained_all_pos_mlm_22/checkpoint-95000/\",\n",
    "                                    48: \"/scratch/data_jz17d/result/pos_mlm_corenlp/retrained_all_pos_mlm_16/checkpoint-95000/\",\n",
    "                                    32: \"/scratch/data_jz17d/result/pos_mlm_corenlp/retrained_all_pos_mlm_10/checkpoint-145000/\",\n",
    "                                    16: \"/scratch/data_jz17d/result/pos_mlm_corenlp/retrained_all_pos_mlm_5/checkpoint-95000/\",}\n",
    "        self.pos_checkpoint = pos_emb_dim2pos_checkpoint[self.pos_emb_dim]\n",
    "\n",
    "        self.save_location = None if not self.save else f'/scratch/data_jz17d/result/{self.modelname} {self.dataset}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "038ca9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class myGNNoutput:\n",
    "    loss: None\n",
    "    logit: None\n",
    "    emb: None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aac635a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GNNtype2layer = {'GATConv':GATConv, 'GATv2Conv':GATv2Conv, 'TransformerConv':TransformerConv, 'PDNConv':PDNConv}\n",
    "\n",
    "class MyGNNBlock(torch.nn.Module):\n",
    "    def __init__(self, \n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 heads,\n",
    "                 dropout=0.1,\n",
    "                 dropout_position='last',\n",
    "                 **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layernorm = nn.LayerNorm(in_channels)\n",
    "        self.gnnlayer = TransformerConv(in_channels=in_channels, out_channels=out_channels, heads=heads, beta=True, **kwargs)\n",
    "        self.dropout = dropout\n",
    "        self.dropout_position = dropout_position\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr=None):\n",
    "        if self.dropout_position=='last':\n",
    "            x = x + self.gnnlayer(self.layernorm(x), edge_index, edge_attr).relu()\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        elif self.dropout_position=='first':\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "            x = x + self.gnnlayer(self.layernorm(x), edge_index, edge_attr).relu()\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9842321c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HierGNN(torch.nn.Module):\n",
    "    def __init__(self, myconfig):\n",
    "        super().__init__()\n",
    "        self.num_classes = myconfig.num_classes\n",
    "    \n",
    "        # model archtectures\n",
    "        self.bert_emb_dim = myconfig.bert_emb_dim\n",
    "        self.pos_emb_dim = myconfig.pos_emb_dim\n",
    "        self.dep_emb_dim = myconfig.dep_emb_dim\n",
    "        self.hidden_dim = myconfig.hidden_dim\n",
    "        self.bert_pos_fuse = myconfig.bert_pos_fuse\n",
    "        \n",
    "        # these 4 parameters are not changable\n",
    "        self.num_dep_type = myconfig.num_dep_type\n",
    "        self.max_num_syllables = myconfig.max_num_syllables\n",
    "        self.max_sentence_num = myconfig.max_sentence_num\n",
    "        self.num_freq_type = myconfig.num_freq_type\n",
    "\n",
    "        self.num_layers = myconfig.num_layers\n",
    "        self.heads = myconfig.heads\n",
    "        self.num_hierarchy = myconfig.num_hierarchy\n",
    "        \n",
    "        assert self.bert_pos_fuse.endswith('hier') != self.num_hierarchy > 0, 'hierarchy settings mismatch'\n",
    "\n",
    "        self.add_self_loops = myconfig.add_self_loops\n",
    "        self.add_syllables = myconfig.add_syllables\n",
    "        self.add_word_freq = myconfig.add_word_freq\n",
    "        self.add_dep = myconfig.add_dep\n",
    "        self.add_sentence_order = myconfig.add_sentence_order\n",
    "\n",
    "        # model misc\n",
    "        self.max_length = myconfig.max_length # this is for pos tokenizer\n",
    "        self.dropout = myconfig.dropout\n",
    "\n",
    "        # pretrained checkpoints\n",
    "        self.pos_checkpoint = myconfig.pos_checkpoint\n",
    "        self.bert_checkpoint = myconfig.bert_checkpoint\n",
    "\n",
    "        # loading pretrained models\n",
    "        self.bert_tokenizer = AutoTokenizer.from_pretrained(self.bert_checkpoint)\n",
    "        self.bert = BertModel.from_pretrained(self.bert_checkpoint, add_pooling_layer = False).to(device)\n",
    "        self.pos_tokenizer = AutoTokenizer.from_pretrained(self.pos_checkpoint, local_files_only=True)\n",
    "        self.pos_bert = BertModel.from_pretrained(self.pos_checkpoint, local_files_only=True, add_pooling_layer = False).to(device)\n",
    "        \n",
    "        # embedding layers\n",
    "        if self.add_syllables:\n",
    "            # the longest word in the world has 17 syllables. However, if either processing error or people speak like that, such there is no space between words, error will arise.\n",
    "            self.syllable_emb_layer = nn.Embedding(self.max_num_syllables, self.pos_emb_dim)\n",
    "        if self.add_word_freq:\n",
    "            self.freq_emb_layer = nn.Embedding(self.num_freq_type, self.pos_emb_dim)\n",
    "        if self.add_dep:\n",
    "            self.dep_emb_layer = nn.Embedding(self.num_dep_type, self.dep_emb_dim)\n",
    "        \n",
    "        # determine dimension size for gnn layers\n",
    "        if self.bert_pos_fuse == 'token':\n",
    "            self.gnn_dim = self.bert_emb_dim + self.pos_emb_dim\n",
    "        else: \n",
    "            self.gnn_dim = self.pos_emb_dim\n",
    "        self.gnn_hidden = self.gnn_dim//self.heads \n",
    "\n",
    "        # gnns within sentences\n",
    "        self.gnns = nn.ModuleList()\n",
    "        for i in range(self.num_layers):\n",
    "            if self.add_dep:\n",
    "                self.gnns.append(MyGNNBlock(self.gnn_dim, self.gnn_hidden, heads = self.heads, add_self_loops=self.add_self_loops, dropout=self.dropout, edge_dim=self.dep_emb_dim))\n",
    "            else:\n",
    "                self.gnns.append(MyGNNBlock(self.gnn_dim, self.gnn_hidden, heads = self.heads, add_self_loops=self.add_self_loops, dropout=self.dropout))\n",
    "\n",
    "        if self.num_hierarchy:\n",
    "            # for sentence order\n",
    "            # the longest text has 104 lines. how to deal with super long text?\n",
    "            if self.add_sentence_order:\n",
    "                self.sentence_position_emb_layer = nn.Embedding(self.max_sentence_num, 2*self.gnn_dim) \n",
    "\n",
    "            # determine dimension size for hier layers\n",
    "            if self.bert_pos_fuse == 'token':\n",
    "                self.hier_dim = 2*(self.bert_emb_dim + self.pos_emb_dim)\n",
    "            elif self.bert_pos_fuse == 'before hier':\n",
    "                self.hier_dim = self.bert_emb_dim + 2*self.pos_emb_dim\n",
    "            elif self.bert_pos_fuse == 'after hier':\n",
    "                self.hier_dim = 2*self.pos_emb_dim\n",
    "            self.hier_hidden = self.hier_dim//self.heads \n",
    "\n",
    "            # hierarchical layer\n",
    "            self.hierarchy_gnns = nn.ModuleList()\n",
    "            for i in range(self.num_hierarchy):\n",
    "                self.hierarchy_gnns.append(MyGNNBlock(self.hier_dim, self.hier_hidden, heads = self.heads, add_self_loops=self.add_self_loops, dropout=self.dropout))\n",
    "            \n",
    "        # determine dimension size for cls layer\n",
    "        if self.bert_pos_fuse == 'token' and self.num_hierarchy:\n",
    "            self.cls_dim = 4*(self.bert_emb_dim + self.pos_emb_dim)\n",
    "        elif self.bert_pos_fuse.endswith('hier'):\n",
    "            self.cls_dim = 2*self.bert_emb_dim + 4*self.pos_emb_dim \n",
    "        elif not self.num_hierarchy:\n",
    "            self.cls_dim = 2*(self.bert_emb_dim + self.pos_emb_dim)\n",
    "\n",
    "        self.classifier = nn.Linear(self.cls_dim, self.num_classes)\n",
    "        self.lossfn = nn.CrossEntropyLoss()\n",
    "        \n",
    "    def forward(self, text, pos, alignments, edge_index, edge_type_ids, batch, ptr, y, segment_ids, num_syllable, word_freq):\n",
    "        # get pos embeddings, reshape and squeeze the dimension 0 to match pyg batching fashion\n",
    "        # x.shape = (sum of #sentence, max_length, pos_emb_dim)\n",
    "        tokens = self.pos_tokenizer(pos, padding=True, truncation=True, max_length=self.max_length, return_tensors='pt').to(device)\n",
    "        x = self.pos_bert(**tokens).last_hidden_state\n",
    "        # reshape! drop padded tokens!\n",
    "        # x.shape = (sum of #token, pos_emb_dim)\n",
    "        x = x.masked_select(tokens.attention_mask.ge(0.5).unsqueeze(2)).reshape((-1,self.pos_emb_dim))\n",
    "        \n",
    "        # add syllables embedding to pos embeddings\n",
    "        if self.add_syllables:\n",
    "            x = x + self.syllable_emb_layer(torch.clip(num_syllable, max=self.max_num_syllables-1)) # clip to make sure no error\n",
    "\n",
    "        # add freq embedding to pos embeddings\n",
    "        if self.add_word_freq:\n",
    "            x = x + self.freq_emb_layer(word_freq)\n",
    "\n",
    "        # get bert embeddings, reshape and squeeze the dimension 0 to match pyg batching fashion\n",
    "        # bert_x.shape = (sum of #sentence, max_length, bert_emb_dim)\n",
    "        bert_tokens = self.bert_tokenizer(text, padding=True, truncation=True, max_length=512, return_tensors='pt').to(device)\n",
    "        bert_x = self.bert(**bert_tokens).last_hidden_state\n",
    "        \n",
    "        if self.bert_pos_fuse == 'token':\n",
    "            bert_x = bert_x.masked_select(bert_tokens.attention_mask.ge(0.5).unsqueeze(2)).reshape((-1,self.bert_emb_dim))\n",
    "            zero_emb = torch.zeros(alignments[-1]+1, self.bert_emb_dim).to(device)\n",
    "            bert_x = zero_emb.index_reduce(0, alignments, bert_x, 'mean', include_self=False)\n",
    "            x = torch.concat([bert_x, x], axis=1) \n",
    "        elif self.bert_pos_fuse.endswith('hier'):\n",
    "            bert_x = bert_x[:,0,:] # CLS token\n",
    "\n",
    "        # get edge embeddings\n",
    "        if self.add_dep:\n",
    "            edge_attr = self.dep_emb_layer(edge_type_ids)\n",
    "\n",
    "        # graph conv\n",
    "        for i in range(self.num_layers):\n",
    "            x = x + self.gnns[i](x, edge_index, edge_attr=edge_attr).relu() if self.add_dep else self.gnns[i](x, edge_index).relu()\n",
    "        \n",
    "        if self.num_hierarchy:\n",
    "            # readout to get sentence embeddings\n",
    "            # x.shape = (#sentence, pos_emb_dim*2)\n",
    "            non_zero_i, non_zero_j = tokens.attention_mask.nonzero(as_tuple=True)\n",
    "            # the input batch is segment level batch indices. Need sentence level batch indices here\n",
    "            sent_batch = (((torch.arange(len(pos)).to(device)+1).unsqueeze(1)*tokens.attention_mask)[non_zero_i, non_zero_j] - 1)\n",
    "            x = torch.cat([global_mean_pool(x, sent_batch), global_max_pool(x, sent_batch)], axis=1)\n",
    "\n",
    "            if self.bert_pos_fuse == 'before hier':\n",
    "                x = torch.cat([bert_x, x], axis=1)\n",
    "                \n",
    "            # calculate edge_index between sentences from the same paragraph\n",
    "            edges_among_sentences = torch.LongTensor().to(device)\n",
    "            if self.add_sentence_order: \n",
    "                sentence_id = torch.LongTensor()\n",
    "            for i in range(segment_ids.max().item()+1):\n",
    "                idx = (segment_ids==i).nonzero().long().squeeze(1)  # select all sentence id belong to current segment\n",
    "                edge_x, edge_y = torch.meshgrid(idx, idx)\n",
    "                edge = torch.vstack([edge_x.flatten(), edge_y.flatten()])\n",
    "                edges_among_sentences = torch.cat([edges_among_sentences, edge], axis = 1)\n",
    "                if self.add_sentence_order: \n",
    "                    sentence_id = torch.cat([sentence_id, torch.arange(len(idx), dtype=torch.long)])\n",
    "            \n",
    "            # add sentence position\n",
    "            if self.add_sentence_order:\n",
    "                sentence_id = sentence_id.to(device)\n",
    "                x = x + self.sentence_position_emb_layer(torch.clip(sentence_id, max=self.max_sentence_num-1))\n",
    "\n",
    "            if self.bert_pos_fuse == 'after hier':\n",
    "                x = torch.cat([bert_x, x], axis=1)\n",
    "\n",
    "            # hierarchical layers\n",
    "            for i in range(self.num_hierarchy):\n",
    "                x = x + self.hierarchy_gnns[i](x, edges_among_sentences).relu()\n",
    "\n",
    "            # readout to get segment/doc embeddings\n",
    "            # x.shape = (#segment/#doc, pos_emb_dim*4)\n",
    "            x = torch.cat([global_mean_pool(x, segment_ids), global_max_pool(x, segment_ids)], axis=1)\n",
    "\n",
    "        else: \n",
    "            # readout to get segment/doc embeddings\n",
    "            # x.shape = (#segment/#doc, pos_emb_dim*2)\n",
    "            x = torch.cat([global_mean_pool(x, batch), global_max_pool(x, batch)], axis=1)\n",
    "\n",
    "        # prepare logits and output\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        logit = self.classifier(x)\n",
    "        loss = self.lossfn(logit, y)\n",
    "        return myGNNoutput(loss=loss, logit=logit, emb=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f41febb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model_folder, model, optimizer, scheduler):\n",
    "    torch.save(model.state_dict(), f\"{model_folder}/pytorch_model.bin\")\n",
    "    torch.save(optimizer.state_dict(), f\"{model_folder}/optimizer.pt\")\n",
    "    torch.save(scheduler.state_dict(), f\"{model_folder}/scheduler.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f8cb4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_model(model, freeze_bert):\n",
    "    '''\n",
    "    if freeze_bert is True, freeze all layer. \n",
    "    if freeze_bert is a positive integer, freeze the bottom {freeze_bert} attention layers\n",
    "    negative integer should also work\n",
    "    '''\n",
    "    if freeze_bert is True: # == True is wrong!!!\n",
    "        for param in model.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "    elif freeze_bert is False: # isinstance(False, int) returns True!\n",
    "        return model\n",
    "    elif isinstance(freeze_bert, (int, np.int32, np.int64, torch.int32, torch.int64)):\n",
    "        for param in model.bert.embeddings.parameters():\n",
    "            param.requires_grad = False  \n",
    "        for layer in model.bert.encoder.layer[:freeze_bert]: \n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = False  \n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c385e825",
   "metadata": {},
   "source": [
    "# run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "470911fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_configs(dataset, num_classes, exclude_keys=['repeat'], **kwargs):\n",
    "    '''\n",
    "    If want to try different settings, give a list. Otherwise, just a number/str.\n",
    "    '''\n",
    "    keys = []\n",
    "    values = []\n",
    "    direct_kwargs = {}\n",
    "    for k,v in kwargs.items():\n",
    "        if k.lower() not in exclude_keys:\n",
    "            assert k.lower() in MyConfig.__dict__['__annotations__'], f\"{k} doesn't match any MyConfig option\"\n",
    "        if isinstance(v, list):\n",
    "            keys.append(k)\n",
    "            values.append(v)\n",
    "        else:\n",
    "            direct_kwargs[k]=v\n",
    "\n",
    "    CONFIGS = itertools.product(*values)\n",
    "    config_lists = []\n",
    "    for raw_config in CONFIGS:\n",
    "        myconfig = MyConfig(dataset=dataset, num_classes=num_classes, **direct_kwargs)\n",
    "        for k,v in zip(keys, raw_config):\n",
    "            if k.lower() not in exclude_keys:\n",
    "                myconfig.__dict__[k.lower()] = v\n",
    "        myconfig.__post_init__()\n",
    "        config_lists.append(myconfig)\n",
    "    return config_lists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5fe50c1d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a763e66144e449bcae929a9118ed4f32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'bert.pooler.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at /scratch/data_jz17d/result/pos_mlm_corenlp/retrained_all_pos_mlm_22/checkpoint-95000/ were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcpuyyp\u001b[0m (\u001b[33mfsu-dsc-cil\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch/data_jz17d/wandb_tmp/wandb/run-20230314_175200-55e8si67</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/fsu-dsc-cil/Bert%20%2B%20POS%20ccat50/runs/55e8si67\" target=\"_blank\">run_0</a></strong> to <a href=\"https://wandb.ai/fsu-dsc-cil/Bert%20%2B%20POS%20ccat50\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href=\"https://wandb.ai/fsu-dsc-cil/Bert%20%2B%20POS%20ccat50\" target=\"_blank\">https://wandb.ai/fsu-dsc-cil/Bert%20%2B%20POS%20ccat50</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href=\"https://wandb.ai/fsu-dsc-cil/Bert%20%2B%20POS%20ccat50/runs/55e8si67\" target=\"_blank\">https://wandb.ai/fsu-dsc-cil/Bert%20%2B%20POS%20ccat50/runs/55e8si67</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23aeb4d4271d49feb75c80d0dda0a53e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/258100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jz17d/anaconda3/envs/torch113/lib/python3.9/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/TensorShape.cpp:3190.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "modelname = 'Bert + POS'\n",
    "dataset='ccat50'\n",
    "num_classes=50\n",
    "\n",
    "scratch_data_dir = '/scratch/data_jz17d/data'\n",
    "dataset_dir = f'{scratch_data_dir}/{dataset}'\n",
    "\n",
    "config_lists = get_configs(modelname=modelname,\n",
    "                           dataset=dataset, \n",
    "                           num_classes=num_classes,\n",
    "                           num_hierarchy=[1],\n",
    "                           segment_length=[2, 3, 4, 'doc'],\n",
    "                           add_sentence_order=True,\n",
    "                           bert_pos_fuse=['token', 'before hier', 'after hier'],\n",
    "                           save=True,\n",
    "                           )\n",
    "\n",
    "skip_runs = -1\n",
    "######################## in most cases, no need to edit the section below ##########################\n",
    "run_pbar = trange(len(config_lists), leave=False)\n",
    "for i_run, myconfig in enumerate(config_lists):\n",
    "\n",
    "    if i_run <= skip_runs:\n",
    "        run_pbar.update(1)\n",
    "        continue\n",
    "    \n",
    "    seed = int(datetime.now().timestamp())\n",
    "    set_seed(seed)\n",
    "    \n",
    "    # load necessary files and dataset\n",
    "    doc_true = np.load(f'{dataset_dir}/doc_true.npy')\n",
    "    with open(f'{dataset_dir}/test_docid2index.json') as f:\n",
    "        test_docid2index = json.load(f, object_hook=keystoint)\n",
    "    \n",
    "    train_loader = get_seg_loader(dataset=dataset, segment_length=myconfig.segment_length, split='train', batch_size=myconfig.batch_size, shuffle=True, max_length=myconfig.max_length)\n",
    "    num_training_steps = len(train_loader)\n",
    "    test_loader = get_seg_loader(dataset=dataset, segment_length=myconfig.segment_length, split='test', batch_size=myconfig.batch_size, shuffle=True, max_length=myconfig.max_length)\n",
    "    # num_test_steps = len(test_loader)\n",
    "    \n",
    "    # initialize model, optimizere, and lr scheduler\n",
    "    model = HierGNN(myconfig)\n",
    "    model = model.to(device)\n",
    "    model = freeze_model(model, myconfig.freeze_bert)    \n",
    "\n",
    "    optimizer = torch.optim.Adam([p for p in model.parameters() if p.requires_grad==True], lr=myconfig.lr)\n",
    "    # paras, other_para = [], []\n",
    "    # for name, module in model.named_children():\n",
    "    #     if name == 'bert':\n",
    "    #         paras.append({\"params\": [p for p in module.parameters() if p.requires_grad==True], 'lr': 5e-5})\n",
    "    #     else:\n",
    "    #         other_para.extend([p for p in module.parameters()])\n",
    "    # paras.append({\"params\": other_para, 'lr': myconfig.lr})\n",
    "    # optimizer = torch.optim.Adam(paras)\n",
    "\n",
    "    scheduler = get_scheduler(\"linear\",\n",
    "                            optimizer=optimizer,\n",
    "                            num_warmup_steps=int(myconfig.warmup_ratio*myconfig.epochs*num_training_steps),\n",
    "                            num_training_steps=myconfig.epochs*num_training_steps)\n",
    "    \n",
    "    # start sync to wandb\n",
    "    wconfig = {}\n",
    "    wconfig['seed'] = seed\n",
    "    wconfig.update(myconfig.__dict__)\n",
    "    run = wandb.init(project=f\"{modelname} {dataset}\", \n",
    "                     entity=\"fsu-dsc-cil\", \n",
    "                     dir='/scratch/data_jz17d/wandb_tmp/', \n",
    "                     config=wconfig,\n",
    "                     name=f'run_{i_run}',\n",
    "                     reinit=True,\n",
    "                     settings=wandb.Settings(start_method='thread'))\n",
    "    \n",
    "    best_evaluation = collections.defaultdict(float)\n",
    "    pbar = trange(myconfig.epochs*num_training_steps, leave=False)\n",
    "    for i_epoch in range(myconfig.epochs):\n",
    "        # train\n",
    "        model.train()\n",
    "        for batch in train_loader:\n",
    "            batch = batch.to(device, non_blocking=True)\n",
    "            optimizer.zero_grad(), \n",
    "            output = model(batch.text, batch.pos, batch.alignments, batch.edge_index, batch.edge_type_ids, batch.batch, batch.ptr, batch.y, batch.segment_ids, batch.num_syllables, batch.word_freqs)\n",
    "            output.loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            pbar.update(1)\n",
    "\n",
    "        # evaluate on test set\n",
    "        model.eval()\n",
    "        doc_score = 1e-8*np.ones((len(test_docid2index), myconfig.num_classes))\n",
    "        metric = evaluate.load('/home/jz17d/Desktop/metrics/accuracy')\n",
    "        for batch in test_loader:\n",
    "            batch = batch.to(device, non_blocking=True)\n",
    "            output = model(batch.text, batch.pos, batch.alignments, batch.edge_index, batch.edge_type_ids, batch.batch, batch.ptr, batch.y, batch.segment_ids, batch.num_syllables, batch.word_freqs)\n",
    "            pred = output.logit.argmax(axis=-1).cpu().detach().numpy()\n",
    "            metric.add_batch(predictions=pred, references=batch.y.cpu().numpy())\n",
    "            doc_id = np.vectorize(test_docid2index.get)(batch.doc_id.cpu().detach().numpy()) \n",
    "            doc_score[doc_id,pred] += 1\n",
    "        \n",
    "        # logging current\n",
    "        evaluation = metric.compute()\n",
    "        for k in range(1, 6):\n",
    "            evaluation.update({f'test_doc_acc@{k}': top_k_accuracy_score(doc_true, doc_score, k=k)})\n",
    "        wandb.log(evaluation, step=pbar.n)\n",
    "        \n",
    "        # logging best\n",
    "        for key in evaluation:\n",
    "            best_evaluation[f'best_{key}'] = max(best_evaluation[f'best_{key}'], evaluation[key])\n",
    "        wandb.log(best_evaluation, step=pbar.n)\n",
    "\n",
    "    if myconfig.save:\n",
    "        model_folder = f\"{myconfig.save_location}/run_{i_run}\"\n",
    "        os.makedirs(model_folder, exist_ok = True) \n",
    "        save_model(model_folder, model, optimizer, scheduler)\n",
    "\n",
    "    run.finish()\n",
    "    run_pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a0eefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in case needed\n",
    "\n",
    "# with open(f'{dataset_dir}/test_docid2index.json') as f:\n",
    "#     test_docid2index = json.load(f, object_hook=keystoint)\n",
    "# test_docid2index = {v:k for k,v in test_docid2index.items()}\n",
    "# with open(f'{dataset_dir}/test_docid2index.json', 'w') as f:\n",
    "#     json.dump(test_docid2index, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d6a2bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch113",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "eff459b455d2c0757a4e89f8cb2c37a927eff263cb260980ebfa13f3344fc7e6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
