{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19286f89",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42f799ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import bisect\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Callable, Dict, List, NewType, Optional, Tuple, Union\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import optim\n",
    "from torch.optim import lr_scheduler\n",
    "# import torchmetrics\n",
    "\n",
    "import datasets\n",
    "from datasets import load_metric\n",
    "from transformers import AutoConfig, AutoTokenizer, BertModel, RobertaModel, BertPreTrainedModel, BertConfig\n",
    "from transformers import BertForSequenceClassification, DataCollatorWithPadding\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, precision_recall_fscore_support\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4991ed86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/huggingface/transformers/issues/5486\n",
    "# os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09bb209c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd0315ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CrowdFlower': 13,\n",
       " 'DailyDialog': 7,\n",
       " 'EmoBank_Valence': 1,\n",
       " 'EmoBank_Arousal': 1,\n",
       " 'EmoBank_Dominance': 1,\n",
       " 'HateOffensive': 3,\n",
       " 'PASTEL_age': 8,\n",
       " 'PASTEL_country': 2,\n",
       " 'PASTEL_education': 10,\n",
       " 'PASTEL_ethnic': 10,\n",
       " 'PASTEL_gender': 3,\n",
       " 'PASTEL_politics': 3,\n",
       " 'PASTEL_tod': 5,\n",
       " 'SARC': 2,\n",
       " 'SarcasmGhosh': 2,\n",
       " 'SentiTreeBank': 1,\n",
       " 'ShortHumor': 2,\n",
       " 'ShortJokeKaggle': 2,\n",
       " 'ShortRomance': 2,\n",
       " 'StanfordPoliteness': 1,\n",
       " 'TroFi': 2,\n",
       " 'VUA': 2}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../data/xslue/tasks.json', 'r') as f:\n",
    "    tasks = json.load(f)\n",
    "tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ee0eac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = {'VUA': 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6f6fd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTasksDatasets(Dataset): \n",
    "    # currently it's a Mapping-style dataset. Not sure if a Iterable-style dataset will be better\n",
    "    def __init__(self, tasks, split):\n",
    "        self.tasks = tasks\n",
    "        self.split = split\n",
    "        self.data_folder = f'../data/xslue/processed/{self.split}'\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "        self.dfs = []\n",
    "        self.ranges = [0]\n",
    "        for task in tasks:\n",
    "            tsv_file = f'{self.data_folder}/{task}.tsv'\n",
    "            df = pd.read_csv(tsv_file, sep='\\t')\n",
    "            df = df.dropna()\n",
    "            df = df.reset_index(drop=True)\n",
    "            if df['label'].dtype == 'float64':\n",
    "                df['label'] = df['label'].astype('float32')\n",
    "            self.ranges.append(len(df))\n",
    "            self.dfs.append(df)\n",
    "            \n",
    "        self.ranges = np.cumsum(self.ranges).tolist()  \n",
    "        \n",
    "#         self.encodings = self.tokenizer(self.df['text'].tolist(), truncation=True, padding=True, max_length=64)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return self.ranges[-1]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        i_task = bisect.bisect_right(self.ranges, idx) - 1 # select task/df\n",
    "        idx_in_df = idx - self.ranges[i_task]\n",
    "        \n",
    "        item = {k: torch.tensor(v) for k, v in self.tokenizer(self.dfs[i_task][\"text\"].iloc[idx_in_df], truncation=True, padding=True, max_length=64).items()}\n",
    "#         item = {}\n",
    "        item[\"i_task\"] = torch.LongTensor([i_task])\n",
    "        item[\"labels\"] = torch.LongTensor([self.dfs[i_task][\"label\"].iloc[idx_in_df]])\n",
    "#         item[\"text\"] = self.dfs[i_task][\"text\"].iloc[idx_in_df]\n",
    "        return item\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6fb8cbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTasksDataCollator(DataCollatorWithPadding):\n",
    "    def __init__(self, tokenizer, **kwargs):\n",
    "        super().__init__(tokenizer, **kwargs)\n",
    "        \n",
    "    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        batch = self.tokenizer.pad(\n",
    "            features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=self.return_tensors,\n",
    "        )\n",
    "        if \"label\" in batch:\n",
    "            batch[\"labels\"] = batch[\"label\"]\n",
    "            del batch[\"label\"]\n",
    "        if \"label_ids\" in batch:\n",
    "            batch[\"labels\"] = batch[\"label_ids\"]\n",
    "            del batch[\"label_ids\"]\n",
    "        return batch    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff115683",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionHead(nn.Module):\n",
    "    def __init__(self, embedding_dim = 768, hidden_dim = 128):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.hidden = nn.Linear(embedding_dim, 1)\n",
    "        \n",
    "        self.loss_fn = nn.MSELoss()\n",
    "    def forward(self, sent_emb, label):\n",
    "        output = self.dropout(self.hidden(sent_emb)).squeeze(1)\n",
    "\n",
    "        loss = self.loss_fn(output, label)\n",
    "        \n",
    "        return output, loss\n",
    "\n",
    "class ClassificationHead(nn.Module):\n",
    "    def __init__(self, num_labels, embedding_dim = 768, hidden_dim = 128):\n",
    "        super().__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.hidden = nn.Linear(embedding_dim, self.num_labels)\n",
    "        \n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "    def forward(self, sent_emb, label):\n",
    "        output = self.dropout(self.hidden(sent_emb))\n",
    "        \n",
    "        loss = self.loss_fn(output.view(-1, self.num_labels), label.view(-1))\n",
    "        \n",
    "        return output, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8aada9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskBert(BertPreTrainedModel):\n",
    "    def __init__(self, config, selected_tasks):\n",
    "        super().__init__(config)\n",
    "        self.num_tasks = len(selected_tasks)\n",
    "        self.basemodel = BertModel(config)\n",
    "        self.style_heads = nn.ModuleList()\n",
    "        for task in selected_tasks:\n",
    "            if tasks[task] == 1:\n",
    "                self.style_heads.append(RegressionHead())\n",
    "            else:\n",
    "                self.style_heads.append(ClassificationHead(tasks[task]))\n",
    "    def forward(self, input_ids, token_type_ids, attention_mask, i_task=None, labels=None, return_loss=True):\n",
    "        output = self.basemodel(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)\n",
    "        sent_emb = output['pooler_output']\n",
    "        \n",
    "        return self.style_heads[0](sent_emb, labels)\n",
    "        \n",
    "        \n",
    "        total_loss = torch.tensor(0.).to(device)\n",
    "        total_pred = []\n",
    "        for j_task in range(self.num_tasks):\n",
    "            pred, loss = self.style_heads[j_task](sent_emb[i_task.view(-1)==j_task], labels[i_task.view(-1)==j_task])\n",
    "            total_loss += loss\n",
    "            total_pred.append(pred.detach().to('cpu'))\n",
    "        if return_loss:\n",
    "            return total_loss\n",
    "        return total_pred, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd9e3d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTrainer(Trainer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)    \n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "#         labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        \n",
    "        logits = outputs[0]\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        return (loss_fn(logits, labels.view(-1)), outputs) if return_outputs else loss_fn(logits, labels.view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fdfe3191",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/joey/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/joey/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/joey/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/joey/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/joey/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/joey/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/joey/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/joey/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/joey/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/joey/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/joey/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/joey/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/joey/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/joey/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/joey/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/joey/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "/home/joey/anaconda3/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 15157\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2370\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2370' max='2370' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2370/2370 04:16, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.700300</td>\n",
       "      <td>0.581456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.621500</td>\n",
       "      <td>0.578720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.598400</td>\n",
       "      <td>0.577645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.598000</td>\n",
       "      <td>0.576772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.594700</td>\n",
       "      <td>0.576694</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1638\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ../result/multitask/baseline_freezed/checkpoint-474\n",
      "Configuration saved in ../result/multitask/baseline_freezed/checkpoint-474/config.json\n",
      "Model weights saved in ../result/multitask/baseline_freezed/checkpoint-474/pytorch_model.bin\n",
      "tokenizer config file saved in ../result/multitask/baseline_freezed/checkpoint-474/tokenizer_config.json\n",
      "Special tokens file saved in ../result/multitask/baseline_freezed/checkpoint-474/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1638\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ../result/multitask/baseline_freezed/checkpoint-948\n",
      "Configuration saved in ../result/multitask/baseline_freezed/checkpoint-948/config.json\n",
      "Model weights saved in ../result/multitask/baseline_freezed/checkpoint-948/pytorch_model.bin\n",
      "tokenizer config file saved in ../result/multitask/baseline_freezed/checkpoint-948/tokenizer_config.json\n",
      "Special tokens file saved in ../result/multitask/baseline_freezed/checkpoint-948/special_tokens_map.json\n",
      "Deleting older checkpoint [../result/multitask/baseline_freezed/checkpoint-474] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1638\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ../result/multitask/baseline_freezed/checkpoint-1422\n",
      "Configuration saved in ../result/multitask/baseline_freezed/checkpoint-1422/config.json\n",
      "Model weights saved in ../result/multitask/baseline_freezed/checkpoint-1422/pytorch_model.bin\n",
      "tokenizer config file saved in ../result/multitask/baseline_freezed/checkpoint-1422/tokenizer_config.json\n",
      "Special tokens file saved in ../result/multitask/baseline_freezed/checkpoint-1422/special_tokens_map.json\n",
      "Deleting older checkpoint [../result/multitask/baseline_freezed/checkpoint-948] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1638\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ../result/multitask/baseline_freezed/checkpoint-1896\n",
      "Configuration saved in ../result/multitask/baseline_freezed/checkpoint-1896/config.json\n",
      "Model weights saved in ../result/multitask/baseline_freezed/checkpoint-1896/pytorch_model.bin\n",
      "tokenizer config file saved in ../result/multitask/baseline_freezed/checkpoint-1896/tokenizer_config.json\n",
      "Special tokens file saved in ../result/multitask/baseline_freezed/checkpoint-1896/special_tokens_map.json\n",
      "Deleting older checkpoint [../result/multitask/baseline_freezed/checkpoint-1422] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1638\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ../result/multitask/baseline_freezed/checkpoint-2370\n",
      "Configuration saved in ../result/multitask/baseline_freezed/checkpoint-2370/config.json\n",
      "Model weights saved in ../result/multitask/baseline_freezed/checkpoint-2370/pytorch_model.bin\n",
      "tokenizer config file saved in ../result/multitask/baseline_freezed/checkpoint-2370/tokenizer_config.json\n",
      "Special tokens file saved in ../result/multitask/baseline_freezed/checkpoint-2370/special_tokens_map.json\n",
      "Deleting older checkpoint [../result/multitask/baseline_freezed/checkpoint-1896] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ../result/multitask/baseline_freezed/checkpoint-2370 (score: 0.5766944289207458).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2370, training_loss=0.6023955406518928, metrics={'train_runtime': 256.2481, 'train_samples_per_second': 295.749, 'train_steps_per_second': 9.249, 'total_flos': 2458619673482760.0, 'train_loss': 0.6023955406518928, 'epoch': 5.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freeze_bert = True\n",
    "batch_size = 32\n",
    "\n",
    "train_dataset = MultiTasksDatasets(tasks, 'train')\n",
    "val_dataset = MultiTasksDatasets(tasks, 'dev')\n",
    "\n",
    "bertconfig = BertConfig.from_pretrained(\"bert-base-uncased\")\n",
    "multitaskbert = MultiTaskBert(bertconfig, tasks)\n",
    "\n",
    "if freeze_bert:\n",
    "    for param in multitaskbert.basemodel.parameters():\n",
    "        param.requires_grad = False\n",
    "        \n",
    "result_folder = '../result'\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"{result_folder}/multitask/{'baseline'+'_freezed' if freeze_bert else 'baseline'}\",   # output directory\n",
    "    num_train_epochs=5,              # total number of training epochs\n",
    "    per_device_train_batch_size=batch_size,  # batch size per device during training\n",
    "    per_device_eval_batch_size=batch_size,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir=f\"{result_folder}/multitask/{'baseline'+'_freezed' if freeze_bert else 'baseline'}/logs\",  # directory for storing logs\n",
    "    logging_first_step = True, \n",
    "#         logging_steps=500,               # log & save weights each logging_steps\n",
    "#         save_steps=500,\n",
    "    evaluation_strategy=\"epoch\",     # evaluate each `logging_steps`\n",
    "    save_total_limit = 1,\n",
    "    save_strategy = 'epoch',\n",
    "    load_best_model_at_end=True, # decide on loss\n",
    ")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "trainer = MyTrainer(\n",
    "    model=multitaskbert,   # the instantiated Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    tokenizer=AutoTokenizer.from_pretrained(\"bert-base-uncased\"), \n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=val_dataset,          # evaluation dataset\n",
    "#     compute_metrics=compute_metrics,     # the callback that computes metrics of interest\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e609b3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
