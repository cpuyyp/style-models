{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ff23f80",
   "metadata": {},
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#train-each-task-and-get-prediction\" data-toc-modified-id=\"train-each-task-and-get-prediction-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>train each task and get prediction</a></span><ul class=\"toc-item\"><li><span><a href=\"#country\" data-toc-modified-id=\"country-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>country</a></span></li><li><span><a href=\"#politics\" data-toc-modified-id=\"politics-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>politics</a></span></li><li><span><a href=\"#tod\" data-toc-modified-id=\"tod-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>tod</a></span></li><li><span><a href=\"#age\" data-toc-modified-id=\"age-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>age</a></span></li><li><span><a href=\"#education\" data-toc-modified-id=\"education-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>education</a></span></li><li><span><a href=\"#ethnic\" data-toc-modified-id=\"ethnic-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>ethnic</a></span></li><li><span><a href=\"#gender\" data-toc-modified-id=\"gender-1.7\"><span class=\"toc-item-num\">1.7&nbsp;&nbsp;</span>gender</a></span></li></ul></li><li><span><a href=\"#combine-all-prediction\" data-toc-modified-id=\"combine-all-prediction-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>combine all prediction</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc709ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./PASTEL_MTL_training_utils.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268e5496",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = 'PASTEL single task on unmasked set.ipynb'\n",
    "os.environ['WANDB_PROJECT']=\"PASTEL single task on unmasked set\"\n",
    "os.environ['WANDB_ENTITY']=\"fsu-dsc-cil\"\n",
    "os.environ['WANDB_DIR']=\"/scratch/data_jz17d/wandb_tmp/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b2a946",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_logits_labels(trainer, dataset, task_name):\n",
    "    pred = trainer.predict(dataset)\n",
    "    df = dataset.df.copy()\n",
    "    logits = pred.predictions[task_name]\n",
    "    labels = logits.argmax(-1)\n",
    "    df['logits'] = logits.tolist()\n",
    "    df[f'predicted_{task_name}'] = labels.tolist()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2eb7e8",
   "metadata": {},
   "source": [
    "# train each task and get prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279f3fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# global settings\n",
    "p=0.8\n",
    "num_epoch=15\n",
    "save_prediction = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9af5bf2",
   "metadata": {},
   "source": [
    "## country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9b8025",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "***** Running training *****\n",
      "  Num examples = 6385\n",
      "  Num Epochs = 15\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3000\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcpuyyp\u001b[0m (\u001b[33mfsu-dsc-cil\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch/data_jz17d/wandb_tmp/wandb/run-20220923_174714-anuo2l1w</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/fsu-dsc-cil/PASTEL%20single%20task%20on%20unmasked%20set/runs/anuo2l1w\" target=\"_blank\">single task bert on unmasked PASTEL-country</a></strong> to <a href=\"https://wandb.ai/fsu-dsc-cil/PASTEL%20single%20task%20on%20unmasked%20set\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3000' max='3000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3000/3000 26:32, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy Country</th>\n",
       "      <th>F1 Country</th>\n",
       "      <th>Precision Country</th>\n",
       "      <th>Recall Country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.313700</td>\n",
       "      <td>0.114288</td>\n",
       "      <td>0.977892</td>\n",
       "      <td>0.329608</td>\n",
       "      <td>0.325964</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.313700</td>\n",
       "      <td>0.107581</td>\n",
       "      <td>0.977892</td>\n",
       "      <td>0.329608</td>\n",
       "      <td>0.325964</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.172300</td>\n",
       "      <td>0.172681</td>\n",
       "      <td>0.963613</td>\n",
       "      <td>0.344475</td>\n",
       "      <td>0.346353</td>\n",
       "      <td>0.343310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.172300</td>\n",
       "      <td>0.204686</td>\n",
       "      <td>0.971087</td>\n",
       "      <td>0.340028</td>\n",
       "      <td>0.347853</td>\n",
       "      <td>0.338732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.025200</td>\n",
       "      <td>0.218312</td>\n",
       "      <td>0.973644</td>\n",
       "      <td>0.342525</td>\n",
       "      <td>0.360677</td>\n",
       "      <td>0.340198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.025200</td>\n",
       "      <td>0.238385</td>\n",
       "      <td>0.974745</td>\n",
       "      <td>0.340333</td>\n",
       "      <td>0.362027</td>\n",
       "      <td>0.338792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.025200</td>\n",
       "      <td>0.245526</td>\n",
       "      <td>0.975257</td>\n",
       "      <td>0.341695</td>\n",
       "      <td>0.371039</td>\n",
       "      <td>0.339560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.249580</td>\n",
       "      <td>0.975886</td>\n",
       "      <td>0.336865</td>\n",
       "      <td>0.361935</td>\n",
       "      <td>0.336806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.254991</td>\n",
       "      <td>0.975886</td>\n",
       "      <td>0.335800</td>\n",
       "      <td>0.357771</td>\n",
       "      <td>0.336212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.257327</td>\n",
       "      <td>0.976004</td>\n",
       "      <td>0.336922</td>\n",
       "      <td>0.363673</td>\n",
       "      <td>0.336846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.259687</td>\n",
       "      <td>0.975414</td>\n",
       "      <td>0.337676</td>\n",
       "      <td>0.359802</td>\n",
       "      <td>0.337239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.261150</td>\n",
       "      <td>0.975414</td>\n",
       "      <td>0.337676</td>\n",
       "      <td>0.359802</td>\n",
       "      <td>0.337239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.262318</td>\n",
       "      <td>0.975375</td>\n",
       "      <td>0.337656</td>\n",
       "      <td>0.359379</td>\n",
       "      <td>0.337225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.263038</td>\n",
       "      <td>0.975414</td>\n",
       "      <td>0.338707</td>\n",
       "      <td>0.363096</td>\n",
       "      <td>0.337832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.263272</td>\n",
       "      <td>0.975414</td>\n",
       "      <td>0.338707</td>\n",
       "      <td>0.363096</td>\n",
       "      <td>0.337832</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 25421\n",
      "  Batch size = 32\n",
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task country/20220923-17:46:55/checkpoint-200\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task country/20220923-17:46:55/checkpoint-200/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task country/20220923-17:46:55/checkpoint-200/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task country/20220923-17:46:55/checkpoint-200/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task country/20220923-17:46:55/checkpoint-200/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25421\n",
      "  Batch size = 32\n",
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task country/20220923-17:46:55/checkpoint-400\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task country/20220923-17:46:55/checkpoint-400/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task country/20220923-17:46:55/checkpoint-400/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task country/20220923-17:46:55/checkpoint-400/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task country/20220923-17:46:55/checkpoint-400/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/PASTEL single task country/20220923-17:46:55/checkpoint-200] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25421\n",
      "  Batch size = 32\n",
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task country/20220923-17:46:55/checkpoint-600\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task country/20220923-17:46:55/checkpoint-600/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task country/20220923-17:46:55/checkpoint-600/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task country/20220923-17:46:55/checkpoint-600/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task country/20220923-17:46:55/checkpoint-600/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25421\n",
      "  Batch size = 32\n",
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task country/20220923-17:46:55/checkpoint-800\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task country/20220923-17:46:55/checkpoint-800/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task country/20220923-17:46:55/checkpoint-800/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task country/20220923-17:46:55/checkpoint-800/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task country/20220923-17:46:55/checkpoint-800/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/PASTEL single task country/20220923-17:46:55/checkpoint-600] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25421\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task country/20220923-17:46:55/checkpoint-1000\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task country/20220923-17:46:55/checkpoint-1000/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task country/20220923-17:46:55/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task country/20220923-17:46:55/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task country/20220923-17:46:55/checkpoint-1000/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/PASTEL single task country/20220923-17:46:55/checkpoint-800] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25421\n",
      "  Batch size = 32\n",
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task country/20220923-17:46:55/checkpoint-1200\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task country/20220923-17:46:55/checkpoint-1200/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task country/20220923-17:46:55/checkpoint-1200/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task country/20220923-17:46:55/checkpoint-1200/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task country/20220923-17:46:55/checkpoint-1200/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/PASTEL single task country/20220923-17:46:55/checkpoint-1000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25421\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task country/20220923-17:46:55/checkpoint-1400\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task country/20220923-17:46:55/checkpoint-1400/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task country/20220923-17:46:55/checkpoint-1400/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task country/20220923-17:46:55/checkpoint-1400/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task country/20220923-17:46:55/checkpoint-1400/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/PASTEL single task country/20220923-17:46:55/checkpoint-1200] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25421\n",
      "  Batch size = 32\n",
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task country/20220923-17:46:55/checkpoint-1600\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task country/20220923-17:46:55/checkpoint-1600/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task country/20220923-17:46:55/checkpoint-1600/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task country/20220923-17:46:55/checkpoint-1600/tokenizer_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task country/20220923-17:46:55/checkpoint-1600/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/PASTEL single task country/20220923-17:46:55/checkpoint-1400] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25421\n",
      "  Batch size = 32\n",
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task country/20220923-17:46:55/checkpoint-1800\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task country/20220923-17:46:55/checkpoint-1800/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task country/20220923-17:46:55/checkpoint-1800/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task country/20220923-17:46:55/checkpoint-1800/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task country/20220923-17:46:55/checkpoint-1800/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/PASTEL single task country/20220923-17:46:55/checkpoint-1600] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25421\n",
      "  Batch size = 32\n",
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task country/20220923-17:46:55/checkpoint-2000\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task country/20220923-17:46:55/checkpoint-2000/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task country/20220923-17:46:55/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task country/20220923-17:46:55/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task country/20220923-17:46:55/checkpoint-2000/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/PASTEL single task country/20220923-17:46:55/checkpoint-1800] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25421\n",
      "  Batch size = 32\n",
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task country/20220923-17:46:55/checkpoint-2200\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task country/20220923-17:46:55/checkpoint-2200/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task country/20220923-17:46:55/checkpoint-2200/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task country/20220923-17:46:55/checkpoint-2200/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task country/20220923-17:46:55/checkpoint-2200/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/PASTEL single task country/20220923-17:46:55/checkpoint-2000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25421\n",
      "  Batch size = 32\n",
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task country/20220923-17:46:55/checkpoint-2400\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task country/20220923-17:46:55/checkpoint-2400/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task country/20220923-17:46:55/checkpoint-2400/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task country/20220923-17:46:55/checkpoint-2400/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task country/20220923-17:46:55/checkpoint-2400/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/PASTEL single task country/20220923-17:46:55/checkpoint-2200] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25421\n",
      "  Batch size = 32\n",
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task country/20220923-17:46:55/checkpoint-2600\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task country/20220923-17:46:55/checkpoint-2600/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task country/20220923-17:46:55/checkpoint-2600/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task country/20220923-17:46:55/checkpoint-2600/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task country/20220923-17:46:55/checkpoint-2600/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/PASTEL single task country/20220923-17:46:55/checkpoint-2400] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25421\n",
      "  Batch size = 32\n",
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task country/20220923-17:46:55/checkpoint-2800\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task country/20220923-17:46:55/checkpoint-2800/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task country/20220923-17:46:55/checkpoint-2800/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task country/20220923-17:46:55/checkpoint-2800/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task country/20220923-17:46:55/checkpoint-2800/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/PASTEL single task country/20220923-17:46:55/checkpoint-2600] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25421\n",
      "  Batch size = 32\n",
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task country/20220923-17:46:55/checkpoint-3000\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task country/20220923-17:46:55/checkpoint-3000/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task country/20220923-17:46:55/checkpoint-3000/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task country/20220923-17:46:55/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task country/20220923-17:46:55/checkpoint-3000/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/PASTEL single task country/20220923-17:46:55/checkpoint-2800] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /scratch/data_jz17d/result/PASTEL single task country/20220923-17:46:55/checkpoint-400 (score: 0.10758054256439209).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 6385\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='995' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [200/200 01:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "***** Running Prediction *****\n",
      "  Num examples = 25421\n",
      "  Batch size = 32\n",
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "################# settings #################\n",
    "selected_tasks = ['country']\n",
    "############################################\n",
    "# build args\n",
    "my_training_args = MyTrainingArgs(selected_tasks=selected_tasks,\n",
    "                                  model_name=f'PASTEL single task {selected_tasks[0]}',\n",
    "                                  base_model_name='bert-base-uncased',\n",
    "                                  freeze_bert=False,\n",
    "                                  use_pooler=False,\n",
    "                                  num_epoch=num_epoch,\n",
    "#                                   data_limit=30000,\n",
    "                                 )\n",
    "hg_training_args = TrainingArguments(\n",
    "    output_dir=my_training_args.model_folder,   # output directory\n",
    "    num_train_epochs=my_training_args.num_epoch,     # total number of training epochs\n",
    "    per_device_train_batch_size=my_training_args.batch_size,  # batch size per device during training\n",
    "    per_device_eval_batch_size=my_training_args.batch_size,   # batch size for evaluation\n",
    "    warmup_steps=my_training_args.num_warmup_steps,    # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir=f\"{my_training_args.model_folder}/logs\",  # directory for storing logs\n",
    "    logging_first_step = True, \n",
    "    evaluation_strategy=\"epoch\",     # either 'epoch' or 'steps'\n",
    "    save_strategy = 'epoch',\n",
    "    save_total_limit = 1,\n",
    "    metric_for_best_model='eval_loss',\n",
    "    greater_is_better=False,\n",
    "    load_best_model_at_end=True,  # by default decide on loss, change it by setting metric_for_best_model\n",
    "    report_to=\"wandb\",\n",
    "    run_name=f\"single task bert on unmasked PASTEL-{selected_tasks[0]}\",\n",
    ")\n",
    "\n",
    "# initialize model and dataset\n",
    "model = init_model(my_training_args)\n",
    "freeze_model(model, my_training_args.freeze_bert)\n",
    "\n",
    "train_dataset = MyDataset(my_training_args, f'p={p}_unmasked_train')\n",
    "val_dataset = MyDataset(my_training_args, f'p={p}_masked_train')\n",
    "\n",
    "trainer = MyTrainer(\n",
    "    model=model,   # the instantiated Transformers model to be trained\n",
    "    args=hg_training_args,                  # training arguments, defined above\n",
    "    tokenizer=model.tokenizer, \n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=val_dataset,          # evaluation dataset\n",
    "    compute_metrics=compute_metrics,     # the callback that computes metrics of interest\n",
    ")\n",
    "\n",
    "# train\n",
    "trainer.train()\n",
    "\n",
    "# get prediction and save to file\n",
    "if save_prediction:\n",
    "    df_train = predict_logits_labels(trainer, train_dataset, selected_tasks[0])\n",
    "    df_val = predict_logits_labels(trainer, val_dataset, selected_tasks[0])\n",
    "    df_combined = pd.concat([df_train, df_val], axis = 0)\n",
    "    df_combined = df_combined.sort_values('Unnamed: 0')\n",
    "\n",
    "    output_path = f'{data_folder}/pastel/processed/p={p}_predicted'\n",
    "    Path(output_path).mkdir(parents=True, exist_ok=True)\n",
    "    df_combined.to_csv(f'{output_path}/{selected_tasks[0]}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63233031",
   "metadata": {},
   "source": [
    "## politics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a2d847",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/jz17d/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/jz17d/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/jz17d/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/jz17d/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/jz17d/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/jz17d/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/jz17d/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/jz17d/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/jz17d/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/jz17d/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "***** Running training *****\n",
      "  Num examples = 6250\n",
      "  Num Epochs = 15\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2940\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2940' max='2940' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2940/2940 26:12, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy Politics</th>\n",
       "      <th>F1 Politics</th>\n",
       "      <th>Precision Politics</th>\n",
       "      <th>Recall Politics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.251900</td>\n",
       "      <td>0.974789</td>\n",
       "      <td>0.486500</td>\n",
       "      <td>0.350041</td>\n",
       "      <td>0.324084</td>\n",
       "      <td>0.384977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.251900</td>\n",
       "      <td>0.961454</td>\n",
       "      <td>0.500117</td>\n",
       "      <td>0.360191</td>\n",
       "      <td>0.442626</td>\n",
       "      <td>0.399834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.960600</td>\n",
       "      <td>1.064373</td>\n",
       "      <td>0.482431</td>\n",
       "      <td>0.410353</td>\n",
       "      <td>0.430978</td>\n",
       "      <td>0.412795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.960600</td>\n",
       "      <td>1.457381</td>\n",
       "      <td>0.445257</td>\n",
       "      <td>0.415506</td>\n",
       "      <td>0.426777</td>\n",
       "      <td>0.431926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.960600</td>\n",
       "      <td>1.899788</td>\n",
       "      <td>0.471866</td>\n",
       "      <td>0.425851</td>\n",
       "      <td>0.425904</td>\n",
       "      <td>0.426023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.416200</td>\n",
       "      <td>2.516419</td>\n",
       "      <td>0.485522</td>\n",
       "      <td>0.411069</td>\n",
       "      <td>0.426216</td>\n",
       "      <td>0.412798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.416200</td>\n",
       "      <td>2.847833</td>\n",
       "      <td>0.468344</td>\n",
       "      <td>0.415947</td>\n",
       "      <td>0.421203</td>\n",
       "      <td>0.417214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.106900</td>\n",
       "      <td>3.253424</td>\n",
       "      <td>0.476053</td>\n",
       "      <td>0.421528</td>\n",
       "      <td>0.425584</td>\n",
       "      <td>0.421071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.106900</td>\n",
       "      <td>3.465040</td>\n",
       "      <td>0.474409</td>\n",
       "      <td>0.424245</td>\n",
       "      <td>0.426462</td>\n",
       "      <td>0.424053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.106900</td>\n",
       "      <td>3.368948</td>\n",
       "      <td>0.484505</td>\n",
       "      <td>0.419879</td>\n",
       "      <td>0.429967</td>\n",
       "      <td>0.419234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.063200</td>\n",
       "      <td>3.907490</td>\n",
       "      <td>0.475270</td>\n",
       "      <td>0.425513</td>\n",
       "      <td>0.427295</td>\n",
       "      <td>0.425756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.063200</td>\n",
       "      <td>3.945775</td>\n",
       "      <td>0.478909</td>\n",
       "      <td>0.421603</td>\n",
       "      <td>0.426717</td>\n",
       "      <td>0.420586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.034000</td>\n",
       "      <td>4.031681</td>\n",
       "      <td>0.477657</td>\n",
       "      <td>0.428245</td>\n",
       "      <td>0.430215</td>\n",
       "      <td>0.427198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.034000</td>\n",
       "      <td>4.023334</td>\n",
       "      <td>0.481296</td>\n",
       "      <td>0.425969</td>\n",
       "      <td>0.430363</td>\n",
       "      <td>0.424619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.034000</td>\n",
       "      <td>4.023862</td>\n",
       "      <td>0.481179</td>\n",
       "      <td>0.424811</td>\n",
       "      <td>0.429456</td>\n",
       "      <td>0.423513</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 25556\n",
      "  Batch size = 32\n",
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task politics/20220923-18:14:58/checkpoint-196\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task politics/20220923-18:14:58/checkpoint-196/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task politics/20220923-18:14:58/checkpoint-196/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task politics/20220923-18:14:58/checkpoint-196/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task politics/20220923-18:14:58/checkpoint-196/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25556\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task politics/20220923-18:14:58/checkpoint-392\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task politics/20220923-18:14:58/checkpoint-392/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task politics/20220923-18:14:58/checkpoint-392/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task politics/20220923-18:14:58/checkpoint-392/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task politics/20220923-18:14:58/checkpoint-392/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/PASTEL single task politics/20220923-18:14:58/checkpoint-196] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25556\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task politics/20220923-18:14:58/checkpoint-588\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task politics/20220923-18:14:58/checkpoint-588/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task politics/20220923-18:14:58/checkpoint-588/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task politics/20220923-18:14:58/checkpoint-588/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task politics/20220923-18:14:58/checkpoint-588/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25556\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task politics/20220923-18:14:58/checkpoint-784\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task politics/20220923-18:14:58/checkpoint-784/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task politics/20220923-18:14:58/checkpoint-784/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task politics/20220923-18:14:58/checkpoint-784/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task politics/20220923-18:14:58/checkpoint-784/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/PASTEL single task politics/20220923-18:14:58/checkpoint-588] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25556\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task politics/20220923-18:14:58/checkpoint-980\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task politics/20220923-18:14:58/checkpoint-980/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task politics/20220923-18:14:58/checkpoint-980/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task politics/20220923-18:14:58/checkpoint-980/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task politics/20220923-18:14:58/checkpoint-980/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/PASTEL single task politics/20220923-18:14:58/checkpoint-784] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25556\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task politics/20220923-18:14:58/checkpoint-1176\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task politics/20220923-18:14:58/checkpoint-1176/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task politics/20220923-18:14:58/checkpoint-1176/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task politics/20220923-18:14:58/checkpoint-1176/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task politics/20220923-18:14:58/checkpoint-1176/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/PASTEL single task politics/20220923-18:14:58/checkpoint-980] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25556\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task politics/20220923-18:14:58/checkpoint-1372\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task politics/20220923-18:14:58/checkpoint-1372/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task politics/20220923-18:14:58/checkpoint-1372/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task politics/20220923-18:14:58/checkpoint-1372/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task politics/20220923-18:14:58/checkpoint-1372/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/PASTEL single task politics/20220923-18:14:58/checkpoint-1176] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25556\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task politics/20220923-18:14:58/checkpoint-1568\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task politics/20220923-18:14:58/checkpoint-1568/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task politics/20220923-18:14:58/checkpoint-1568/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task politics/20220923-18:14:58/checkpoint-1568/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task politics/20220923-18:14:58/checkpoint-1568/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/PASTEL single task politics/20220923-18:14:58/checkpoint-1372] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25556\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task politics/20220923-18:14:58/checkpoint-1764\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task politics/20220923-18:14:58/checkpoint-1764/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task politics/20220923-18:14:58/checkpoint-1764/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task politics/20220923-18:14:58/checkpoint-1764/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task politics/20220923-18:14:58/checkpoint-1764/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/PASTEL single task politics/20220923-18:14:58/checkpoint-1568] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25556\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task politics/20220923-18:14:58/checkpoint-1960\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task politics/20220923-18:14:58/checkpoint-1960/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task politics/20220923-18:14:58/checkpoint-1960/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task politics/20220923-18:14:58/checkpoint-1960/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task politics/20220923-18:14:58/checkpoint-1960/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/PASTEL single task politics/20220923-18:14:58/checkpoint-1764] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25556\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task politics/20220923-18:14:58/checkpoint-2156\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task politics/20220923-18:14:58/checkpoint-2156/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task politics/20220923-18:14:58/checkpoint-2156/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task politics/20220923-18:14:58/checkpoint-2156/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task politics/20220923-18:14:58/checkpoint-2156/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/PASTEL single task politics/20220923-18:14:58/checkpoint-1960] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25556\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task politics/20220923-18:14:58/checkpoint-2352\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task politics/20220923-18:14:58/checkpoint-2352/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task politics/20220923-18:14:58/checkpoint-2352/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task politics/20220923-18:14:58/checkpoint-2352/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task politics/20220923-18:14:58/checkpoint-2352/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/PASTEL single task politics/20220923-18:14:58/checkpoint-2156] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25556\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task politics/20220923-18:14:58/checkpoint-2548\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task politics/20220923-18:14:58/checkpoint-2548/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task politics/20220923-18:14:58/checkpoint-2548/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task politics/20220923-18:14:58/checkpoint-2548/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task politics/20220923-18:14:58/checkpoint-2548/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/PASTEL single task politics/20220923-18:14:58/checkpoint-2352] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25556\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task politics/20220923-18:14:58/checkpoint-2744\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task politics/20220923-18:14:58/checkpoint-2744/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task politics/20220923-18:14:58/checkpoint-2744/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task politics/20220923-18:14:58/checkpoint-2744/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task politics/20220923-18:14:58/checkpoint-2744/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/PASTEL single task politics/20220923-18:14:58/checkpoint-2548] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25556\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task politics/20220923-18:14:58/checkpoint-2940\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task politics/20220923-18:14:58/checkpoint-2940/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task politics/20220923-18:14:58/checkpoint-2940/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task politics/20220923-18:14:58/checkpoint-2940/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task politics/20220923-18:14:58/checkpoint-2940/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/PASTEL single task politics/20220923-18:14:58/checkpoint-2744] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /scratch/data_jz17d/result/PASTEL single task politics/20220923-18:14:58/checkpoint-392 (score: 0.9614539742469788).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 6250\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='995' max='196' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [196/196 01:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 25556\n",
      "  Batch size = 32\n"
     ]
    }
   ],
   "source": [
    "################# settings #################\n",
    "selected_tasks = ['politics']\n",
    "############################################\n",
    "# build args\n",
    "my_training_args = MyTrainingArgs(selected_tasks=selected_tasks,\n",
    "                                  model_name=f'PASTEL single task {selected_tasks[0]}',\n",
    "                                  base_model_name='bert-base-uncased',\n",
    "                                  freeze_bert=False,\n",
    "                                  use_pooler=False,\n",
    "                                  num_epoch=num_epoch,\n",
    "#                                   data_limit=30000,\n",
    "                                 )\n",
    "hg_training_args = TrainingArguments(\n",
    "    output_dir=my_training_args.model_folder,   # output directory\n",
    "    num_train_epochs=my_training_args.num_epoch,     # total number of training epochs\n",
    "    per_device_train_batch_size=my_training_args.batch_size,  # batch size per device during training\n",
    "    per_device_eval_batch_size=my_training_args.batch_size,   # batch size for evaluation\n",
    "    warmup_steps=my_training_args.num_warmup_steps,    # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir=f\"{my_training_args.model_folder}/logs\",  # directory for storing logs\n",
    "    logging_first_step = True, \n",
    "    evaluation_strategy=\"epoch\",     # either 'epoch' or 'steps'\n",
    "    save_strategy = 'epoch',\n",
    "    save_total_limit = 1,\n",
    "    metric_for_best_model='eval_loss',\n",
    "    greater_is_better=False,\n",
    "    load_best_model_at_end=True,  # by default decide on loss, change it by setting metric_for_best_model\n",
    "    report_to=\"wandb\",\n",
    "    run_name=f\"single task bert on unmasked PASTEL-{selected_tasks[0]}\",\n",
    ")\n",
    "\n",
    "# initialize model and dataset\n",
    "model = init_model(my_training_args)\n",
    "freeze_model(model, my_training_args.freeze_bert)\n",
    "\n",
    "train_dataset = MyDataset(my_training_args, f'p={p}_unmasked_train')\n",
    "val_dataset = MyDataset(my_training_args, f'p={p}_masked_train')\n",
    "\n",
    "trainer = MyTrainer(\n",
    "    model=model,   # the instantiated Transformers model to be trained\n",
    "    args=hg_training_args,                  # training arguments, defined above\n",
    "    tokenizer=model.tokenizer, \n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=val_dataset,          # evaluation dataset\n",
    "    compute_metrics=compute_metrics,     # the callback that computes metrics of interest\n",
    ")\n",
    "\n",
    "# train\n",
    "trainer.train()\n",
    "\n",
    "# get prediction and save to file\n",
    "if save_prediction:\n",
    "    df_train = predict_logits_labels(trainer, train_dataset, selected_tasks[0])\n",
    "    df_val = predict_logits_labels(trainer, val_dataset, selected_tasks[0])\n",
    "    df_combined = pd.concat([df_train, df_val], axis = 0)\n",
    "    df_combined = df_combined.sort_values('Unnamed: 0')\n",
    "\n",
    "    output_path = f'{data_folder}/pastel/processed/p={p}_predicted'\n",
    "    Path(output_path).mkdir(parents=True, exist_ok=True)\n",
    "    df_combined.to_csv(f'{output_path}/{selected_tasks[0]}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc0327e",
   "metadata": {},
   "source": [
    "## tod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf5e302",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/jz17d/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/jz17d/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/jz17d/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/jz17d/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/jz17d/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/jz17d/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/jz17d/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/jz17d/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/jz17d/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/jz17d/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "***** Running training *****\n",
      "  Num examples = 6371\n",
      "  Num Epochs = 15\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3000\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3000' max='3000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3000/3000 26:07, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy Tod</th>\n",
       "      <th>F1 Tod</th>\n",
       "      <th>Precision Tod</th>\n",
       "      <th>Recall Tod</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.749100</td>\n",
       "      <td>1.353066</td>\n",
       "      <td>0.447926</td>\n",
       "      <td>0.209758</td>\n",
       "      <td>0.249461</td>\n",
       "      <td>0.255770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.749100</td>\n",
       "      <td>1.344156</td>\n",
       "      <td>0.456104</td>\n",
       "      <td>0.212646</td>\n",
       "      <td>0.260964</td>\n",
       "      <td>0.260374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.343400</td>\n",
       "      <td>1.401648</td>\n",
       "      <td>0.405032</td>\n",
       "      <td>0.235131</td>\n",
       "      <td>0.270942</td>\n",
       "      <td>0.255489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.343400</td>\n",
       "      <td>1.803065</td>\n",
       "      <td>0.353017</td>\n",
       "      <td>0.248724</td>\n",
       "      <td>0.281381</td>\n",
       "      <td>0.264400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.725400</td>\n",
       "      <td>2.272214</td>\n",
       "      <td>0.347671</td>\n",
       "      <td>0.256616</td>\n",
       "      <td>0.264026</td>\n",
       "      <td>0.265415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.725400</td>\n",
       "      <td>2.782790</td>\n",
       "      <td>0.371732</td>\n",
       "      <td>0.253984</td>\n",
       "      <td>0.277604</td>\n",
       "      <td>0.263957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.725400</td>\n",
       "      <td>3.443300</td>\n",
       "      <td>0.371142</td>\n",
       "      <td>0.270854</td>\n",
       "      <td>0.277932</td>\n",
       "      <td>0.273546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.143500</td>\n",
       "      <td>3.638229</td>\n",
       "      <td>0.379005</td>\n",
       "      <td>0.258481</td>\n",
       "      <td>0.267590</td>\n",
       "      <td>0.262939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.143500</td>\n",
       "      <td>4.024141</td>\n",
       "      <td>0.388284</td>\n",
       "      <td>0.258392</td>\n",
       "      <td>0.270638</td>\n",
       "      <td>0.262818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.070100</td>\n",
       "      <td>4.111552</td>\n",
       "      <td>0.385217</td>\n",
       "      <td>0.266739</td>\n",
       "      <td>0.275496</td>\n",
       "      <td>0.268168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.070100</td>\n",
       "      <td>4.344781</td>\n",
       "      <td>0.377629</td>\n",
       "      <td>0.267400</td>\n",
       "      <td>0.274148</td>\n",
       "      <td>0.270154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.070100</td>\n",
       "      <td>4.360345</td>\n",
       "      <td>0.382308</td>\n",
       "      <td>0.267124</td>\n",
       "      <td>0.275828</td>\n",
       "      <td>0.269604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.046100</td>\n",
       "      <td>4.437543</td>\n",
       "      <td>0.378416</td>\n",
       "      <td>0.270418</td>\n",
       "      <td>0.275242</td>\n",
       "      <td>0.271977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.046100</td>\n",
       "      <td>4.498432</td>\n",
       "      <td>0.380499</td>\n",
       "      <td>0.272624</td>\n",
       "      <td>0.280310</td>\n",
       "      <td>0.273709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.040400</td>\n",
       "      <td>4.507311</td>\n",
       "      <td>0.379595</td>\n",
       "      <td>0.271048</td>\n",
       "      <td>0.278913</td>\n",
       "      <td>0.272372</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 25435\n",
      "  Batch size = 32\n",
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task tod/20220923-18:42:26/checkpoint-200\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task tod/20220923-18:42:26/checkpoint-200/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task tod/20220923-18:42:26/checkpoint-200/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task tod/20220923-18:42:26/checkpoint-200/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task tod/20220923-18:42:26/checkpoint-200/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25435\n",
      "  Batch size = 32\n",
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task tod/20220923-18:42:26/checkpoint-400\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task tod/20220923-18:42:26/checkpoint-400/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task tod/20220923-18:42:26/checkpoint-400/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task tod/20220923-18:42:26/checkpoint-400/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task tod/20220923-18:42:26/checkpoint-400/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/PASTEL single task tod/20220923-18:42:26/checkpoint-200] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25435\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task tod/20220923-18:42:26/checkpoint-600\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task tod/20220923-18:42:26/checkpoint-600/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task tod/20220923-18:42:26/checkpoint-600/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task tod/20220923-18:42:26/checkpoint-600/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task tod/20220923-18:42:26/checkpoint-600/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25435\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task tod/20220923-18:42:26/checkpoint-800\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task tod/20220923-18:42:26/checkpoint-800/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task tod/20220923-18:42:26/checkpoint-800/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task tod/20220923-18:42:26/checkpoint-800/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task tod/20220923-18:42:26/checkpoint-800/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/PASTEL single task tod/20220923-18:42:26/checkpoint-600] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25435\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task tod/20220923-18:42:26/checkpoint-1000\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task tod/20220923-18:42:26/checkpoint-1000/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task tod/20220923-18:42:26/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task tod/20220923-18:42:26/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task tod/20220923-18:42:26/checkpoint-1000/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/PASTEL single task tod/20220923-18:42:26/checkpoint-800] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25435\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task tod/20220923-18:42:26/checkpoint-1200\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task tod/20220923-18:42:26/checkpoint-1200/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task tod/20220923-18:42:26/checkpoint-1200/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task tod/20220923-18:42:26/checkpoint-1200/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task tod/20220923-18:42:26/checkpoint-1200/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/PASTEL single task tod/20220923-18:42:26/checkpoint-1000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25435\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task tod/20220923-18:42:26/checkpoint-1400\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task tod/20220923-18:42:26/checkpoint-1400/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task tod/20220923-18:42:26/checkpoint-1400/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task tod/20220923-18:42:26/checkpoint-1400/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task tod/20220923-18:42:26/checkpoint-1400/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/PASTEL single task tod/20220923-18:42:26/checkpoint-1200] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25435\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task tod/20220923-18:42:26/checkpoint-1600\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task tod/20220923-18:42:26/checkpoint-1600/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task tod/20220923-18:42:26/checkpoint-1600/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task tod/20220923-18:42:26/checkpoint-1600/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task tod/20220923-18:42:26/checkpoint-1600/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/PASTEL single task tod/20220923-18:42:26/checkpoint-1400] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25435\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task tod/20220923-18:42:26/checkpoint-1800\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task tod/20220923-18:42:26/checkpoint-1800/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task tod/20220923-18:42:26/checkpoint-1800/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task tod/20220923-18:42:26/checkpoint-1800/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task tod/20220923-18:42:26/checkpoint-1800/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/PASTEL single task tod/20220923-18:42:26/checkpoint-1600] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25435\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task tod/20220923-18:42:26/checkpoint-2000\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task tod/20220923-18:42:26/checkpoint-2000/config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task tod/20220923-18:42:26/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task tod/20220923-18:42:26/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task tod/20220923-18:42:26/checkpoint-2000/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/PASTEL single task tod/20220923-18:42:26/checkpoint-1800] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25435\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task tod/20220923-18:42:26/checkpoint-2200\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task tod/20220923-18:42:26/checkpoint-2200/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task tod/20220923-18:42:26/checkpoint-2200/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task tod/20220923-18:42:26/checkpoint-2200/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task tod/20220923-18:42:26/checkpoint-2200/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/PASTEL single task tod/20220923-18:42:26/checkpoint-2000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25435\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task tod/20220923-18:42:26/checkpoint-2400\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task tod/20220923-18:42:26/checkpoint-2400/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task tod/20220923-18:42:26/checkpoint-2400/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task tod/20220923-18:42:26/checkpoint-2400/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task tod/20220923-18:42:26/checkpoint-2400/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/PASTEL single task tod/20220923-18:42:26/checkpoint-2200] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25435\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task tod/20220923-18:42:26/checkpoint-2600\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task tod/20220923-18:42:26/checkpoint-2600/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task tod/20220923-18:42:26/checkpoint-2600/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task tod/20220923-18:42:26/checkpoint-2600/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task tod/20220923-18:42:26/checkpoint-2600/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/PASTEL single task tod/20220923-18:42:26/checkpoint-2400] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25435\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task tod/20220923-18:42:26/checkpoint-2800\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task tod/20220923-18:42:26/checkpoint-2800/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task tod/20220923-18:42:26/checkpoint-2800/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task tod/20220923-18:42:26/checkpoint-2800/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task tod/20220923-18:42:26/checkpoint-2800/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/PASTEL single task tod/20220923-18:42:26/checkpoint-2600] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25435\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task tod/20220923-18:42:26/checkpoint-3000\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task tod/20220923-18:42:26/checkpoint-3000/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task tod/20220923-18:42:26/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task tod/20220923-18:42:26/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task tod/20220923-18:42:26/checkpoint-3000/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/PASTEL single task tod/20220923-18:42:26/checkpoint-2800] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /scratch/data_jz17d/result/PASTEL single task tod/20220923-18:42:26/checkpoint-400 (score: 1.344156265258789).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 6371\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='995' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [200/200 01:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "***** Running Prediction *****\n",
      "  Num examples = 25435\n",
      "  Batch size = 32\n",
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "################# settings #################\n",
    "selected_tasks = ['tod']\n",
    "############################################\n",
    "# build args\n",
    "my_training_args = MyTrainingArgs(selected_tasks=selected_tasks,\n",
    "                                  model_name=f'PASTEL single task {selected_tasks[0]}',\n",
    "                                  base_model_name='bert-base-uncased',\n",
    "                                  freeze_bert=False,\n",
    "                                  use_pooler=False,\n",
    "                                  num_epoch=num_epoch,\n",
    "#                                   data_limit=30000,\n",
    "                                 )\n",
    "hg_training_args = TrainingArguments(\n",
    "    output_dir=my_training_args.model_folder,   # output directory\n",
    "    num_train_epochs=my_training_args.num_epoch,     # total number of training epochs\n",
    "    per_device_train_batch_size=my_training_args.batch_size,  # batch size per device during training\n",
    "    per_device_eval_batch_size=my_training_args.batch_size,   # batch size for evaluation\n",
    "    warmup_steps=my_training_args.num_warmup_steps,    # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir=f\"{my_training_args.model_folder}/logs\",  # directory for storing logs\n",
    "    logging_first_step = True, \n",
    "    evaluation_strategy=\"epoch\",     # either 'epoch' or 'steps'\n",
    "    save_strategy = 'epoch',\n",
    "    save_total_limit = 1,\n",
    "    metric_for_best_model='eval_loss',\n",
    "    greater_is_better=False,\n",
    "    load_best_model_at_end=True,  # by default decide on loss, change it by setting metric_for_best_model\n",
    "    report_to=\"wandb\",\n",
    "    run_name=f\"single task bert on unmasked PASTEL-{selected_tasks[0]}\",\n",
    ")\n",
    "\n",
    "# initialize model and dataset\n",
    "model = init_model(my_training_args)\n",
    "freeze_model(model, my_training_args.freeze_bert)\n",
    "\n",
    "train_dataset = MyDataset(my_training_args, f'p={p}_unmasked_train')\n",
    "val_dataset = MyDataset(my_training_args, f'p={p}_masked_train')\n",
    "\n",
    "trainer = MyTrainer(\n",
    "    model=model,   # the instantiated Transformers model to be trained\n",
    "    args=hg_training_args,                  # training arguments, defined above\n",
    "    tokenizer=model.tokenizer, \n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=val_dataset,          # evaluation dataset\n",
    "    compute_metrics=compute_metrics,     # the callback that computes metrics of interest\n",
    ")\n",
    "\n",
    "# train\n",
    "trainer.train()\n",
    "\n",
    "# get prediction and save to file\n",
    "if save_prediction:\n",
    "    df_train = predict_logits_labels(trainer, train_dataset, selected_tasks[0])\n",
    "    df_val = predict_logits_labels(trainer, val_dataset, selected_tasks[0])\n",
    "    df_combined = pd.concat([df_train, df_val], axis = 0)\n",
    "    df_combined = df_combined.sort_values('Unnamed: 0')\n",
    "\n",
    "    output_path = f'{data_folder}/pastel/processed/p={p}_predicted'\n",
    "    Path(output_path).mkdir(parents=True, exist_ok=True)\n",
    "    df_combined.to_csv(f'{output_path}/{selected_tasks[0]}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39917462",
   "metadata": {},
   "source": [
    "## age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368ec959",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/jz17d/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/jz17d/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/jz17d/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/jz17d/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/jz17d/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/jz17d/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/jz17d/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/jz17d/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/jz17d/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/jz17d/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "***** Running training *****\n",
      "  Num examples = 6365\n",
      "  Num Epochs = 15\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2985\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2985' max='2985' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2985/2985 26:16, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy Age</th>\n",
       "      <th>F1 Age</th>\n",
       "      <th>Precision Age</th>\n",
       "      <th>Recall Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.024000</td>\n",
       "      <td>1.459636</td>\n",
       "      <td>0.398530</td>\n",
       "      <td>0.073932</td>\n",
       "      <td>0.131372</td>\n",
       "      <td>0.125896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.024000</td>\n",
       "      <td>1.391327</td>\n",
       "      <td>0.415628</td>\n",
       "      <td>0.164631</td>\n",
       "      <td>0.243831</td>\n",
       "      <td>0.172038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.444500</td>\n",
       "      <td>1.490575</td>\n",
       "      <td>0.411344</td>\n",
       "      <td>0.164774</td>\n",
       "      <td>0.263879</td>\n",
       "      <td>0.176918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.444500</td>\n",
       "      <td>1.755475</td>\n",
       "      <td>0.392948</td>\n",
       "      <td>0.198472</td>\n",
       "      <td>0.200025</td>\n",
       "      <td>0.199297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.444500</td>\n",
       "      <td>2.352129</td>\n",
       "      <td>0.390000</td>\n",
       "      <td>0.200063</td>\n",
       "      <td>0.231782</td>\n",
       "      <td>0.198415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.707000</td>\n",
       "      <td>2.859619</td>\n",
       "      <td>0.389961</td>\n",
       "      <td>0.198583</td>\n",
       "      <td>0.224027</td>\n",
       "      <td>0.196832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.707000</td>\n",
       "      <td>3.315315</td>\n",
       "      <td>0.379073</td>\n",
       "      <td>0.205988</td>\n",
       "      <td>0.237101</td>\n",
       "      <td>0.205179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.135900</td>\n",
       "      <td>3.746656</td>\n",
       "      <td>0.377973</td>\n",
       "      <td>0.204286</td>\n",
       "      <td>0.237902</td>\n",
       "      <td>0.204678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.135900</td>\n",
       "      <td>3.863787</td>\n",
       "      <td>0.389175</td>\n",
       "      <td>0.204100</td>\n",
       "      <td>0.222519</td>\n",
       "      <td>0.201419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.135900</td>\n",
       "      <td>4.014277</td>\n",
       "      <td>0.378719</td>\n",
       "      <td>0.217466</td>\n",
       "      <td>0.229516</td>\n",
       "      <td>0.215580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.053100</td>\n",
       "      <td>4.149844</td>\n",
       "      <td>0.385637</td>\n",
       "      <td>0.224271</td>\n",
       "      <td>0.248689</td>\n",
       "      <td>0.218898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.053100</td>\n",
       "      <td>4.209584</td>\n",
       "      <td>0.394010</td>\n",
       "      <td>0.215380</td>\n",
       "      <td>0.238934</td>\n",
       "      <td>0.209629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.027200</td>\n",
       "      <td>4.271204</td>\n",
       "      <td>0.379545</td>\n",
       "      <td>0.218190</td>\n",
       "      <td>0.231032</td>\n",
       "      <td>0.214718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.027200</td>\n",
       "      <td>4.332695</td>\n",
       "      <td>0.385284</td>\n",
       "      <td>0.216900</td>\n",
       "      <td>0.226691</td>\n",
       "      <td>0.214580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.027200</td>\n",
       "      <td>4.329983</td>\n",
       "      <td>0.389018</td>\n",
       "      <td>0.216932</td>\n",
       "      <td>0.228216</td>\n",
       "      <td>0.212352</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 25441\n",
      "  Batch size = 32\n",
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task age/20220923-19:09:46/checkpoint-199\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task age/20220923-19:09:46/checkpoint-199/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task age/20220923-19:09:46/checkpoint-199/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task age/20220923-19:09:46/checkpoint-199/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task age/20220923-19:09:46/checkpoint-199/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25441\n",
      "  Batch size = 32\n",
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task age/20220923-19:09:46/checkpoint-398\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task age/20220923-19:09:46/checkpoint-398/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task age/20220923-19:09:46/checkpoint-398/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task age/20220923-19:09:46/checkpoint-398/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task age/20220923-19:09:46/checkpoint-398/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/PASTEL single task age/20220923-19:09:46/checkpoint-199] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25441\n",
      "  Batch size = 32\n",
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task age/20220923-19:09:46/checkpoint-597\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task age/20220923-19:09:46/checkpoint-597/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task age/20220923-19:09:46/checkpoint-597/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task age/20220923-19:09:46/checkpoint-597/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task age/20220923-19:09:46/checkpoint-597/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25441\n",
      "  Batch size = 32\n",
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task age/20220923-19:09:46/checkpoint-796\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task age/20220923-19:09:46/checkpoint-796/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task age/20220923-19:09:46/checkpoint-796/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task age/20220923-19:09:46/checkpoint-796/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task age/20220923-19:09:46/checkpoint-796/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/PASTEL single task age/20220923-19:09:46/checkpoint-597] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25441\n",
      "  Batch size = 32\n",
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task age/20220923-19:09:46/checkpoint-995\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task age/20220923-19:09:46/checkpoint-995/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task age/20220923-19:09:46/checkpoint-995/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task age/20220923-19:09:46/checkpoint-995/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task age/20220923-19:09:46/checkpoint-995/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/PASTEL single task age/20220923-19:09:46/checkpoint-796] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25441\n",
      "  Batch size = 32\n",
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task age/20220923-19:09:46/checkpoint-1194\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task age/20220923-19:09:46/checkpoint-1194/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task age/20220923-19:09:46/checkpoint-1194/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task age/20220923-19:09:46/checkpoint-1194/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task age/20220923-19:09:46/checkpoint-1194/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/PASTEL single task age/20220923-19:09:46/checkpoint-995] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25441\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task age/20220923-19:09:46/checkpoint-1393\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task age/20220923-19:09:46/checkpoint-1393/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task age/20220923-19:09:46/checkpoint-1393/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task age/20220923-19:09:46/checkpoint-1393/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task age/20220923-19:09:46/checkpoint-1393/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/PASTEL single task age/20220923-19:09:46/checkpoint-1194] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25441\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task age/20220923-19:09:46/checkpoint-1592\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task age/20220923-19:09:46/checkpoint-1592/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task age/20220923-19:09:46/checkpoint-1592/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task age/20220923-19:09:46/checkpoint-1592/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task age/20220923-19:09:46/checkpoint-1592/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [/scratch/data_jz17d/result/PASTEL single task age/20220923-19:09:46/checkpoint-1393] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25441\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task age/20220923-19:09:46/checkpoint-1791\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task age/20220923-19:09:46/checkpoint-1791/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task age/20220923-19:09:46/checkpoint-1791/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task age/20220923-19:09:46/checkpoint-1791/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task age/20220923-19:09:46/checkpoint-1791/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/PASTEL single task age/20220923-19:09:46/checkpoint-1592] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25441\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task age/20220923-19:09:46/checkpoint-1990\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task age/20220923-19:09:46/checkpoint-1990/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task age/20220923-19:09:46/checkpoint-1990/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task age/20220923-19:09:46/checkpoint-1990/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task age/20220923-19:09:46/checkpoint-1990/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/PASTEL single task age/20220923-19:09:46/checkpoint-1791] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25441\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task age/20220923-19:09:46/checkpoint-2189\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task age/20220923-19:09:46/checkpoint-2189/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task age/20220923-19:09:46/checkpoint-2189/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task age/20220923-19:09:46/checkpoint-2189/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task age/20220923-19:09:46/checkpoint-2189/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/PASTEL single task age/20220923-19:09:46/checkpoint-1990] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25441\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task age/20220923-19:09:46/checkpoint-2388\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task age/20220923-19:09:46/checkpoint-2388/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task age/20220923-19:09:46/checkpoint-2388/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task age/20220923-19:09:46/checkpoint-2388/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task age/20220923-19:09:46/checkpoint-2388/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/PASTEL single task age/20220923-19:09:46/checkpoint-2189] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25441\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task age/20220923-19:09:46/checkpoint-2587\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task age/20220923-19:09:46/checkpoint-2587/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task age/20220923-19:09:46/checkpoint-2587/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task age/20220923-19:09:46/checkpoint-2587/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task age/20220923-19:09:46/checkpoint-2587/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/PASTEL single task age/20220923-19:09:46/checkpoint-2388] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25441\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task age/20220923-19:09:46/checkpoint-2786\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task age/20220923-19:09:46/checkpoint-2786/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task age/20220923-19:09:46/checkpoint-2786/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task age/20220923-19:09:46/checkpoint-2786/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task age/20220923-19:09:46/checkpoint-2786/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/PASTEL single task age/20220923-19:09:46/checkpoint-2587] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25441\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task age/20220923-19:09:46/checkpoint-2985\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task age/20220923-19:09:46/checkpoint-2985/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task age/20220923-19:09:46/checkpoint-2985/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task age/20220923-19:09:46/checkpoint-2985/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task age/20220923-19:09:46/checkpoint-2985/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/PASTEL single task age/20220923-19:09:46/checkpoint-2786] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /scratch/data_jz17d/result/PASTEL single task age/20220923-19:09:46/checkpoint-398 (score: 1.3913270235061646).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 6365\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='995' max='199' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [199/199 01:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "***** Running Prediction *****\n",
      "  Num examples = 25441\n",
      "  Batch size = 32\n",
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "################# settings #################\n",
    "selected_tasks = ['age']\n",
    "############################################\n",
    "# build args\n",
    "my_training_args = MyTrainingArgs(selected_tasks=selected_tasks,\n",
    "                                  model_name=f'PASTEL single task {selected_tasks[0]}',\n",
    "                                  base_model_name='bert-base-uncased',\n",
    "                                  freeze_bert=False,\n",
    "                                  use_pooler=False,\n",
    "                                  num_epoch=num_epoch,\n",
    "#                                   data_limit=30000,\n",
    "                                 )\n",
    "hg_training_args = TrainingArguments(\n",
    "    output_dir=my_training_args.model_folder,   # output directory\n",
    "    num_train_epochs=my_training_args.num_epoch,     # total number of training epochs\n",
    "    per_device_train_batch_size=my_training_args.batch_size,  # batch size per device during training\n",
    "    per_device_eval_batch_size=my_training_args.batch_size,   # batch size for evaluation\n",
    "    warmup_steps=my_training_args.num_warmup_steps,    # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir=f\"{my_training_args.model_folder}/logs\",  # directory for storing logs\n",
    "    logging_first_step = True, \n",
    "    evaluation_strategy=\"epoch\",     # either 'epoch' or 'steps'\n",
    "    save_strategy = 'epoch',\n",
    "    save_total_limit = 1,\n",
    "    metric_for_best_model='eval_loss',\n",
    "    greater_is_better=False,\n",
    "    load_best_model_at_end=True,  # by default decide on loss, change it by setting metric_for_best_model\n",
    "    report_to=\"wandb\",\n",
    "    run_name=f\"single task bert on unmasked PASTEL-{selected_tasks[0]}\",\n",
    ")\n",
    "\n",
    "# initialize model and dataset\n",
    "model = init_model(my_training_args)\n",
    "freeze_model(model, my_training_args.freeze_bert)\n",
    "\n",
    "train_dataset = MyDataset(my_training_args, f'p={p}_unmasked_train')\n",
    "val_dataset = MyDataset(my_training_args, f'p={p}_masked_train')\n",
    "\n",
    "trainer = MyTrainer(\n",
    "    model=model,   # the instantiated Transformers model to be trained\n",
    "    args=hg_training_args,                  # training arguments, defined above\n",
    "    tokenizer=model.tokenizer, \n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=val_dataset,          # evaluation dataset\n",
    "    compute_metrics=compute_metrics,     # the callback that computes metrics of interest\n",
    ")\n",
    "\n",
    "# train\n",
    "trainer.train()\n",
    "\n",
    "# get prediction and save to file\n",
    "if save_prediction:\n",
    "    df_train = predict_logits_labels(trainer, train_dataset, selected_tasks[0])\n",
    "    df_val = predict_logits_labels(trainer, val_dataset, selected_tasks[0])\n",
    "    df_combined = pd.concat([df_train, df_val], axis = 0)\n",
    "    df_combined = df_combined.sort_values('Unnamed: 0')\n",
    "\n",
    "    output_path = f'{data_folder}/pastel/processed/p={p}_predicted'\n",
    "    Path(output_path).mkdir(parents=True, exist_ok=True)\n",
    "    df_combined.to_csv(f'{output_path}/{selected_tasks[0]}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05adc197",
   "metadata": {},
   "source": [
    "## education"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c522203",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/jz17d/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/jz17d/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/jz17d/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/jz17d/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/jz17d/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/jz17d/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/jz17d/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/jz17d/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/jz17d/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/jz17d/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "***** Running training *****\n",
      "  Num examples = 6314\n",
      "  Num Epochs = 15\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2970\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2970' max='2970' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2970/2970 26:07, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy Education</th>\n",
       "      <th>F1 Education</th>\n",
       "      <th>Precision Education</th>\n",
       "      <th>Recall Education</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.444800</td>\n",
       "      <td>1.615172</td>\n",
       "      <td>0.394124</td>\n",
       "      <td>0.139571</td>\n",
       "      <td>0.216514</td>\n",
       "      <td>0.165244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.444800</td>\n",
       "      <td>1.611636</td>\n",
       "      <td>0.361878</td>\n",
       "      <td>0.178142</td>\n",
       "      <td>0.254431</td>\n",
       "      <td>0.183198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.674100</td>\n",
       "      <td>1.607066</td>\n",
       "      <td>0.397772</td>\n",
       "      <td>0.185275</td>\n",
       "      <td>0.230656</td>\n",
       "      <td>0.184299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.674100</td>\n",
       "      <td>1.892083</td>\n",
       "      <td>0.380668</td>\n",
       "      <td>0.189269</td>\n",
       "      <td>0.205374</td>\n",
       "      <td>0.191107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.674100</td>\n",
       "      <td>2.514521</td>\n",
       "      <td>0.364114</td>\n",
       "      <td>0.198194</td>\n",
       "      <td>0.238764</td>\n",
       "      <td>0.194247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.849800</td>\n",
       "      <td>3.020540</td>\n",
       "      <td>0.350698</td>\n",
       "      <td>0.183330</td>\n",
       "      <td>0.200035</td>\n",
       "      <td>0.188808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.849800</td>\n",
       "      <td>3.370916</td>\n",
       "      <td>0.359838</td>\n",
       "      <td>0.204367</td>\n",
       "      <td>0.212950</td>\n",
       "      <td>0.200008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.125900</td>\n",
       "      <td>3.850363</td>\n",
       "      <td>0.356386</td>\n",
       "      <td>0.190323</td>\n",
       "      <td>0.203167</td>\n",
       "      <td>0.191779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.125900</td>\n",
       "      <td>4.067533</td>\n",
       "      <td>0.360270</td>\n",
       "      <td>0.198357</td>\n",
       "      <td>0.208224</td>\n",
       "      <td>0.197419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.125900</td>\n",
       "      <td>4.288568</td>\n",
       "      <td>0.354700</td>\n",
       "      <td>0.200870</td>\n",
       "      <td>0.207072</td>\n",
       "      <td>0.202110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.033200</td>\n",
       "      <td>4.519716</td>\n",
       "      <td>0.354307</td>\n",
       "      <td>0.204170</td>\n",
       "      <td>0.206158</td>\n",
       "      <td>0.205980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.033200</td>\n",
       "      <td>4.490906</td>\n",
       "      <td>0.366743</td>\n",
       "      <td>0.204590</td>\n",
       "      <td>0.214392</td>\n",
       "      <td>0.201222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.007900</td>\n",
       "      <td>4.543574</td>\n",
       "      <td>0.368469</td>\n",
       "      <td>0.198517</td>\n",
       "      <td>0.210342</td>\n",
       "      <td>0.195516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.007900</td>\n",
       "      <td>4.570016</td>\n",
       "      <td>0.360819</td>\n",
       "      <td>0.203393</td>\n",
       "      <td>0.211030</td>\n",
       "      <td>0.201460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.007900</td>\n",
       "      <td>4.573004</td>\n",
       "      <td>0.365409</td>\n",
       "      <td>0.200279</td>\n",
       "      <td>0.208616</td>\n",
       "      <td>0.197971</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 25492\n",
      "  Batch size = 32\n",
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task education/20220923-19:37:15/checkpoint-198\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task education/20220923-19:37:15/checkpoint-198/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task education/20220923-19:37:15/checkpoint-198/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task education/20220923-19:37:15/checkpoint-198/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task education/20220923-19:37:15/checkpoint-198/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25492\n",
      "  Batch size = 32\n",
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task education/20220923-19:37:15/checkpoint-396\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task education/20220923-19:37:15/checkpoint-396/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task education/20220923-19:37:15/checkpoint-396/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task education/20220923-19:37:15/checkpoint-396/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task education/20220923-19:37:15/checkpoint-396/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/PASTEL single task education/20220923-19:37:15/checkpoint-198] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25492\n",
      "  Batch size = 32\n",
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task education/20220923-19:37:15/checkpoint-594\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task education/20220923-19:37:15/checkpoint-594/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task education/20220923-19:37:15/checkpoint-594/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task education/20220923-19:37:15/checkpoint-594/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task education/20220923-19:37:15/checkpoint-594/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/PASTEL single task education/20220923-19:37:15/checkpoint-396] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25492\n",
      "  Batch size = 32\n",
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task education/20220923-19:37:15/checkpoint-792\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task education/20220923-19:37:15/checkpoint-792/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task education/20220923-19:37:15/checkpoint-792/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task education/20220923-19:37:15/checkpoint-792/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task education/20220923-19:37:15/checkpoint-792/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25492\n",
      "  Batch size = 32\n",
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task education/20220923-19:37:15/checkpoint-990\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task education/20220923-19:37:15/checkpoint-990/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task education/20220923-19:37:15/checkpoint-990/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task education/20220923-19:37:15/checkpoint-990/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task education/20220923-19:37:15/checkpoint-990/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/PASTEL single task education/20220923-19:37:15/checkpoint-792] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25492\n",
      "  Batch size = 32\n",
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task education/20220923-19:37:15/checkpoint-1188\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task education/20220923-19:37:15/checkpoint-1188/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task education/20220923-19:37:15/checkpoint-1188/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task education/20220923-19:37:15/checkpoint-1188/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task education/20220923-19:37:15/checkpoint-1188/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/PASTEL single task education/20220923-19:37:15/checkpoint-990] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25492\n",
      "  Batch size = 32\n",
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task education/20220923-19:37:15/checkpoint-1386\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task education/20220923-19:37:15/checkpoint-1386/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task education/20220923-19:37:15/checkpoint-1386/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task education/20220923-19:37:15/checkpoint-1386/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task education/20220923-19:37:15/checkpoint-1386/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/PASTEL single task education/20220923-19:37:15/checkpoint-1188] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Batch size = 32\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task education/20220923-19:37:15/checkpoint-1584\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task education/20220923-19:37:15/checkpoint-1584/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task education/20220923-19:37:15/checkpoint-1584/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task education/20220923-19:37:15/checkpoint-1584/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task education/20220923-19:37:15/checkpoint-1584/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/PASTEL single task education/20220923-19:37:15/checkpoint-1386] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25492\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task education/20220923-19:37:15/checkpoint-1782\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task education/20220923-19:37:15/checkpoint-1782/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task education/20220923-19:37:15/checkpoint-1782/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task education/20220923-19:37:15/checkpoint-1782/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task education/20220923-19:37:15/checkpoint-1782/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/PASTEL single task education/20220923-19:37:15/checkpoint-1584] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25492\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task education/20220923-19:37:15/checkpoint-1980\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task education/20220923-19:37:15/checkpoint-1980/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task education/20220923-19:37:15/checkpoint-1980/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task education/20220923-19:37:15/checkpoint-1980/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task education/20220923-19:37:15/checkpoint-1980/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/PASTEL single task education/20220923-19:37:15/checkpoint-1782] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25492\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task education/20220923-19:37:15/checkpoint-2178\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task education/20220923-19:37:15/checkpoint-2178/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task education/20220923-19:37:15/checkpoint-2178/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task education/20220923-19:37:15/checkpoint-2178/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task education/20220923-19:37:15/checkpoint-2178/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/PASTEL single task education/20220923-19:37:15/checkpoint-1980] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25492\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task education/20220923-19:37:15/checkpoint-2376\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task education/20220923-19:37:15/checkpoint-2376/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task education/20220923-19:37:15/checkpoint-2376/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task education/20220923-19:37:15/checkpoint-2376/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task education/20220923-19:37:15/checkpoint-2376/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/PASTEL single task education/20220923-19:37:15/checkpoint-2178] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25492\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task education/20220923-19:37:15/checkpoint-2574\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task education/20220923-19:37:15/checkpoint-2574/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task education/20220923-19:37:15/checkpoint-2574/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task education/20220923-19:37:15/checkpoint-2574/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task education/20220923-19:37:15/checkpoint-2574/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/PASTEL single task education/20220923-19:37:15/checkpoint-2376] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25492\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task education/20220923-19:37:15/checkpoint-2772\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task education/20220923-19:37:15/checkpoint-2772/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task education/20220923-19:37:15/checkpoint-2772/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task education/20220923-19:37:15/checkpoint-2772/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task education/20220923-19:37:15/checkpoint-2772/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/PASTEL single task education/20220923-19:37:15/checkpoint-2574] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25492\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task education/20220923-19:37:15/checkpoint-2970\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task education/20220923-19:37:15/checkpoint-2970/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task education/20220923-19:37:15/checkpoint-2970/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task education/20220923-19:37:15/checkpoint-2970/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task education/20220923-19:37:15/checkpoint-2970/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/PASTEL single task education/20220923-19:37:15/checkpoint-2772] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /scratch/data_jz17d/result/PASTEL single task education/20220923-19:37:15/checkpoint-594 (score: 1.607066035270691).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 6314\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='995' max='198' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [198/198 01:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "***** Running Prediction *****\n",
      "  Num examples = 25492\n",
      "  Batch size = 32\n",
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "################# settings #################\n",
    "selected_tasks = ['education']\n",
    "############################################\n",
    "# build args\n",
    "my_training_args = MyTrainingArgs(selected_tasks=selected_tasks,\n",
    "                                  model_name=f'PASTEL single task {selected_tasks[0]}',\n",
    "                                  base_model_name='bert-base-uncased',\n",
    "                                  freeze_bert=False,\n",
    "                                  use_pooler=False,\n",
    "                                  num_epoch=num_epoch,\n",
    "#                                   data_limit=30000,\n",
    "                                 )\n",
    "hg_training_args = TrainingArguments(\n",
    "    output_dir=my_training_args.model_folder,   # output directory\n",
    "    num_train_epochs=my_training_args.num_epoch,     # total number of training epochs\n",
    "    per_device_train_batch_size=my_training_args.batch_size,  # batch size per device during training\n",
    "    per_device_eval_batch_size=my_training_args.batch_size,   # batch size for evaluation\n",
    "    warmup_steps=my_training_args.num_warmup_steps,    # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir=f\"{my_training_args.model_folder}/logs\",  # directory for storing logs\n",
    "    logging_first_step = True, \n",
    "    evaluation_strategy=\"epoch\",     # either 'epoch' or 'steps'\n",
    "    save_strategy = 'epoch',\n",
    "    save_total_limit = 1,\n",
    "    metric_for_best_model='eval_loss',\n",
    "    greater_is_better=False,\n",
    "    load_best_model_at_end=True,  # by default decide on loss, change it by setting metric_for_best_model\n",
    "    report_to=\"wandb\",\n",
    "    run_name=f\"single task bert on unmasked PASTEL-{selected_tasks[0]}\",\n",
    ")\n",
    "\n",
    "# initialize model and dataset\n",
    "model = init_model(my_training_args)\n",
    "freeze_model(model, my_training_args.freeze_bert)\n",
    "\n",
    "train_dataset = MyDataset(my_training_args, f'p={p}_unmasked_train')\n",
    "val_dataset = MyDataset(my_training_args, f'p={p}_masked_train')\n",
    "\n",
    "trainer = MyTrainer(\n",
    "    model=model,   # the instantiated Transformers model to be trained\n",
    "    args=hg_training_args,                  # training arguments, defined above\n",
    "    tokenizer=model.tokenizer, \n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=val_dataset,          # evaluation dataset\n",
    "    compute_metrics=compute_metrics,     # the callback that computes metrics of interest\n",
    ")\n",
    "\n",
    "# train\n",
    "trainer.train()\n",
    "\n",
    "# get prediction and save to file\n",
    "if save_prediction:\n",
    "    df_train = predict_logits_labels(trainer, train_dataset, selected_tasks[0])\n",
    "    df_val = predict_logits_labels(trainer, val_dataset, selected_tasks[0])\n",
    "    df_combined = pd.concat([df_train, df_val], axis = 0)\n",
    "    df_combined = df_combined.sort_values('Unnamed: 0')\n",
    "\n",
    "    output_path = f'{data_folder}/pastel/processed/p={p}_predicted'\n",
    "    Path(output_path).mkdir(parents=True, exist_ok=True)\n",
    "    df_combined.to_csv(f'{output_path}/{selected_tasks[0]}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d65792",
   "metadata": {},
   "source": [
    "## ethnic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d400a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/jz17d/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/jz17d/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/jz17d/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/jz17d/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/jz17d/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/jz17d/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/jz17d/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/jz17d/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/jz17d/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/jz17d/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "***** Running training *****\n",
      "  Num examples = 6346\n",
      "  Num Epochs = 15\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2985\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2985' max='2985' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2985/2985 26:16, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy Ethnic</th>\n",
       "      <th>F1 Ethnic</th>\n",
       "      <th>Precision Ethnic</th>\n",
       "      <th>Recall Ethnic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.073800</td>\n",
       "      <td>0.732880</td>\n",
       "      <td>0.823998</td>\n",
       "      <td>0.181246</td>\n",
       "      <td>0.174819</td>\n",
       "      <td>0.189549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.073800</td>\n",
       "      <td>0.737943</td>\n",
       "      <td>0.827023</td>\n",
       "      <td>0.195441</td>\n",
       "      <td>0.291460</td>\n",
       "      <td>0.195738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.824000</td>\n",
       "      <td>0.753929</td>\n",
       "      <td>0.825373</td>\n",
       "      <td>0.214168</td>\n",
       "      <td>0.321434</td>\n",
       "      <td>0.210010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.824000</td>\n",
       "      <td>0.884629</td>\n",
       "      <td>0.817243</td>\n",
       "      <td>0.212105</td>\n",
       "      <td>0.294603</td>\n",
       "      <td>0.207276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.824000</td>\n",
       "      <td>1.230417</td>\n",
       "      <td>0.798115</td>\n",
       "      <td>0.228884</td>\n",
       "      <td>0.276504</td>\n",
       "      <td>0.219622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.330000</td>\n",
       "      <td>1.463447</td>\n",
       "      <td>0.802435</td>\n",
       "      <td>0.221225</td>\n",
       "      <td>0.275348</td>\n",
       "      <td>0.214236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.330000</td>\n",
       "      <td>1.541822</td>\n",
       "      <td>0.795208</td>\n",
       "      <td>0.236036</td>\n",
       "      <td>0.272753</td>\n",
       "      <td>0.227990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.050800</td>\n",
       "      <td>1.631372</td>\n",
       "      <td>0.801571</td>\n",
       "      <td>0.230119</td>\n",
       "      <td>0.282121</td>\n",
       "      <td>0.217932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.050800</td>\n",
       "      <td>1.669639</td>\n",
       "      <td>0.802985</td>\n",
       "      <td>0.232775</td>\n",
       "      <td>0.294984</td>\n",
       "      <td>0.218726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.050800</td>\n",
       "      <td>1.720805</td>\n",
       "      <td>0.816889</td>\n",
       "      <td>0.227942</td>\n",
       "      <td>0.315748</td>\n",
       "      <td>0.216032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.006200</td>\n",
       "      <td>1.774852</td>\n",
       "      <td>0.813826</td>\n",
       "      <td>0.227676</td>\n",
       "      <td>0.317166</td>\n",
       "      <td>0.215587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.006200</td>\n",
       "      <td>1.764282</td>\n",
       "      <td>0.812451</td>\n",
       "      <td>0.227496</td>\n",
       "      <td>0.302180</td>\n",
       "      <td>0.215696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>1.762867</td>\n",
       "      <td>0.809859</td>\n",
       "      <td>0.229830</td>\n",
       "      <td>0.297608</td>\n",
       "      <td>0.217667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>1.763121</td>\n",
       "      <td>0.808955</td>\n",
       "      <td>0.230506</td>\n",
       "      <td>0.294838</td>\n",
       "      <td>0.218289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>1.764714</td>\n",
       "      <td>0.809544</td>\n",
       "      <td>0.230571</td>\n",
       "      <td>0.296780</td>\n",
       "      <td>0.218235</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 25460\n",
      "  Batch size = 32\n",
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task ethnic/20220923-20:04:34/checkpoint-199\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task ethnic/20220923-20:04:34/checkpoint-199/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task ethnic/20220923-20:04:34/checkpoint-199/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task ethnic/20220923-20:04:34/checkpoint-199/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task ethnic/20220923-20:04:34/checkpoint-199/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25460\n",
      "  Batch size = 32\n",
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task ethnic/20220923-20:04:34/checkpoint-398\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task ethnic/20220923-20:04:34/checkpoint-398/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task ethnic/20220923-20:04:34/checkpoint-398/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task ethnic/20220923-20:04:34/checkpoint-398/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task ethnic/20220923-20:04:34/checkpoint-398/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25460\n",
      "  Batch size = 32\n",
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task ethnic/20220923-20:04:34/checkpoint-597\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task ethnic/20220923-20:04:34/checkpoint-597/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task ethnic/20220923-20:04:34/checkpoint-597/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task ethnic/20220923-20:04:34/checkpoint-597/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task ethnic/20220923-20:04:34/checkpoint-597/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/PASTEL single task ethnic/20220923-20:04:34/checkpoint-398] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25460\n",
      "  Batch size = 32\n",
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task ethnic/20220923-20:04:34/checkpoint-796\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task ethnic/20220923-20:04:34/checkpoint-796/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task ethnic/20220923-20:04:34/checkpoint-796/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task ethnic/20220923-20:04:34/checkpoint-796/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task ethnic/20220923-20:04:34/checkpoint-796/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/PASTEL single task ethnic/20220923-20:04:34/checkpoint-597] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25460\n",
      "  Batch size = 32\n",
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task ethnic/20220923-20:04:34/checkpoint-995\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task ethnic/20220923-20:04:34/checkpoint-995/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task ethnic/20220923-20:04:34/checkpoint-995/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task ethnic/20220923-20:04:34/checkpoint-995/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task ethnic/20220923-20:04:34/checkpoint-995/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/PASTEL single task ethnic/20220923-20:04:34/checkpoint-796] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25460\n",
      "  Batch size = 32\n",
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task ethnic/20220923-20:04:34/checkpoint-1194\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task ethnic/20220923-20:04:34/checkpoint-1194/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task ethnic/20220923-20:04:34/checkpoint-1194/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task ethnic/20220923-20:04:34/checkpoint-1194/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task ethnic/20220923-20:04:34/checkpoint-1194/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/PASTEL single task ethnic/20220923-20:04:34/checkpoint-995] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25460\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task ethnic/20220923-20:04:34/checkpoint-1393\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task ethnic/20220923-20:04:34/checkpoint-1393/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task ethnic/20220923-20:04:34/checkpoint-1393/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task ethnic/20220923-20:04:34/checkpoint-1393/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task ethnic/20220923-20:04:34/checkpoint-1393/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/PASTEL single task ethnic/20220923-20:04:34/checkpoint-1194] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25460\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task ethnic/20220923-20:04:34/checkpoint-1592\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task ethnic/20220923-20:04:34/checkpoint-1592/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task ethnic/20220923-20:04:34/checkpoint-1592/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task ethnic/20220923-20:04:34/checkpoint-1592/tokenizer_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task ethnic/20220923-20:04:34/checkpoint-1592/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/PASTEL single task ethnic/20220923-20:04:34/checkpoint-1393] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25460\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task ethnic/20220923-20:04:34/checkpoint-1791\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task ethnic/20220923-20:04:34/checkpoint-1791/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task ethnic/20220923-20:04:34/checkpoint-1791/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task ethnic/20220923-20:04:34/checkpoint-1791/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task ethnic/20220923-20:04:34/checkpoint-1791/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/PASTEL single task ethnic/20220923-20:04:34/checkpoint-1592] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25460\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task ethnic/20220923-20:04:34/checkpoint-1990\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task ethnic/20220923-20:04:34/checkpoint-1990/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task ethnic/20220923-20:04:34/checkpoint-1990/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task ethnic/20220923-20:04:34/checkpoint-1990/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task ethnic/20220923-20:04:34/checkpoint-1990/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/PASTEL single task ethnic/20220923-20:04:34/checkpoint-1791] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25460\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task ethnic/20220923-20:04:34/checkpoint-2189\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task ethnic/20220923-20:04:34/checkpoint-2189/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task ethnic/20220923-20:04:34/checkpoint-2189/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task ethnic/20220923-20:04:34/checkpoint-2189/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task ethnic/20220923-20:04:34/checkpoint-2189/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/PASTEL single task ethnic/20220923-20:04:34/checkpoint-1990] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25460\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task ethnic/20220923-20:04:34/checkpoint-2388\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task ethnic/20220923-20:04:34/checkpoint-2388/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task ethnic/20220923-20:04:34/checkpoint-2388/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task ethnic/20220923-20:04:34/checkpoint-2388/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task ethnic/20220923-20:04:34/checkpoint-2388/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/PASTEL single task ethnic/20220923-20:04:34/checkpoint-2189] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25460\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task ethnic/20220923-20:04:34/checkpoint-2587\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task ethnic/20220923-20:04:34/checkpoint-2587/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task ethnic/20220923-20:04:34/checkpoint-2587/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task ethnic/20220923-20:04:34/checkpoint-2587/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task ethnic/20220923-20:04:34/checkpoint-2587/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/PASTEL single task ethnic/20220923-20:04:34/checkpoint-2388] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25460\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task ethnic/20220923-20:04:34/checkpoint-2786\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task ethnic/20220923-20:04:34/checkpoint-2786/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task ethnic/20220923-20:04:34/checkpoint-2786/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task ethnic/20220923-20:04:34/checkpoint-2786/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task ethnic/20220923-20:04:34/checkpoint-2786/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/PASTEL single task ethnic/20220923-20:04:34/checkpoint-2587] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25460\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task ethnic/20220923-20:04:34/checkpoint-2985\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task ethnic/20220923-20:04:34/checkpoint-2985/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task ethnic/20220923-20:04:34/checkpoint-2985/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task ethnic/20220923-20:04:34/checkpoint-2985/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task ethnic/20220923-20:04:34/checkpoint-2985/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/PASTEL single task ethnic/20220923-20:04:34/checkpoint-2786] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /scratch/data_jz17d/result/PASTEL single task ethnic/20220923-20:04:34/checkpoint-199 (score: 0.732879638671875).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 6346\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='995' max='199' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [199/199 01:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "***** Running Prediction *****\n",
      "  Num examples = 25460\n",
      "  Batch size = 32\n",
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "################# settings #################\n",
    "selected_tasks = ['ethnic']\n",
    "############################################\n",
    "# build args\n",
    "my_training_args = MyTrainingArgs(selected_tasks=selected_tasks,\n",
    "                                  model_name=f'PASTEL single task {selected_tasks[0]}',\n",
    "                                  base_model_name='bert-base-uncased',\n",
    "                                  freeze_bert=False,\n",
    "                                  use_pooler=False,\n",
    "                                  num_epoch=num_epoch,\n",
    "#                                   data_limit=30000,\n",
    "                                 )\n",
    "hg_training_args = TrainingArguments(\n",
    "    output_dir=my_training_args.model_folder,   # output directory\n",
    "    num_train_epochs=my_training_args.num_epoch,     # total number of training epochs\n",
    "    per_device_train_batch_size=my_training_args.batch_size,  # batch size per device during training\n",
    "    per_device_eval_batch_size=my_training_args.batch_size,   # batch size for evaluation\n",
    "    warmup_steps=my_training_args.num_warmup_steps,    # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir=f\"{my_training_args.model_folder}/logs\",  # directory for storing logs\n",
    "    logging_first_step = True, \n",
    "    evaluation_strategy=\"epoch\",     # either 'epoch' or 'steps'\n",
    "    save_strategy = 'epoch',\n",
    "    save_total_limit = 1,\n",
    "    metric_for_best_model='eval_loss',\n",
    "    greater_is_better=False,\n",
    "    load_best_model_at_end=True,  # by default decide on loss, change it by setting metric_for_best_model\n",
    "    report_to=\"wandb\",\n",
    "    run_name=f\"single task bert on unmasked PASTEL-{selected_tasks[0]}\",\n",
    ")\n",
    "\n",
    "# initialize model and dataset\n",
    "model = init_model(my_training_args)\n",
    "freeze_model(model, my_training_args.freeze_bert)\n",
    "\n",
    "train_dataset = MyDataset(my_training_args, f'p={p}_unmasked_train')\n",
    "val_dataset = MyDataset(my_training_args, f'p={p}_masked_train')\n",
    "\n",
    "trainer = MyTrainer(\n",
    "    model=model,   # the instantiated Transformers model to be trained\n",
    "    args=hg_training_args,                  # training arguments, defined above\n",
    "    tokenizer=model.tokenizer, \n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=val_dataset,          # evaluation dataset\n",
    "    compute_metrics=compute_metrics,     # the callback that computes metrics of interest\n",
    ")\n",
    "\n",
    "# train\n",
    "trainer.train()\n",
    "\n",
    "# get prediction and save to file\n",
    "if save_prediction:\n",
    "    df_train = predict_logits_labels(trainer, train_dataset, selected_tasks[0])\n",
    "    df_val = predict_logits_labels(trainer, val_dataset, selected_tasks[0])\n",
    "    df_combined = pd.concat([df_train, df_val], axis = 0)\n",
    "    df_combined = df_combined.sort_values('Unnamed: 0')\n",
    "\n",
    "    output_path = f'{data_folder}/pastel/processed/p={p}_predicted'\n",
    "    Path(output_path).mkdir(parents=True, exist_ok=True)\n",
    "    df_combined.to_csv(f'{output_path}/{selected_tasks[0]}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc0c332",
   "metadata": {},
   "source": [
    "## gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f7d385",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/jz17d/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/jz17d/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/jz17d/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/jz17d/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/jz17d/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/jz17d/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/jz17d/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/jz17d/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/jz17d/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/jz17d/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "***** Running training *****\n",
      "  Num examples = 6465\n",
      "  Num Epochs = 15\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3045\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3045' max='3045' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3045/3045 26:18, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy Gender</th>\n",
       "      <th>F1 Gender</th>\n",
       "      <th>Precision Gender</th>\n",
       "      <th>Recall Gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.538900</td>\n",
       "      <td>0.614407</td>\n",
       "      <td>0.720137</td>\n",
       "      <td>0.415768</td>\n",
       "      <td>0.522393</td>\n",
       "      <td>0.421009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.538900</td>\n",
       "      <td>0.603215</td>\n",
       "      <td>0.727240</td>\n",
       "      <td>0.448253</td>\n",
       "      <td>0.483211</td>\n",
       "      <td>0.444678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.638400</td>\n",
       "      <td>0.674099</td>\n",
       "      <td>0.738013</td>\n",
       "      <td>0.444116</td>\n",
       "      <td>0.516560</td>\n",
       "      <td>0.441921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.638400</td>\n",
       "      <td>0.855589</td>\n",
       "      <td>0.707391</td>\n",
       "      <td>0.445786</td>\n",
       "      <td>0.456279</td>\n",
       "      <td>0.443348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.294000</td>\n",
       "      <td>1.327684</td>\n",
       "      <td>0.666785</td>\n",
       "      <td>0.432554</td>\n",
       "      <td>0.430010</td>\n",
       "      <td>0.435983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.294000</td>\n",
       "      <td>1.638546</td>\n",
       "      <td>0.720571</td>\n",
       "      <td>0.447603</td>\n",
       "      <td>0.503298</td>\n",
       "      <td>0.441895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.294000</td>\n",
       "      <td>1.863245</td>\n",
       "      <td>0.695908</td>\n",
       "      <td>0.439915</td>\n",
       "      <td>0.447327</td>\n",
       "      <td>0.437821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.080800</td>\n",
       "      <td>2.037188</td>\n",
       "      <td>0.697644</td>\n",
       "      <td>0.444781</td>\n",
       "      <td>0.454738</td>\n",
       "      <td>0.441608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.080800</td>\n",
       "      <td>2.170302</td>\n",
       "      <td>0.717533</td>\n",
       "      <td>0.455552</td>\n",
       "      <td>0.495311</td>\n",
       "      <td>0.449200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.044100</td>\n",
       "      <td>2.272039</td>\n",
       "      <td>0.710075</td>\n",
       "      <td>0.449841</td>\n",
       "      <td>0.471159</td>\n",
       "      <td>0.444719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.044100</td>\n",
       "      <td>2.290395</td>\n",
       "      <td>0.713429</td>\n",
       "      <td>0.450482</td>\n",
       "      <td>0.477722</td>\n",
       "      <td>0.444944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.044100</td>\n",
       "      <td>2.344296</td>\n",
       "      <td>0.710627</td>\n",
       "      <td>0.450623</td>\n",
       "      <td>0.471782</td>\n",
       "      <td>0.445447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.031000</td>\n",
       "      <td>2.417286</td>\n",
       "      <td>0.717217</td>\n",
       "      <td>0.450547</td>\n",
       "      <td>0.478071</td>\n",
       "      <td>0.445694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.031000</td>\n",
       "      <td>2.433922</td>\n",
       "      <td>0.709404</td>\n",
       "      <td>0.449085</td>\n",
       "      <td>0.468399</td>\n",
       "      <td>0.444801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.026000</td>\n",
       "      <td>2.446001</td>\n",
       "      <td>0.710509</td>\n",
       "      <td>0.448895</td>\n",
       "      <td>0.469503</td>\n",
       "      <td>0.444510</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 25341\n",
      "  Batch size = 32\n",
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task gender/20220923-20:32:02/checkpoint-203\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task gender/20220923-20:32:02/checkpoint-203/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task gender/20220923-20:32:02/checkpoint-203/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task gender/20220923-20:32:02/checkpoint-203/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task gender/20220923-20:32:02/checkpoint-203/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25341\n",
      "  Batch size = 32\n",
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task gender/20220923-20:32:02/checkpoint-406\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task gender/20220923-20:32:02/checkpoint-406/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task gender/20220923-20:32:02/checkpoint-406/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task gender/20220923-20:32:02/checkpoint-406/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task gender/20220923-20:32:02/checkpoint-406/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/PASTEL single task gender/20220923-20:32:02/checkpoint-203] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25341\n",
      "  Batch size = 32\n",
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task gender/20220923-20:32:02/checkpoint-609\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task gender/20220923-20:32:02/checkpoint-609/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task gender/20220923-20:32:02/checkpoint-609/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task gender/20220923-20:32:02/checkpoint-609/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task gender/20220923-20:32:02/checkpoint-609/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25341\n",
      "  Batch size = 32\n",
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task gender/20220923-20:32:02/checkpoint-812\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task gender/20220923-20:32:02/checkpoint-812/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task gender/20220923-20:32:02/checkpoint-812/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task gender/20220923-20:32:02/checkpoint-812/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task gender/20220923-20:32:02/checkpoint-812/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/PASTEL single task gender/20220923-20:32:02/checkpoint-609] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25341\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task gender/20220923-20:32:02/checkpoint-1015\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task gender/20220923-20:32:02/checkpoint-1015/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task gender/20220923-20:32:02/checkpoint-1015/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task gender/20220923-20:32:02/checkpoint-1015/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task gender/20220923-20:32:02/checkpoint-1015/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/PASTEL single task gender/20220923-20:32:02/checkpoint-812] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25341\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task gender/20220923-20:32:02/checkpoint-1218\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task gender/20220923-20:32:02/checkpoint-1218/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task gender/20220923-20:32:02/checkpoint-1218/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task gender/20220923-20:32:02/checkpoint-1218/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task gender/20220923-20:32:02/checkpoint-1218/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/PASTEL single task gender/20220923-20:32:02/checkpoint-1015] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25341\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task gender/20220923-20:32:02/checkpoint-1421\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task gender/20220923-20:32:02/checkpoint-1421/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task gender/20220923-20:32:02/checkpoint-1421/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task gender/20220923-20:32:02/checkpoint-1421/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task gender/20220923-20:32:02/checkpoint-1421/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/PASTEL single task gender/20220923-20:32:02/checkpoint-1218] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25341\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task gender/20220923-20:32:02/checkpoint-1624\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task gender/20220923-20:32:02/checkpoint-1624/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task gender/20220923-20:32:02/checkpoint-1624/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task gender/20220923-20:32:02/checkpoint-1624/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task gender/20220923-20:32:02/checkpoint-1624/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/PASTEL single task gender/20220923-20:32:02/checkpoint-1421] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25341\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task gender/20220923-20:32:02/checkpoint-1827\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task gender/20220923-20:32:02/checkpoint-1827/config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task gender/20220923-20:32:02/checkpoint-1827/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task gender/20220923-20:32:02/checkpoint-1827/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task gender/20220923-20:32:02/checkpoint-1827/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/PASTEL single task gender/20220923-20:32:02/checkpoint-1624] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25341\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task gender/20220923-20:32:02/checkpoint-2030\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task gender/20220923-20:32:02/checkpoint-2030/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task gender/20220923-20:32:02/checkpoint-2030/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task gender/20220923-20:32:02/checkpoint-2030/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task gender/20220923-20:32:02/checkpoint-2030/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/PASTEL single task gender/20220923-20:32:02/checkpoint-1827] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25341\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task gender/20220923-20:32:02/checkpoint-2233\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task gender/20220923-20:32:02/checkpoint-2233/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task gender/20220923-20:32:02/checkpoint-2233/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task gender/20220923-20:32:02/checkpoint-2233/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task gender/20220923-20:32:02/checkpoint-2233/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/PASTEL single task gender/20220923-20:32:02/checkpoint-2030] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25341\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task gender/20220923-20:32:02/checkpoint-2436\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task gender/20220923-20:32:02/checkpoint-2436/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task gender/20220923-20:32:02/checkpoint-2436/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task gender/20220923-20:32:02/checkpoint-2436/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task gender/20220923-20:32:02/checkpoint-2436/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/PASTEL single task gender/20220923-20:32:02/checkpoint-2233] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25341\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task gender/20220923-20:32:02/checkpoint-2639\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task gender/20220923-20:32:02/checkpoint-2639/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task gender/20220923-20:32:02/checkpoint-2639/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task gender/20220923-20:32:02/checkpoint-2639/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task gender/20220923-20:32:02/checkpoint-2639/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/PASTEL single task gender/20220923-20:32:02/checkpoint-2436] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25341\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task gender/20220923-20:32:02/checkpoint-2842\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task gender/20220923-20:32:02/checkpoint-2842/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task gender/20220923-20:32:02/checkpoint-2842/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task gender/20220923-20:32:02/checkpoint-2842/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task gender/20220923-20:32:02/checkpoint-2842/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/PASTEL single task gender/20220923-20:32:02/checkpoint-2639] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25341\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task gender/20220923-20:32:02/checkpoint-3045\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task gender/20220923-20:32:02/checkpoint-3045/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task gender/20220923-20:32:02/checkpoint-3045/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task gender/20220923-20:32:02/checkpoint-3045/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task gender/20220923-20:32:02/checkpoint-3045/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/PASTEL single task gender/20220923-20:32:02/checkpoint-2842] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /scratch/data_jz17d/result/PASTEL single task gender/20220923-20:32:02/checkpoint-406 (score: 0.6032149791717529).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 6465\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='995' max='203' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [203/203 01:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "***** Running Prediction *****\n",
      "  Num examples = 25341\n",
      "  Batch size = 32\n",
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "################# settings #################\n",
    "selected_tasks = ['gender']\n",
    "############################################\n",
    "# build args\n",
    "my_training_args = MyTrainingArgs(selected_tasks=selected_tasks,\n",
    "                                  model_name=f'PASTEL single task {selected_tasks[0]}',\n",
    "                                  base_model_name='bert-base-uncased',\n",
    "                                  freeze_bert=False,\n",
    "                                  use_pooler=False,\n",
    "                                  num_epoch=num_epoch,\n",
    "#                                   data_limit=30000,\n",
    "                                 )\n",
    "hg_training_args = TrainingArguments(\n",
    "    output_dir=my_training_args.model_folder,   # output directory\n",
    "    num_train_epochs=my_training_args.num_epoch,     # total number of training epochs\n",
    "    per_device_train_batch_size=my_training_args.batch_size,  # batch size per device during training\n",
    "    per_device_eval_batch_size=my_training_args.batch_size,   # batch size for evaluation\n",
    "    warmup_steps=my_training_args.num_warmup_steps,    # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir=f\"{my_training_args.model_folder}/logs\",  # directory for storing logs\n",
    "    logging_first_step = True, \n",
    "    evaluation_strategy=\"epoch\",     # either 'epoch' or 'steps'\n",
    "    save_strategy = 'epoch',\n",
    "    save_total_limit = 1,\n",
    "    metric_for_best_model='eval_loss',\n",
    "    greater_is_better=False,\n",
    "    load_best_model_at_end=True,  # by default decide on loss, change it by setting metric_for_best_model\n",
    "    report_to=\"wandb\",\n",
    "    run_name=f\"single task bert on unmasked PASTEL-{selected_tasks[0]}\",\n",
    ")\n",
    "\n",
    "# initialize model and dataset\n",
    "model = init_model(my_training_args)\n",
    "freeze_model(model, my_training_args.freeze_bert)\n",
    "\n",
    "train_dataset = MyDataset(my_training_args, f'p={p}_unmasked_train')\n",
    "val_dataset = MyDataset(my_training_args, f'p={p}_masked_train')\n",
    "\n",
    "trainer = MyTrainer(\n",
    "    model=model,   # the instantiated Transformers model to be trained\n",
    "    args=hg_training_args,                  # training arguments, defined above\n",
    "    tokenizer=model.tokenizer, \n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=val_dataset,          # evaluation dataset\n",
    "    compute_metrics=compute_metrics,     # the callback that computes metrics of interest\n",
    ")\n",
    "\n",
    "# train\n",
    "trainer.train()\n",
    "\n",
    "# get prediction and save to file\n",
    "if save_prediction:\n",
    "    df_train = predict_logits_labels(trainer, train_dataset, selected_tasks[0])\n",
    "    df_val = predict_logits_labels(trainer, val_dataset, selected_tasks[0])\n",
    "    df_combined = pd.concat([df_train, df_val], axis = 0)\n",
    "    df_combined = df_combined.sort_values('Unnamed: 0')\n",
    "\n",
    "    output_path = f'{data_folder}/pastel/processed/p={p}_predicted'\n",
    "    Path(output_path).mkdir(parents=True, exist_ok=True)\n",
    "    df_combined.to_csv(f'{output_path}/{selected_tasks[0]}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052eb701",
   "metadata": {},
   "source": [
    "# combine all prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f3f12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_prediction:\n",
    "    df_combined = None\n",
    "    for task in tasks2labels:\n",
    "        df = pd.read_csv(f'{data_folder}/pastel/processed/p={p}_predicted/{task}.csv')\n",
    "        df = df.rename({'logits': f'logits_{task}'}, axis=1)\n",
    "        if df_combined is None:\n",
    "            df_combined = df\n",
    "        else:\n",
    "            df_combined[task] = df[task]\n",
    "            df_combined[f'logits_{task}'] = df[f'logits_{task}']\n",
    "            df_combined[f'predicted_{task}'] = df[f'predicted_{task}']\n",
    "    del df_combined['Unnamed: 0']\n",
    "    df_combined.to_csv(f'{data_folder}/pastel/processed/p={p}_predicted/pastel.csv', index=False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c060d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy_age</td><td>▅█▇▄▃▃▁▁▃▁▂▄▁▂▃</td></tr><tr><td>eval/accuracy_country</td><td>██▁▅▆▆▇▇▇▇▇▇▇▇▇</td></tr><tr><td>eval/accuracy_education</td><td>▇▃█▅▃▁▂▂▂▂▂▃▄▃▃</td></tr><tr><td>eval/accuracy_ethnic</td><td>▇██▆▂▃▁▂▃▆▅▅▄▄▄</td></tr><tr><td>eval/accuracy_gender</td><td>▆▇█▅▁▆▄▄▆▅▆▅▆▅▅</td></tr><tr><td>eval/accuracy_politics</td><td>▆█▆▁▄▆▄▅▅▆▅▅▅▆▆</td></tr><tr><td>eval/accuracy_tod</td><td>▇█▅▁▁▃▃▃▄▃▃▃▃▃▃</td></tr><tr><td>eval/f1_age</td><td>▁▅▅▇▇▇▇▇▇██████</td></tr><tr><td>eval/f1_country</td><td>▁▁█▆▇▆▇▄▄▄▅▅▅▅▅</td></tr><tr><td>eval/f1_education</td><td>▁▅▆▆▇▆█▆▇███▇██</td></tr><tr><td>eval/f1_ethnic</td><td>▁▃▅▅▇▆█▇█▇▇▇▇▇▇</td></tr><tr><td>eval/f1_gender</td><td>▁▇▆▆▄▇▅▆█▇▇▇▇▇▇</td></tr><tr><td>eval/f1_politics</td><td>▁▂▆▇█▆▇▇█▇█▇███</td></tr><tr><td>eval/f1_tod</td><td>▁▁▄▅▆▆█▆▆▇▇▇███</td></tr><tr><td>eval/loss</td><td>▁▁▁▁▁▁▂▃▅▆▇▇▃▄▇██▃▄▅▇▇█▃▅▆███▂▃▃▄▄▂▂▄▄▅▅</td></tr><tr><td>eval/precision_age</td><td>▁▇█▅▆▆▇▇▆▆▇▇▆▆▆</td></tr><tr><td>eval/precision_country</td><td>▁▁▄▄▆▇█▇▆▇▆▆▆▇▇</td></tr><tr><td>eval/precision_education</td><td>▃█▅▂▆▁▃▁▂▂▂▃▂▂▂</td></tr><tr><td>eval/precision_ethnic</td><td>▁▇█▇▆▆▆▆▇██▇▇▇▇</td></tr><tr><td>eval/precision_gender</td><td>█▅█▃▁▇▂▃▆▄▅▄▅▄▄</td></tr><tr><td>eval/precision_politics</td><td>▁█▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>eval/precision_tod</td><td>▁▄▆█▄▇▇▅▆▇▆▇▇█▇</td></tr><tr><td>eval/recall_age</td><td>▁▄▅▇▆▆▇▇▇██▇███</td></tr><tr><td>eval/recall_country</td><td>▁▁█▅▆▅▅▃▃▃▄▄▄▄▄</td></tr><tr><td>eval/recall_education</td><td>▁▄▄▅▆▅▇▆▇▇█▇▆▇▇</td></tr><tr><td>eval/recall_ethnic</td><td>▁▂▅▄▆▅█▆▆▆▆▆▆▆▆</td></tr><tr><td>eval/recall_gender</td><td>▁▇▆▇▅▆▅▆█▇▇▇▇▇▇</td></tr><tr><td>eval/recall_politics</td><td>▁▃▅█▇▅▆▆▇▆▇▆▇▇▇</td></tr><tr><td>eval/recall_tod</td><td>▁▃▁▄▅▄█▄▄▆▇▆▇█▇</td></tr><tr><td>eval/runtime</td><td>▁▂█▄▃▃▄▄▃▃▃▃▂▃▂▂▂▃▃▃▃▃▃▃▃▃▃▃▃▃▃▂▃▂▂▂▂▂▂▂</td></tr><tr><td>eval/samples_per_second</td><td>█▇▁▅▅▆▆▆▇▆▆▆▇▆▇▇▇▆▆▆▆▆▆▆▆▇▇▆▆▇▆▇▆▇▆▇▇▆▇▆</td></tr><tr><td>eval/steps_per_second</td><td>█▇▁▅▅▆▆▆▇▆▆▆▇▆▇▆▇▆▆▆▆▆▆▆▆▇▇▆▆▇▆▇▆▇▆▇▇▆▇▆</td></tr><tr><td>train/epoch</td><td>▁▂▃▅▆█▁▂▄▅▇█▂▃▄▆▇█▂▃▅▆█▁▃▄▆▇█▂▃▅▆█▁▃▄▆▇█</td></tr><tr><td>train/global_step</td><td>▁▂▃▅▆▇▁▂▄▅▆█▂▃▄▆▇█▂▃▅▆▇▁▃▄▆▇█▂▃▅▆▇▁▃▄▆▇█</td></tr><tr><td>train/learning_rate</td><td>▁█▇▅▄▂▁▁▇▅▄▂▁█▇▅▂▁▁█▇▅▄▂█▇▅▄▂▁█▇▄▂▁█▇▅▄▁</td></tr><tr><td>train/loss</td><td>▅▂▁▁▁▁▁▅▂▁▁▁▇▆▃▁▁▁█▆▃▁▁▁▇▄▁▁▁█▄▂▁▁▆▃▂▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁▃▄▄▂▅█</td></tr><tr><td>train/train_loss</td><td>▁▅▇▇█▄▄</td></tr><tr><td>train/train_runtime</td><td>█▂▁▃▁▃▃</td></tr><tr><td>train/train_samples_per_second</td><td>▂▁▆▅▄▄█</td></tr><tr><td>train/train_steps_per_second</td><td>▁▁▆▄▄▄█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy_age</td><td>0.38902</td></tr><tr><td>eval/accuracy_country</td><td>0.97541</td></tr><tr><td>eval/accuracy_education</td><td>0.36541</td></tr><tr><td>eval/accuracy_ethnic</td><td>0.80954</td></tr><tr><td>eval/accuracy_gender</td><td>0.71051</td></tr><tr><td>eval/accuracy_politics</td><td>0.48118</td></tr><tr><td>eval/accuracy_tod</td><td>0.3796</td></tr><tr><td>eval/f1_age</td><td>0.21693</td></tr><tr><td>eval/f1_country</td><td>0.33871</td></tr><tr><td>eval/f1_education</td><td>0.20028</td></tr><tr><td>eval/f1_ethnic</td><td>0.23057</td></tr><tr><td>eval/f1_gender</td><td>0.4489</td></tr><tr><td>eval/f1_politics</td><td>0.42481</td></tr><tr><td>eval/f1_tod</td><td>0.27105</td></tr><tr><td>eval/loss</td><td>2.446</td></tr><tr><td>eval/precision_age</td><td>0.22822</td></tr><tr><td>eval/precision_country</td><td>0.3631</td></tr><tr><td>eval/precision_education</td><td>0.20862</td></tr><tr><td>eval/precision_ethnic</td><td>0.29678</td></tr><tr><td>eval/precision_gender</td><td>0.4695</td></tr><tr><td>eval/precision_politics</td><td>0.42946</td></tr><tr><td>eval/precision_tod</td><td>0.27891</td></tr><tr><td>eval/recall_age</td><td>0.21235</td></tr><tr><td>eval/recall_country</td><td>0.33783</td></tr><tr><td>eval/recall_education</td><td>0.19797</td></tr><tr><td>eval/recall_ethnic</td><td>0.21824</td></tr><tr><td>eval/recall_gender</td><td>0.44451</td></tr><tr><td>eval/recall_politics</td><td>0.42351</td></tr><tr><td>eval/recall_tod</td><td>0.27237</td></tr><tr><td>eval/runtime</td><td>50.0277</td></tr><tr><td>eval/samples_per_second</td><td>506.54</td></tr><tr><td>eval/steps_per_second</td><td>15.831</td></tr><tr><td>train/epoch</td><td>15.0</td></tr><tr><td>train/global_step</td><td>3045</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.026</td></tr><tr><td>train/total_flos</td><td>1446265595536020.0</td></tr><tr><td>train/train_loss</td><td>0.18364</td></tr><tr><td>train/train_runtime</td><td>1578.8722</td></tr><tr><td>train/train_samples_per_second</td><td>61.42</td></tr><tr><td>train/train_steps_per_second</td><td>1.929</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">single task bert on unmasked PASTEL-country</strong>: <a href=\"https://wandb.ai/fsu-dsc-cil/PASTEL%20single%20task%20on%20unmasked%20set/runs/anuo2l1w\" target=\"_blank\">https://wandb.ai/fsu-dsc-cil/PASTEL%20single%20task%20on%20unmasked%20set/runs/anuo2l1w</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/scratch/data_jz17d/wandb_tmp/wandb/run-20220923_174714-anuo2l1w/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867848a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
