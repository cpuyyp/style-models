{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b71f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pcgrad import PCGrad\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import collections\n",
    "import json\n",
    "from itertools import cycle\n",
    "from ast import literal_eval\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import Any, Callable, Dict, List, NewType, Optional, Tuple, Union\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler\n",
    "from torch import optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torchmetrics\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "from transformers import *\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput, ModelOutput\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0eb3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['WANDB_NOTEBOOK_NAME'] = 'PASTEL all together with PCgrad.ipynb'\n",
    "import wandb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92b87bc",
   "metadata": {},
   "source": [
    "# definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a499f05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_folder = os.environ[\"scratch_result_folder\"] if \"scratch_result_folder\" in os.environ else '../result'\n",
    "scratch_data_folder = os.environ[\"scratch_data_folder\"] if \"scratch_data_folder\" in os.environ else None\n",
    "repo_folder = os.environ[\"style_models_repo_folder\"] if \"style_models_repo_folder\" in os.environ else None\n",
    "data_folder = f\"{repo_folder}/data\" if repo_folder else '../../data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334ab2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# https://github.com/huggingface/transformers/issues/5486\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9edc6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary: task_name: number_of_labels\n",
    "with open(f'{data_folder}/pastel/pastel_tasks2labels.json', 'r') as f:\n",
    "    tasks2labels = json.load(f)\n",
    "# Dictionary: task_name: task index\n",
    "tasks2idx = {k:i for i,k in enumerate(tasks2labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b042d536",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class MyTrainingArgs:\n",
    "    # training args\n",
    "    selected_tasks: List\n",
    "    base_model_name: str \n",
    "    freeze_bert: bool\n",
    "    use_pooler: bool\n",
    "    num_epoch: int\n",
    "    lr: float = 5e-5\n",
    "    num_warmup_steps = 500\n",
    "    warmup_ratio = 0.1\n",
    "    model_folder: str = None # if None, this will be inferred based on tasks\n",
    "    model_name: str = None # if provide, use to name model_folder, otherwise use style to name model_folder\n",
    "        \n",
    "    # data loader args\n",
    "    batch_size: int = 32\n",
    "    max_length: int = 64\n",
    "    shuffle: bool = False\n",
    "    num_workers: int = 4\n",
    "    data_limit: int = None # if not None, truncate dataset to keep only top {data_limit} rows\n",
    "    \n",
    "    # post training args\n",
    "    save_best: bool = True\n",
    "    load_best_at_end: bool = True\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        excute_time = datetime.now() \n",
    "        model_name = self.model_name if self.model_name else '+'.join(self.selected_tasks)\n",
    "        model_folder = f\"{result_folder}/{model_name}/{excute_time.now().strftime('%Y%m%d-%H:%M:%S')}\"\n",
    "        self.model_folder = model_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1338e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset): \n",
    "    # currently it's a Mapping-style dataset. Not sure if a Iterable-style dataset will be better\n",
    "    # this works for standard class indices and also class probilities\n",
    "    # limit: use to truncate dataset. This will drop rows after certain index. May influence label distribution.\n",
    "    def __init__(self, training_args, split, label_prefix = None):\n",
    "        self.tasks = training_args.selected_tasks\n",
    "        self.max_length = training_args.max_length\n",
    "        self.split = split\n",
    "        self.label_prefix = label_prefix\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(training_args.base_model_name)\n",
    "        self.df = pd.read_csv(f\"{data_folder}/pastel/processed/{self.split}/{self.tasks[0] if len(self.tasks)==1 else 'pastel'}.csv\")\n",
    "        self.df = self.df.dropna()\n",
    "        self.df = self.df.reset_index(drop=True)\n",
    "        \n",
    "        # for distill model, logits that written to files need eval to be correctly recognized\n",
    "        # also apply softmax on logits\n",
    "        for task in self.tasks:\n",
    "            if self.label_prefix is not None:\n",
    "                task = self.label_prefix + task\n",
    "            if isinstance(self.df[task][0], str):\n",
    "                self.df[task] = torch.tensor(self.df[task].apply(literal_eval)).softmax(dim=1).numpy().tolist()\n",
    "\n",
    "        if training_args.data_limit:\n",
    "            self.df = self.df.iloc[:training_args.data_limit]\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        dataslice = self.df.iloc[idx]\n",
    "#         item = {k: v for k, v in self.tokenizer(dataslice['output.sentences'], truncation=True, padding=True, max_length=self.max_length).items()}\n",
    "        item = {'text':dataslice['output.sentences']}\n",
    "        item.update({task: dataslice[task] if self.label_prefix is None else dataslice[self.label_prefix+task] for task in self.tasks}) \n",
    "        return item\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee72916",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionHead(nn.Module):\n",
    "    def __init__(self, embedding_dim = 768, hidden_dim = 128):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.hidden = nn.Linear(embedding_dim, 1)\n",
    "        \n",
    "        self.loss_fn = nn.MSELoss()\n",
    "    def forward(self, sent_emb, label):\n",
    "        batchsize = sent_emb.shape[0]\n",
    "        output = self.hidden(self.dropout(sent_emb)).squeeze(1)\n",
    "\n",
    "        loss = self.loss_fn(output, label.view(batchsize, -1).squeeze(-1))\n",
    "        return output, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9294042e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationHead(nn.Module):\n",
    "    def __init__(self, num_labels, embedding_dim = 768, hidden_dim = 128):\n",
    "        super().__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.hidden = nn.Linear(embedding_dim, self.num_labels)\n",
    "        \n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "    def forward(self, sent_emb, label):\n",
    "        batchsize = sent_emb.shape[0]\n",
    "        output = self.hidden(self.dropout(sent_emb))\n",
    "        \n",
    "        loss = self.loss_fn(output.view(-1, self.num_labels), label.view(batchsize, -1).squeeze(-1))\n",
    "        return output, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e911dc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class MultiTaskOutput(ModelOutput):\n",
    "    total_loss: torch.FloatTensor = None\n",
    "    losses: List[torch.FloatTensor] = None\n",
    "    sent_emb: torch.FloatTensor = None\n",
    "    all_logits: Optional[Dict[str, torch.FloatTensor]] = None\n",
    "    bert_hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    bert_attentions: Optional[Tuple[torch.FloatTensor]] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25c4ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskBert(PreTrainedModel):\n",
    "    def __init__(self, config, training_args):\n",
    "        super().__init__(config)\n",
    "#         self.training_args = training_args\n",
    "        self.tasks = training_args.selected_tasks\n",
    "        self.use_pooler = training_args.use_pooler\n",
    "        self.basemodel = AutoModel.from_pretrained(training_args.base_model_name)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(training_args.base_model_name)\n",
    "        self.style_heads = nn.ModuleList()\n",
    "        \n",
    "        for task in self.tasks:\n",
    "            if tasks2labels[task] == 1:\n",
    "                self.style_heads.append(RegressionHead())\n",
    "            else:\n",
    "                self.style_heads.append(ClassificationHead(tasks2labels[task]))\n",
    "                \n",
    "    def forward(self, input_ids, token_type_ids, attention_mask, return_logits=False, return_sent_emb=True, **kwargs):\n",
    "        output = self.basemodel(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)\n",
    "        if self.use_pooler and ('pooler_output' in output):\n",
    "            sent_emb = output['pooler_output']\n",
    "        else:\n",
    "            sent_emb = output['last_hidden_state'][:,0,:]\n",
    "        \n",
    "        total_loss = None\n",
    "        losses = []\n",
    "        all_logits = None\n",
    "        if return_logits:\n",
    "            all_logits = {}\n",
    "        all_logits = {}\n",
    "        for task in kwargs:\n",
    "            i_task = self.tasks.index(task)\n",
    "            logits, loss = self.style_heads[i_task](sent_emb, kwargs[task]) \n",
    "            losses.append(loss)\n",
    "            if total_loss is None:\n",
    "                total_loss = loss\n",
    "            else:\n",
    "                total_loss += loss\n",
    "            if return_logits:\n",
    "                all_logits[task] = logits.detach()\n",
    "        return MultiTaskOutput(total_loss=total_loss, losses=losses, sent_emb=sent_emb, all_logits=all_logits, bert_hidden_states=output.hidden_states, bert_attentions=output.attentions)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9b2c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(training_args):\n",
    "    config = AutoConfig.from_pretrained(training_args.base_model_name) \n",
    "    model = MultiTaskBert(config, training_args).to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0cbf9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_model(model, freeze_bert):\n",
    "    '''\n",
    "    if freeze_bert == True, freeze all layer. \n",
    "    if freeze_bert is a positive integer, freeze the bottom {freeze_bert} attention layers\n",
    "    negative integer should also work\n",
    "    '''\n",
    "    if freeze_bert==True:\n",
    "        for param in model.basemodel.parameters():\n",
    "            param.requires_grad = False\n",
    "    elif isinstance(freeze_bert, (int, np.int32, np.int64, torch.int32, torch.int64)):\n",
    "        for param in model.basemodel.embeddings.parameters():\n",
    "            param.requires_grad = False  \n",
    "        for layer in model.basemodel.encoder.layer[:freeze_bert]: \n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = False  \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2028b7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_best_model(model, model_folder):\n",
    "    model.load_state_dict(torch.load(f\"{model_folder}/pytorch_model.bin\"))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628d0e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_eval(model, cycle_valid_loader, num_valid_steps):\n",
    "    model.eval()\n",
    "    tasks = model.tasks\n",
    "    task_f1s = {task: torchmetrics.F1Score(num_classes=tasks2labels[task], average='macro') for task in tasks}\n",
    "    \n",
    "    for i_step in trange(num_valid_steps, leave=False):\n",
    "        batch = next(cycle_valid_loader)\n",
    "        output = model(**batch, return_logits=True)\n",
    "        logits = output['all_logits']\n",
    "\n",
    "        for task in tasks:\n",
    "            task_f1s[task].update(logits[task].detach().cpu().argmax(-1), batch[task].detach().cpu())\n",
    "    evaluation = {'f1_'+task: task_f1s[task].compute().item() for task in tasks}\n",
    "    evaluation.update({'f1_avg':np.mean(list(evaluation.values()))})\n",
    "    return evaluation\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f1e176",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    batch_out = collections.defaultdict(list)\n",
    "    for item in batch:\n",
    "        for col in item:\n",
    "            batch_out[col].append(item[col])\n",
    "    for col in batch_out:\n",
    "        if col != 'text':\n",
    "            batch_out[col] = torch.tensor(batch_out[col], dtype=torch.int64).to(device)\n",
    "            \n",
    "    batch_out.update({k:v for k,v in tokenizer(text = batch_out['text'], return_tensors='pt', padding=True, truncation=True, max_length=64).to(device).items()})\n",
    "    \n",
    "    del batch_out['text']\n",
    "    return batch_out\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a2b011",
   "metadata": {},
   "source": [
    "# training starts here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20bb556",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf66ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_training_args = MyTrainingArgs(selected_tasks=list(tasks2labels.keys()),\n",
    "                                  model_name=f'PASTEL all together with PCgrad',\n",
    "                                  base_model_name='bert-base-uncased',\n",
    "                                  freeze_bert=False,\n",
    "                                  use_pooler=False,\n",
    "                                  num_epoch=10,\n",
    "                                  batch_size=64,\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c414915",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcpuyyp\u001b[0m (\u001b[33mfsu-dsc-cil\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch/data_jz17d/wandb_tmp/wandb/run-20221004_185412-s9gmf57k</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/fsu-dsc-cil/PASTEL%20all%20together%20with%20PCgrad/runs/s9gmf57k\" target=\"_blank\">run 0</a></strong> to <a href=\"https://wandb.ai/fsu-dsc-cil/PASTEL%20all%20together%20with%20PCgrad\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: logging graph, to disable use `wandb.watch(log_graph=False)`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "646b379070d24a7da98934615a245e86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4970 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_196956/1353421580.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'losses'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpc_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mpbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/style-models/code/PASTEL/pcgrad.py\u001b[0m in \u001b[0;36mpc_backward\u001b[0;34m(self, objectives)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshapes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_grads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pack_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjectives\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mpc_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_project_conflicting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0mpc_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unflatten_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpc_grad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpc_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/style-models/code/PASTEL/pcgrad.py\u001b[0m in \u001b[0;36m_project_conflicting\u001b[0;34m(self, grads, has_grads, shapes)\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mg_j\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m                 \u001b[0mg_i_g_j\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_j\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mg_i_g_j\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m                     \u001b[0mg_i\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mg_i_g_j\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mg_j\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mg_j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mmerged_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "batch_size = 64\n",
    "save_best_only = True # if not save the best, save the last\n",
    "\n",
    "# some parameters to sweep\n",
    "LR = [3e-5, 5e-5, 7e-5]\n",
    "FREEZE_BERT = [9, 11]\n",
    "WARMUP_RATIO = [0.1, 0.15, 0.2]\n",
    "\n",
    "LR, FREEZE_BERT, WARMUP_RATIO = np.meshgrid(LR,FREEZE_BERT, WARMUP_RATIO)\n",
    "LR, FREEZE_BERT, WARMUP_RATIO = LR.flatten(), FREEZE_BERT.flatten(), WARMUP_RATIO.flatten()\n",
    "num_runs = len(LR)\n",
    "\n",
    "# create dataset\n",
    "train_data = MyDataset(my_training_args, 'train')\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "num_training_steps = len(train_loader)\n",
    "cycle_train_loader = cycle(iter(train_loader))\n",
    "\n",
    "valid_data = MyDataset(my_training_args, 'valid')\n",
    "valid_loader = DataLoader(valid_data, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "num_valid_steps = len(valid_loader)\n",
    "cycle_valid_loader = cycle(iter(valid_loader))\n",
    "\n",
    "# start runs\n",
    "for i_run in range(num_runs):\n",
    "    model_folder = f\"{result_folder}/pastel_pcgrad/run_{i_run}\"\n",
    "    Path(model_folder).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    lr = LR[i_run]\n",
    "    freeze_bert = FREEZE_BERT[i_run]\n",
    "    warmup_ratio = WARMUP_RATIO[i_run]\n",
    "    \n",
    "    my_training_args.lr = lr\n",
    "    my_training_args.freeze_bert = freeze_bert\n",
    "    my_training_args.warmup_ratio = warmup_ratio\n",
    "    \n",
    "    # use wandb to track experiments\n",
    "    wconfig = {}\n",
    "    wconfig['lr'] = lr\n",
    "    wconfig['freeze_bert'] = freeze_bert\n",
    "    wconfig['warmup_ratio'] = warmup_ratio\n",
    "\n",
    "    run = wandb.init(project=\"PASTEL all together with PCgrad\", \n",
    "                     entity=\"fsu-dsc-cil\", \n",
    "                     dir='/scratch/data_jz17d/wandb_tmp/', \n",
    "                     config=wconfig,\n",
    "                     name=f'run {i_run}',\n",
    "                     reinit=True)\n",
    "\n",
    "    model = init_model(my_training_args)\n",
    "    model = freeze_model(model, freeze_bert)\n",
    "    \n",
    "    wandb.watch(model, log=\"all\", log_freq=1000, log_graph=True)\n",
    "\n",
    "    optimizer = torch.optim.Adam([p for p in model.parameters() if p.requires_grad==True], lr=lr)\n",
    "    optimizer = PCGrad(optimizer)\n",
    "\n",
    "    scheduler = get_scheduler(\"linear\",\n",
    "                            optimizer=optimizer._optim,\n",
    "                            num_warmup_steps=int(warmup_ratio*num_epochs*num_training_steps),\n",
    "                            num_training_steps=num_epochs*num_training_steps)\n",
    "\n",
    "    # start training and logging\n",
    "    best_metric = 0.0\n",
    "    df = pd.DataFrame(columns=['global_step'])\n",
    "    pbar = trange(num_epochs*num_training_steps)\n",
    "    for i_epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for i_step in range(num_training_steps):\n",
    "\n",
    "            batch = next(cycle_train_loader)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(**batch)\n",
    "            losses = output['losses']\n",
    "            optimizer.pc_backward(losses)\n",
    "            optimizer.step()\n",
    "            pbar.update(1)\n",
    "\n",
    "        model.eval()\n",
    "        evaluation = do_eval(model, cycle_valid_loader, num_valid_steps)\n",
    "        wandb.log(evaluation, step=pbar.n)\n",
    "        evaluation.update({'global_step':pbar.n})\n",
    "        df = df.append(evaluation, ignore_index=True)\n",
    "        \n",
    "        # save best model\n",
    "        if save_best_only and (best_metric < evaluation['f1_avg']):\n",
    "            best_metric = evaluation['f1_avg']\n",
    "            torch.save(model.state_dict(), f\"{model_folder}/pytorch_model.bin\")\n",
    "            torch.save(optimizer._optim.state_dict(), f\"{model_folder}/optimizer.pt\")\n",
    "            torch.save(scheduler.state_dict(), f\"{model_folder}/scheduler.pt\")\n",
    "    \n",
    "    # if not save best, save the last\n",
    "    if not save_best_only:\n",
    "        torch.save(model.state_dict(), f\"{model_folder}/pytorch_model.bin\")\n",
    "        torch.save(optimizer._optim.state_dict(), f\"{model_folder}/optimizer.pt\")\n",
    "        torch.save(scheduler.state_dict(), f\"{model_folder}/scheduler.pt\")\n",
    "    with open(f\"{model_folder}/training_args.json\", \"w\") as outfile:\n",
    "        json.dump(asdict(my_training_args), outfile)\n",
    "    df.to_csv(f\"{model_folder}/evaluation.csv\", index=False)\n",
    "    run.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa431578",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f1_country': 0.015550041571259499,\n",
       " 'f1_politics': 0.2742529511451721,\n",
       " 'f1_tod': 0.09492255747318268,\n",
       " 'f1_age': 0.07916268706321716,\n",
       " 'f1_education': 0.06269937008619308,\n",
       " 'f1_ethnic': 0.01124968659132719,\n",
       " 'f1_gender': 0.06537345051765442,\n",
       " 'f1_avg': 0.08617296349257231,\n",
       " 'global_step': 1988}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03aa9124",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
