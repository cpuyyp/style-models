{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ff23f80",
   "metadata": {},
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#train-each-task-and-get-prediction\" data-toc-modified-id=\"train-each-task-and-get-prediction-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>train each task and get prediction</a></span><ul class=\"toc-item\"><li><span><a href=\"#country\" data-toc-modified-id=\"country-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>country</a></span></li><li><span><a href=\"#politics\" data-toc-modified-id=\"politics-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>politics</a></span></li><li><span><a href=\"#tod\" data-toc-modified-id=\"tod-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>tod</a></span></li><li><span><a href=\"#age\" data-toc-modified-id=\"age-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>age</a></span></li><li><span><a href=\"#education\" data-toc-modified-id=\"education-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>education</a></span></li><li><span><a href=\"#ethnic\" data-toc-modified-id=\"ethnic-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>ethnic</a></span></li><li><span><a href=\"#gender\" data-toc-modified-id=\"gender-1.7\"><span class=\"toc-item-num\">1.7&nbsp;&nbsp;</span>gender</a></span></li></ul></li><li><span><a href=\"#combine-all-prediction\" data-toc-modified-id=\"combine-all-prediction-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>combine all prediction</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc709ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./PASTEL_MTL_training_utils.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b2a946",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_logits_labels(trainer, dataset, task_name):\n",
    "    pred = trainer.predict(dataset)\n",
    "    df = dataset.df.copy()\n",
    "    logits = pred.predictions[task_name]\n",
    "    labels = logits.argmax(-1)\n",
    "    df['logits'] = logits.tolist()\n",
    "    df[f'predicted_{task_name}'] = labels.tolist()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2eb7e8",
   "metadata": {},
   "source": [
    "# train each task and get prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9af5bf2",
   "metadata": {},
   "source": [
    "## country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9b8025",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "***** Running training *****\n",
      "  Num examples = 6385\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 08:51, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy Country</th>\n",
       "      <th>F1 Country</th>\n",
       "      <th>Precision Country</th>\n",
       "      <th>Recall Country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.068700</td>\n",
       "      <td>0.118699</td>\n",
       "      <td>0.977892</td>\n",
       "      <td>0.329608</td>\n",
       "      <td>0.325964</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.068700</td>\n",
       "      <td>0.105295</td>\n",
       "      <td>0.977892</td>\n",
       "      <td>0.329608</td>\n",
       "      <td>0.325964</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.167100</td>\n",
       "      <td>0.135593</td>\n",
       "      <td>0.973172</td>\n",
       "      <td>0.341269</td>\n",
       "      <td>0.355773</td>\n",
       "      <td>0.339443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.167100</td>\n",
       "      <td>0.205761</td>\n",
       "      <td>0.977853</td>\n",
       "      <td>0.333198</td>\n",
       "      <td>0.468859</td>\n",
       "      <td>0.335101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.028000</td>\n",
       "      <td>0.208294</td>\n",
       "      <td>0.975571</td>\n",
       "      <td>0.344883</td>\n",
       "      <td>0.382315</td>\n",
       "      <td>0.341449</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 25421\n",
      "  Batch size = 32\n",
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task country/20220920-14:51:28/checkpoint-200\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task country/20220920-14:51:28/checkpoint-200/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task country/20220920-14:51:28/checkpoint-200/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task country/20220920-14:51:28/checkpoint-200/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task country/20220920-14:51:28/checkpoint-200/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25421\n",
      "  Batch size = 32\n",
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task country/20220920-14:51:28/checkpoint-400\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task country/20220920-14:51:28/checkpoint-400/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task country/20220920-14:51:28/checkpoint-400/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task country/20220920-14:51:28/checkpoint-400/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task country/20220920-14:51:28/checkpoint-400/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/PASTEL single task country/20220920-14:51:28/checkpoint-200] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25421\n",
      "  Batch size = 32\n",
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task country/20220920-14:51:28/checkpoint-600\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task country/20220920-14:51:28/checkpoint-600/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task country/20220920-14:51:28/checkpoint-600/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task country/20220920-14:51:28/checkpoint-600/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task country/20220920-14:51:28/checkpoint-600/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25421\n",
      "  Batch size = 32\n",
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task country/20220920-14:51:28/checkpoint-800\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task country/20220920-14:51:28/checkpoint-800/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task country/20220920-14:51:28/checkpoint-800/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task country/20220920-14:51:28/checkpoint-800/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task country/20220920-14:51:28/checkpoint-800/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/PASTEL single task country/20220920-14:51:28/checkpoint-600] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25421\n",
      "  Batch size = 32\n",
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task country/20220920-14:51:28/checkpoint-1000\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task country/20220920-14:51:28/checkpoint-1000/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task country/20220920-14:51:28/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task country/20220920-14:51:28/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task country/20220920-14:51:28/checkpoint-1000/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/PASTEL single task country/20220920-14:51:28/checkpoint-800] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /scratch/data_jz17d/result/PASTEL single task country/20220920-14:51:28/checkpoint-400 (score: 0.10529539734125137).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 6385\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='995' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [200/200 01:09]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "***** Running Prediction *****\n",
      "  Num examples = 25421\n",
      "  Batch size = 32\n",
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "################# settings #################\n",
    "selected_tasks = ['country']\n",
    "p=0.8\n",
    "############################################\n",
    "# build args\n",
    "my_training_args = MyTrainingArgs(selected_tasks=selected_tasks,\n",
    "                                  model_name=f'PASTEL single task {selected_tasks[0]}',\n",
    "                                  base_model_name='bert-base-uncased',\n",
    "                                  freeze_bert=False,\n",
    "                                  use_pooler=False,\n",
    "                                  num_epoch=5,\n",
    "#                                   data_limit=30000,\n",
    "                                 )\n",
    "hg_training_args = TrainingArguments(\n",
    "    output_dir=my_training_args.model_folder,   # output directory\n",
    "    num_train_epochs=my_training_args.num_epoch,     # total number of training epochs\n",
    "    per_device_train_batch_size=my_training_args.batch_size,  # batch size per device during training\n",
    "    per_device_eval_batch_size=my_training_args.batch_size,   # batch size for evaluation\n",
    "    warmup_steps=my_training_args.num_warmup_steps,    # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir=f\"{my_training_args.model_folder}/logs\",  # directory for storing logs\n",
    "    logging_first_step = True, \n",
    "    evaluation_strategy=\"epoch\",     # either 'epoch' or 'steps'\n",
    "    save_strategy = 'epoch',\n",
    "    save_total_limit = 1,\n",
    "    metric_for_best_model='eval_loss',\n",
    "    greater_is_better=False,\n",
    "    load_best_model_at_end=True,  # by default decide on loss, change it by setting metric_for_best_model\n",
    ")\n",
    "\n",
    "# initialize model and dataset\n",
    "model = init_model(my_training_args)\n",
    "freeze_model(model, my_training_args.freeze_bert)\n",
    "\n",
    "train_dataset = MyDataset(my_training_args, f'p={p}_unmasked_train')\n",
    "val_dataset = MyDataset(my_training_args, f'p={p}_masked_train')\n",
    "\n",
    "trainer = MyTrainer(\n",
    "    model=model,   # the instantiated Transformers model to be trained\n",
    "    args=hg_training_args,                  # training arguments, defined above\n",
    "    tokenizer=model.tokenizer, \n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=val_dataset,          # evaluation dataset\n",
    "    compute_metrics=compute_metrics,     # the callback that computes metrics of interest\n",
    ")\n",
    "\n",
    "# train\n",
    "trainer.train()\n",
    "\n",
    "# get prediction and save to file\n",
    "df_train = predict_logits_labels(trainer, train_dataset, selected_tasks[0])\n",
    "df_val = predict_logits_labels(trainer, val_dataset, selected_tasks[0])\n",
    "df_combined = pd.concat([df_train, df_val], axis = 0)\n",
    "df_combined = df_combined.sort_values('Unnamed: 0')\n",
    "\n",
    "output_path = f'{data_folder}/pastel/processed/p={p}_predicted'\n",
    "Path(output_path).mkdir(parents=True, exist_ok=True)\n",
    "df_combined.to_csv(f'{output_path}/{selected_tasks[0]}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63233031",
   "metadata": {},
   "source": [
    "## politics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a2d847",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/jz17d/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/jz17d/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/jz17d/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/jz17d/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/jz17d/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/jz17d/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/jz17d/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/jz17d/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/jz17d/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/jz17d/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "***** Running training *****\n",
      "  Num examples = 6250\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 980\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='980' max='980' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [980/980 08:45, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy Politics</th>\n",
       "      <th>F1 Politics</th>\n",
       "      <th>Precision Politics</th>\n",
       "      <th>Recall Politics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.162300</td>\n",
       "      <td>0.963141</td>\n",
       "      <td>0.487909</td>\n",
       "      <td>0.353017</td>\n",
       "      <td>0.324977</td>\n",
       "      <td>0.386710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.162300</td>\n",
       "      <td>0.965067</td>\n",
       "      <td>0.492839</td>\n",
       "      <td>0.369040</td>\n",
       "      <td>0.425839</td>\n",
       "      <td>0.394754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.955800</td>\n",
       "      <td>1.056180</td>\n",
       "      <td>0.490139</td>\n",
       "      <td>0.417108</td>\n",
       "      <td>0.435062</td>\n",
       "      <td>0.418692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.955800</td>\n",
       "      <td>1.507034</td>\n",
       "      <td>0.471514</td>\n",
       "      <td>0.424575</td>\n",
       "      <td>0.428055</td>\n",
       "      <td>0.427600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.955800</td>\n",
       "      <td>2.013652</td>\n",
       "      <td>0.472844</td>\n",
       "      <td>0.426788</td>\n",
       "      <td>0.427024</td>\n",
       "      <td>0.426900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 25556\n",
      "  Batch size = 32\n",
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task politics/20220920-15:01:41/checkpoint-196\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task politics/20220920-15:01:41/checkpoint-196/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task politics/20220920-15:01:41/checkpoint-196/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task politics/20220920-15:01:41/checkpoint-196/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task politics/20220920-15:01:41/checkpoint-196/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25556\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task politics/20220920-15:01:41/checkpoint-392\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task politics/20220920-15:01:41/checkpoint-392/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task politics/20220920-15:01:41/checkpoint-392/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task politics/20220920-15:01:41/checkpoint-392/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task politics/20220920-15:01:41/checkpoint-392/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25556\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task politics/20220920-15:01:41/checkpoint-588\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task politics/20220920-15:01:41/checkpoint-588/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task politics/20220920-15:01:41/checkpoint-588/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task politics/20220920-15:01:41/checkpoint-588/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task politics/20220920-15:01:41/checkpoint-588/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/PASTEL single task politics/20220920-15:01:41/checkpoint-392] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25556\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task politics/20220920-15:01:41/checkpoint-784\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task politics/20220920-15:01:41/checkpoint-784/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task politics/20220920-15:01:41/checkpoint-784/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task politics/20220920-15:01:41/checkpoint-784/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task politics/20220920-15:01:41/checkpoint-784/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/PASTEL single task politics/20220920-15:01:41/checkpoint-588] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25556\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task politics/20220920-15:01:41/checkpoint-980\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task politics/20220920-15:01:41/checkpoint-980/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task politics/20220920-15:01:41/checkpoint-980/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task politics/20220920-15:01:41/checkpoint-980/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task politics/20220920-15:01:41/checkpoint-980/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/PASTEL single task politics/20220920-15:01:41/checkpoint-784] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /scratch/data_jz17d/result/PASTEL single task politics/20220920-15:01:41/checkpoint-196 (score: 0.9631405472755432).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 6250\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='995' max='196' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [196/196 01:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "***** Running Prediction *****\n",
      "  Num examples = 25556\n",
      "  Batch size = 32\n",
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "################# settings #################\n",
    "selected_tasks = ['politics']\n",
    "p=0.8\n",
    "############################################\n",
    "# build args\n",
    "my_training_args = MyTrainingArgs(selected_tasks=selected_tasks,\n",
    "                                  model_name=f'PASTEL single task {selected_tasks[0]}',\n",
    "                                  base_model_name='bert-base-uncased',\n",
    "                                  freeze_bert=False,\n",
    "                                  use_pooler=False,\n",
    "                                  num_epoch=5,\n",
    "#                                   data_limit=30000,\n",
    "                                 )\n",
    "hg_training_args = TrainingArguments(\n",
    "    output_dir=my_training_args.model_folder,   # output directory\n",
    "    num_train_epochs=my_training_args.num_epoch,     # total number of training epochs\n",
    "    per_device_train_batch_size=my_training_args.batch_size,  # batch size per device during training\n",
    "    per_device_eval_batch_size=my_training_args.batch_size,   # batch size for evaluation\n",
    "    warmup_steps=my_training_args.num_warmup_steps,    # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir=f\"{my_training_args.model_folder}/logs\",  # directory for storing logs\n",
    "    logging_first_step = True, \n",
    "    evaluation_strategy=\"epoch\",     # either 'epoch' or 'steps'\n",
    "    save_strategy = 'epoch',\n",
    "    save_total_limit = 1,\n",
    "    metric_for_best_model='eval_loss',\n",
    "    greater_is_better=False,\n",
    "    load_best_model_at_end=True,  # by default decide on loss, change it by setting metric_for_best_model\n",
    ")\n",
    "\n",
    "# initialize model and dataset\n",
    "model = init_model(my_training_args)\n",
    "freeze_model(model, my_training_args.freeze_bert)\n",
    "\n",
    "train_dataset = MyDataset(my_training_args, f'p={p}_unmasked_train')\n",
    "val_dataset = MyDataset(my_training_args, f'p={p}_masked_train')\n",
    "\n",
    "trainer = MyTrainer(\n",
    "    model=model,   # the instantiated Transformers model to be trained\n",
    "    args=hg_training_args,                  # training arguments, defined above\n",
    "    tokenizer=model.tokenizer, \n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=val_dataset,          # evaluation dataset\n",
    "    compute_metrics=compute_metrics,     # the callback that computes metrics of interest\n",
    ")\n",
    "\n",
    "# train\n",
    "trainer.train()\n",
    "\n",
    "# get prediction and save to file\n",
    "df_train = predict_logits_labels(trainer, train_dataset, selected_tasks[0])\n",
    "df_val = predict_logits_labels(trainer, val_dataset, selected_tasks[0])\n",
    "df_combined = pd.concat([df_train, df_val], axis = 0)\n",
    "df_combined = df_combined.sort_values('Unnamed: 0')\n",
    "\n",
    "output_path = f'{data_folder}/pastel/processed/p={p}_predicted'\n",
    "Path(output_path).mkdir(parents=True, exist_ok=True)\n",
    "df_combined.to_csv(f'{output_path}/{selected_tasks[0]}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc0327e",
   "metadata": {},
   "source": [
    "## tod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf5e302",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/jz17d/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/jz17d/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/jz17d/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/jz17d/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/jz17d/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/jz17d/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/jz17d/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/jz17d/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/jz17d/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/jz17d/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "***** Running training *****\n",
      "  Num examples = 6371\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 08:50, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy Tod</th>\n",
       "      <th>F1 Tod</th>\n",
       "      <th>Precision Tod</th>\n",
       "      <th>Recall Tod</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.655200</td>\n",
       "      <td>1.351164</td>\n",
       "      <td>0.450285</td>\n",
       "      <td>0.210755</td>\n",
       "      <td>0.252591</td>\n",
       "      <td>0.257068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.655200</td>\n",
       "      <td>1.353979</td>\n",
       "      <td>0.455711</td>\n",
       "      <td>0.215211</td>\n",
       "      <td>0.315895</td>\n",
       "      <td>0.260798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.344300</td>\n",
       "      <td>1.416900</td>\n",
       "      <td>0.412070</td>\n",
       "      <td>0.230261</td>\n",
       "      <td>0.282637</td>\n",
       "      <td>0.255093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.344300</td>\n",
       "      <td>1.758377</td>\n",
       "      <td>0.362925</td>\n",
       "      <td>0.252186</td>\n",
       "      <td>0.266998</td>\n",
       "      <td>0.263765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.697700</td>\n",
       "      <td>2.179619</td>\n",
       "      <td>0.379792</td>\n",
       "      <td>0.266598</td>\n",
       "      <td>0.272248</td>\n",
       "      <td>0.270368</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 25435\n",
      "  Batch size = 32\n",
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task tod/20220920-15:11:39/checkpoint-200\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task tod/20220920-15:11:39/checkpoint-200/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task tod/20220920-15:11:39/checkpoint-200/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task tod/20220920-15:11:39/checkpoint-200/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task tod/20220920-15:11:39/checkpoint-200/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25435\n",
      "  Batch size = 32\n",
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task tod/20220920-15:11:39/checkpoint-400\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task tod/20220920-15:11:39/checkpoint-400/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task tod/20220920-15:11:39/checkpoint-400/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task tod/20220920-15:11:39/checkpoint-400/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task tod/20220920-15:11:39/checkpoint-400/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25435\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task tod/20220920-15:11:39/checkpoint-600\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task tod/20220920-15:11:39/checkpoint-600/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task tod/20220920-15:11:39/checkpoint-600/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task tod/20220920-15:11:39/checkpoint-600/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task tod/20220920-15:11:39/checkpoint-600/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/PASTEL single task tod/20220920-15:11:39/checkpoint-400] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25435\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task tod/20220920-15:11:39/checkpoint-800\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task tod/20220920-15:11:39/checkpoint-800/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task tod/20220920-15:11:39/checkpoint-800/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task tod/20220920-15:11:39/checkpoint-800/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task tod/20220920-15:11:39/checkpoint-800/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/PASTEL single task tod/20220920-15:11:39/checkpoint-600] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25435\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task tod/20220920-15:11:39/checkpoint-1000\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task tod/20220920-15:11:39/checkpoint-1000/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task tod/20220920-15:11:39/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task tod/20220920-15:11:39/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task tod/20220920-15:11:39/checkpoint-1000/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/PASTEL single task tod/20220920-15:11:39/checkpoint-800] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /scratch/data_jz17d/result/PASTEL single task tod/20220920-15:11:39/checkpoint-200 (score: 1.3511638641357422).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 6371\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='995' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [200/200 01:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "***** Running Prediction *****\n",
      "  Num examples = 25435\n",
      "  Batch size = 32\n",
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "################# settings #################\n",
    "selected_tasks = ['tod']\n",
    "p=0.8\n",
    "############################################\n",
    "# build args\n",
    "my_training_args = MyTrainingArgs(selected_tasks=selected_tasks,\n",
    "                                  model_name=f'PASTEL single task {selected_tasks[0]}',\n",
    "                                  base_model_name='bert-base-uncased',\n",
    "                                  freeze_bert=False,\n",
    "                                  use_pooler=False,\n",
    "                                  num_epoch=5,\n",
    "#                                   data_limit=30000,\n",
    "                                 )\n",
    "hg_training_args = TrainingArguments(\n",
    "    output_dir=my_training_args.model_folder,   # output directory\n",
    "    num_train_epochs=my_training_args.num_epoch,     # total number of training epochs\n",
    "    per_device_train_batch_size=my_training_args.batch_size,  # batch size per device during training\n",
    "    per_device_eval_batch_size=my_training_args.batch_size,   # batch size for evaluation\n",
    "    warmup_steps=my_training_args.num_warmup_steps,    # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir=f\"{my_training_args.model_folder}/logs\",  # directory for storing logs\n",
    "    logging_first_step = True, \n",
    "    evaluation_strategy=\"epoch\",     # either 'epoch' or 'steps'\n",
    "    save_strategy = 'epoch',\n",
    "    save_total_limit = 1,\n",
    "    metric_for_best_model='eval_loss',\n",
    "    greater_is_better=False,\n",
    "    load_best_model_at_end=True,  # by default decide on loss, change it by setting metric_for_best_model\n",
    ")\n",
    "\n",
    "# initialize model and dataset\n",
    "model = init_model(my_training_args)\n",
    "freeze_model(model, my_training_args.freeze_bert)\n",
    "\n",
    "train_dataset = MyDataset(my_training_args, f'p={p}_unmasked_train')\n",
    "val_dataset = MyDataset(my_training_args, f'p={p}_masked_train')\n",
    "\n",
    "trainer = MyTrainer(\n",
    "    model=model,   # the instantiated Transformers model to be trained\n",
    "    args=hg_training_args,                  # training arguments, defined above\n",
    "    tokenizer=model.tokenizer, \n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=val_dataset,          # evaluation dataset\n",
    "    compute_metrics=compute_metrics,     # the callback that computes metrics of interest\n",
    ")\n",
    "\n",
    "# train\n",
    "trainer.train()\n",
    "\n",
    "# get prediction and save to file\n",
    "df_train = predict_logits_labels(trainer, train_dataset, selected_tasks[0])\n",
    "df_val = predict_logits_labels(trainer, val_dataset, selected_tasks[0])\n",
    "df_combined = pd.concat([df_train, df_val], axis = 0)\n",
    "df_combined = df_combined.sort_values('Unnamed: 0')\n",
    "\n",
    "output_path = f'{data_folder}/pastel/processed/p={p}_predicted'\n",
    "Path(output_path).mkdir(parents=True, exist_ok=True)\n",
    "df_combined.to_csv(f'{output_path}/{selected_tasks[0]}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39917462",
   "metadata": {},
   "source": [
    "## age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368ec959",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/jz17d/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/jz17d/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/jz17d/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/jz17d/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/jz17d/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/jz17d/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/jz17d/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/jz17d/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/jz17d/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/jz17d/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "***** Running training *****\n",
      "  Num examples = 6365\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 995\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='995' max='995' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [995/995 09:05, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy Age</th>\n",
       "      <th>F1 Age</th>\n",
       "      <th>Precision Age</th>\n",
       "      <th>Recall Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.033000</td>\n",
       "      <td>1.458330</td>\n",
       "      <td>0.400888</td>\n",
       "      <td>0.082059</td>\n",
       "      <td>0.274071</td>\n",
       "      <td>0.128819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.033000</td>\n",
       "      <td>1.381173</td>\n",
       "      <td>0.422782</td>\n",
       "      <td>0.162077</td>\n",
       "      <td>0.247864</td>\n",
       "      <td>0.171336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.443000</td>\n",
       "      <td>1.470824</td>\n",
       "      <td>0.410990</td>\n",
       "      <td>0.177771</td>\n",
       "      <td>0.232187</td>\n",
       "      <td>0.187566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.443000</td>\n",
       "      <td>1.752889</td>\n",
       "      <td>0.378916</td>\n",
       "      <td>0.197750</td>\n",
       "      <td>0.199823</td>\n",
       "      <td>0.201005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.443000</td>\n",
       "      <td>2.148510</td>\n",
       "      <td>0.386306</td>\n",
       "      <td>0.194791</td>\n",
       "      <td>0.196219</td>\n",
       "      <td>0.194474</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 25441\n",
      "  Batch size = 32\n",
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task age/20220920-15:21:44/checkpoint-199\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task age/20220920-15:21:44/checkpoint-199/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task age/20220920-15:21:44/checkpoint-199/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task age/20220920-15:21:44/checkpoint-199/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task age/20220920-15:21:44/checkpoint-199/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25441\n",
      "  Batch size = 32\n",
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task age/20220920-15:21:44/checkpoint-398\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task age/20220920-15:21:44/checkpoint-398/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task age/20220920-15:21:44/checkpoint-398/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task age/20220920-15:21:44/checkpoint-398/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task age/20220920-15:21:44/checkpoint-398/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/PASTEL single task age/20220920-15:21:44/checkpoint-199] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25441\n",
      "  Batch size = 32\n",
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task age/20220920-15:21:44/checkpoint-597\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task age/20220920-15:21:44/checkpoint-597/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task age/20220920-15:21:44/checkpoint-597/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task age/20220920-15:21:44/checkpoint-597/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task age/20220920-15:21:44/checkpoint-597/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25441\n",
      "  Batch size = 32\n",
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task age/20220920-15:21:44/checkpoint-796\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task age/20220920-15:21:44/checkpoint-796/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task age/20220920-15:21:44/checkpoint-796/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task age/20220920-15:21:44/checkpoint-796/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task age/20220920-15:21:44/checkpoint-796/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/PASTEL single task age/20220920-15:21:44/checkpoint-597] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25441\n",
      "  Batch size = 32\n",
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task age/20220920-15:21:44/checkpoint-995\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task age/20220920-15:21:44/checkpoint-995/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task age/20220920-15:21:44/checkpoint-995/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task age/20220920-15:21:44/checkpoint-995/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task age/20220920-15:21:44/checkpoint-995/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/PASTEL single task age/20220920-15:21:44/checkpoint-796] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /scratch/data_jz17d/result/PASTEL single task age/20220920-15:21:44/checkpoint-398 (score: 1.38117253780365).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 6365\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='995' max='199' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [199/199 01:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "***** Running Prediction *****\n",
      "  Num examples = 25441\n",
      "  Batch size = 32\n",
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "################# settings #################\n",
    "selected_tasks = ['age']\n",
    "p=0.8\n",
    "############################################\n",
    "# build args\n",
    "my_training_args = MyTrainingArgs(selected_tasks=selected_tasks,\n",
    "                                  model_name=f'PASTEL single task {selected_tasks[0]}',\n",
    "                                  base_model_name='bert-base-uncased',\n",
    "                                  freeze_bert=False,\n",
    "                                  use_pooler=False,\n",
    "                                  num_epoch=5,\n",
    "#                                   data_limit=30000,\n",
    "                                 )\n",
    "hg_training_args = TrainingArguments(\n",
    "    output_dir=my_training_args.model_folder,   # output directory\n",
    "    num_train_epochs=my_training_args.num_epoch,     # total number of training epochs\n",
    "    per_device_train_batch_size=my_training_args.batch_size,  # batch size per device during training\n",
    "    per_device_eval_batch_size=my_training_args.batch_size,   # batch size for evaluation\n",
    "    warmup_steps=my_training_args.num_warmup_steps,    # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir=f\"{my_training_args.model_folder}/logs\",  # directory for storing logs\n",
    "    logging_first_step = True, \n",
    "    evaluation_strategy=\"epoch\",     # either 'epoch' or 'steps'\n",
    "    save_strategy = 'epoch',\n",
    "    save_total_limit = 1,\n",
    "    metric_for_best_model='eval_loss',\n",
    "    greater_is_better=False,\n",
    "    load_best_model_at_end=True,  # by default decide on loss, change it by setting metric_for_best_model\n",
    ")\n",
    "\n",
    "# initialize model and dataset\n",
    "model = init_model(my_training_args)\n",
    "freeze_model(model, my_training_args.freeze_bert)\n",
    "\n",
    "train_dataset = MyDataset(my_training_args, f'p={p}_unmasked_train')\n",
    "val_dataset = MyDataset(my_training_args, f'p={p}_masked_train')\n",
    "\n",
    "trainer = MyTrainer(\n",
    "    model=model,   # the instantiated Transformers model to be trained\n",
    "    args=hg_training_args,                  # training arguments, defined above\n",
    "    tokenizer=model.tokenizer, \n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=val_dataset,          # evaluation dataset\n",
    "    compute_metrics=compute_metrics,     # the callback that computes metrics of interest\n",
    ")\n",
    "\n",
    "# train\n",
    "trainer.train()\n",
    "\n",
    "# get prediction and save to file\n",
    "df_train = predict_logits_labels(trainer, train_dataset, selected_tasks[0])\n",
    "df_val = predict_logits_labels(trainer, val_dataset, selected_tasks[0])\n",
    "df_combined = pd.concat([df_train, df_val], axis = 0)\n",
    "df_combined = df_combined.sort_values('Unnamed: 0')\n",
    "\n",
    "output_path = f'{data_folder}/pastel/processed/p={p}_predicted'\n",
    "Path(output_path).mkdir(parents=True, exist_ok=True)\n",
    "df_combined.to_csv(f'{output_path}/{selected_tasks[0]}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05adc197",
   "metadata": {},
   "source": [
    "## education"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c522203",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/jz17d/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/jz17d/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/jz17d/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/jz17d/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/jz17d/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/jz17d/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/jz17d/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/jz17d/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/jz17d/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/jz17d/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "***** Running training *****\n",
      "  Num examples = 6314\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 990\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='990' max='990' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [990/990 08:57, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy Education</th>\n",
       "      <th>F1 Education</th>\n",
       "      <th>Precision Education</th>\n",
       "      <th>Recall Education</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.312100</td>\n",
       "      <td>1.603246</td>\n",
       "      <td>0.398713</td>\n",
       "      <td>0.143512</td>\n",
       "      <td>0.200389</td>\n",
       "      <td>0.165608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.312100</td>\n",
       "      <td>1.614885</td>\n",
       "      <td>0.358936</td>\n",
       "      <td>0.184647</td>\n",
       "      <td>0.245151</td>\n",
       "      <td>0.186866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.669500</td>\n",
       "      <td>1.614504</td>\n",
       "      <td>0.391692</td>\n",
       "      <td>0.189858</td>\n",
       "      <td>0.215372</td>\n",
       "      <td>0.189362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.669500</td>\n",
       "      <td>1.897781</td>\n",
       "      <td>0.383767</td>\n",
       "      <td>0.201917</td>\n",
       "      <td>0.221484</td>\n",
       "      <td>0.197592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.669500</td>\n",
       "      <td>2.293798</td>\n",
       "      <td>0.367606</td>\n",
       "      <td>0.198302</td>\n",
       "      <td>0.249895</td>\n",
       "      <td>0.196994</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 25492\n",
      "  Batch size = 32\n",
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task education/20220920-15:32:04/checkpoint-198\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task education/20220920-15:32:04/checkpoint-198/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task education/20220920-15:32:04/checkpoint-198/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task education/20220920-15:32:04/checkpoint-198/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task education/20220920-15:32:04/checkpoint-198/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25492\n",
      "  Batch size = 32\n",
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task education/20220920-15:32:04/checkpoint-396\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task education/20220920-15:32:04/checkpoint-396/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task education/20220920-15:32:04/checkpoint-396/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task education/20220920-15:32:04/checkpoint-396/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task education/20220920-15:32:04/checkpoint-396/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25492\n",
      "  Batch size = 32\n",
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task education/20220920-15:32:04/checkpoint-594\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task education/20220920-15:32:04/checkpoint-594/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task education/20220920-15:32:04/checkpoint-594/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task education/20220920-15:32:04/checkpoint-594/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task education/20220920-15:32:04/checkpoint-594/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/PASTEL single task education/20220920-15:32:04/checkpoint-396] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25492\n",
      "  Batch size = 32\n",
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task education/20220920-15:32:04/checkpoint-792\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task education/20220920-15:32:04/checkpoint-792/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task education/20220920-15:32:04/checkpoint-792/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task education/20220920-15:32:04/checkpoint-792/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task education/20220920-15:32:04/checkpoint-792/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/PASTEL single task education/20220920-15:32:04/checkpoint-594] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25492\n",
      "  Batch size = 32\n",
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task education/20220920-15:32:04/checkpoint-990\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task education/20220920-15:32:04/checkpoint-990/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task education/20220920-15:32:04/checkpoint-990/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task education/20220920-15:32:04/checkpoint-990/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task education/20220920-15:32:04/checkpoint-990/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/PASTEL single task education/20220920-15:32:04/checkpoint-792] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /scratch/data_jz17d/result/PASTEL single task education/20220920-15:32:04/checkpoint-198 (score: 1.603245735168457).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 6314\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='995' max='198' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [198/198 01:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "***** Running Prediction *****\n",
      "  Num examples = 25492\n",
      "  Batch size = 32\n",
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "################# settings #################\n",
    "selected_tasks = ['education']\n",
    "p=0.8\n",
    "############################################\n",
    "# build args\n",
    "my_training_args = MyTrainingArgs(selected_tasks=selected_tasks,\n",
    "                                  model_name=f'PASTEL single task {selected_tasks[0]}',\n",
    "                                  base_model_name='bert-base-uncased',\n",
    "                                  freeze_bert=False,\n",
    "                                  use_pooler=False,\n",
    "                                  num_epoch=5,\n",
    "#                                   data_limit=30000,\n",
    "                                 )\n",
    "hg_training_args = TrainingArguments(\n",
    "    output_dir=my_training_args.model_folder,   # output directory\n",
    "    num_train_epochs=my_training_args.num_epoch,     # total number of training epochs\n",
    "    per_device_train_batch_size=my_training_args.batch_size,  # batch size per device during training\n",
    "    per_device_eval_batch_size=my_training_args.batch_size,   # batch size for evaluation\n",
    "    warmup_steps=my_training_args.num_warmup_steps,    # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir=f\"{my_training_args.model_folder}/logs\",  # directory for storing logs\n",
    "    logging_first_step = True, \n",
    "    evaluation_strategy=\"epoch\",     # either 'epoch' or 'steps'\n",
    "    save_strategy = 'epoch',\n",
    "    save_total_limit = 1,\n",
    "    metric_for_best_model='eval_loss',\n",
    "    greater_is_better=False,\n",
    "    load_best_model_at_end=True,  # by default decide on loss, change it by setting metric_for_best_model\n",
    ")\n",
    "\n",
    "# initialize model and dataset\n",
    "model = init_model(my_training_args)\n",
    "freeze_model(model, my_training_args.freeze_bert)\n",
    "\n",
    "train_dataset = MyDataset(my_training_args, f'p={p}_unmasked_train')\n",
    "val_dataset = MyDataset(my_training_args, f'p={p}_masked_train')\n",
    "\n",
    "trainer = MyTrainer(\n",
    "    model=model,   # the instantiated Transformers model to be trained\n",
    "    args=hg_training_args,                  # training arguments, defined above\n",
    "    tokenizer=model.tokenizer, \n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=val_dataset,          # evaluation dataset\n",
    "    compute_metrics=compute_metrics,     # the callback that computes metrics of interest\n",
    ")\n",
    "\n",
    "# train\n",
    "trainer.train()\n",
    "\n",
    "# get prediction and save to file\n",
    "df_train = predict_logits_labels(trainer, train_dataset, selected_tasks[0])\n",
    "df_val = predict_logits_labels(trainer, val_dataset, selected_tasks[0])\n",
    "df_combined = pd.concat([df_train, df_val], axis = 0)\n",
    "df_combined = df_combined.sort_values('Unnamed: 0')\n",
    "\n",
    "output_path = f'{data_folder}/pastel/processed/p={p}_predicted'\n",
    "Path(output_path).mkdir(parents=True, exist_ok=True)\n",
    "df_combined.to_csv(f'{output_path}/{selected_tasks[0]}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d65792",
   "metadata": {},
   "source": [
    "## ethnic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d400a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/jz17d/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/jz17d/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/jz17d/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/jz17d/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/jz17d/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/jz17d/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/jz17d/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/jz17d/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/jz17d/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/jz17d/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "***** Running training *****\n",
      "  Num examples = 6346\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 995\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcpuyyp\u001b[0m (\u001b[33mfsu-dsc-cil\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jz17d/Desktop/style-models/code/PASTEL/wandb/run-20220920_154244-1imzjgsq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/fsu-dsc-cil/huggingface/runs/1imzjgsq\" target=\"_blank\">/scratch/data_jz17d/result/PASTEL single task ethnic/20220920-15:42:19</a></strong> to <a href=\"https://wandb.ai/fsu-dsc-cil/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='995' max='995' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [995/995 09:04, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy Ethnic</th>\n",
       "      <th>F1 Ethnic</th>\n",
       "      <th>Precision Ethnic</th>\n",
       "      <th>Recall Ethnic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.223800</td>\n",
       "      <td>0.735223</td>\n",
       "      <td>0.823056</td>\n",
       "      <td>0.180968</td>\n",
       "      <td>0.257832</td>\n",
       "      <td>0.190352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.223800</td>\n",
       "      <td>0.733628</td>\n",
       "      <td>0.825962</td>\n",
       "      <td>0.193853</td>\n",
       "      <td>0.288427</td>\n",
       "      <td>0.194008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.834200</td>\n",
       "      <td>0.757316</td>\n",
       "      <td>0.823527</td>\n",
       "      <td>0.219020</td>\n",
       "      <td>0.326797</td>\n",
       "      <td>0.213115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.834200</td>\n",
       "      <td>0.959900</td>\n",
       "      <td>0.822427</td>\n",
       "      <td>0.216823</td>\n",
       "      <td>0.296929</td>\n",
       "      <td>0.209658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.834200</td>\n",
       "      <td>1.111066</td>\n",
       "      <td>0.809034</td>\n",
       "      <td>0.228458</td>\n",
       "      <td>0.277332</td>\n",
       "      <td>0.215817</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 25460\n",
      "  Batch size = 32\n",
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task ethnic/20220920-15:42:19/checkpoint-199\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task ethnic/20220920-15:42:19/checkpoint-199/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task ethnic/20220920-15:42:19/checkpoint-199/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task ethnic/20220920-15:42:19/checkpoint-199/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task ethnic/20220920-15:42:19/checkpoint-199/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25460\n",
      "  Batch size = 32\n",
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task ethnic/20220920-15:42:19/checkpoint-398\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task ethnic/20220920-15:42:19/checkpoint-398/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task ethnic/20220920-15:42:19/checkpoint-398/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task ethnic/20220920-15:42:19/checkpoint-398/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task ethnic/20220920-15:42:19/checkpoint-398/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/PASTEL single task ethnic/20220920-15:42:19/checkpoint-199] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25460\n",
      "  Batch size = 32\n",
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task ethnic/20220920-15:42:19/checkpoint-597\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task ethnic/20220920-15:42:19/checkpoint-597/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task ethnic/20220920-15:42:19/checkpoint-597/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task ethnic/20220920-15:42:19/checkpoint-597/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task ethnic/20220920-15:42:19/checkpoint-597/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25460\n",
      "  Batch size = 32\n",
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task ethnic/20220920-15:42:19/checkpoint-796\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task ethnic/20220920-15:42:19/checkpoint-796/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task ethnic/20220920-15:42:19/checkpoint-796/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task ethnic/20220920-15:42:19/checkpoint-796/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task ethnic/20220920-15:42:19/checkpoint-796/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/PASTEL single task ethnic/20220920-15:42:19/checkpoint-597] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25460\n",
      "  Batch size = 32\n",
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task ethnic/20220920-15:42:19/checkpoint-995\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task ethnic/20220920-15:42:19/checkpoint-995/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task ethnic/20220920-15:42:19/checkpoint-995/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task ethnic/20220920-15:42:19/checkpoint-995/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task ethnic/20220920-15:42:19/checkpoint-995/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/PASTEL single task ethnic/20220920-15:42:19/checkpoint-796] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /scratch/data_jz17d/result/PASTEL single task ethnic/20220920-15:42:19/checkpoint-398 (score: 0.7336282730102539).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 6346\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='995' max='199' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [199/199 01:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "***** Running Prediction *****\n",
      "  Num examples = 25460\n",
      "  Batch size = 32\n",
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "################# settings #################\n",
    "selected_tasks = ['ethnic']\n",
    "p=0.8\n",
    "############################################\n",
    "# build args\n",
    "my_training_args = MyTrainingArgs(selected_tasks=selected_tasks,\n",
    "                                  model_name=f'PASTEL single task {selected_tasks[0]}',\n",
    "                                  base_model_name='bert-base-uncased',\n",
    "                                  freeze_bert=False,\n",
    "                                  use_pooler=False,\n",
    "                                  num_epoch=5,\n",
    "#                                   data_limit=30000,\n",
    "                                 )\n",
    "hg_training_args = TrainingArguments(\n",
    "    output_dir=my_training_args.model_folder,   # output directory\n",
    "    num_train_epochs=my_training_args.num_epoch,     # total number of training epochs\n",
    "    per_device_train_batch_size=my_training_args.batch_size,  # batch size per device during training\n",
    "    per_device_eval_batch_size=my_training_args.batch_size,   # batch size for evaluation\n",
    "    warmup_steps=my_training_args.num_warmup_steps,    # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir=f\"{my_training_args.model_folder}/logs\",  # directory for storing logs\n",
    "    logging_first_step = True, \n",
    "    evaluation_strategy=\"epoch\",     # either 'epoch' or 'steps'\n",
    "    save_strategy = 'epoch',\n",
    "    save_total_limit = 1,\n",
    "    metric_for_best_model='eval_loss',\n",
    "    greater_is_better=False,\n",
    "    load_best_model_at_end=True,  # by default decide on loss, change it by setting metric_for_best_model\n",
    ")\n",
    "\n",
    "# initialize model and dataset\n",
    "model = init_model(my_training_args)\n",
    "freeze_model(model, my_training_args.freeze_bert)\n",
    "\n",
    "train_dataset = MyDataset(my_training_args, f'p={p}_unmasked_train')\n",
    "val_dataset = MyDataset(my_training_args, f'p={p}_masked_train')\n",
    "\n",
    "trainer = MyTrainer(\n",
    "    model=model,   # the instantiated Transformers model to be trained\n",
    "    args=hg_training_args,                  # training arguments, defined above\n",
    "    tokenizer=model.tokenizer, \n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=val_dataset,          # evaluation dataset\n",
    "    compute_metrics=compute_metrics,     # the callback that computes metrics of interest\n",
    ")\n",
    "\n",
    "# train\n",
    "trainer.train()\n",
    "\n",
    "# get prediction and save to file\n",
    "df_train = predict_logits_labels(trainer, train_dataset, selected_tasks[0])\n",
    "df_val = predict_logits_labels(trainer, val_dataset, selected_tasks[0])\n",
    "df_combined = pd.concat([df_train, df_val], axis = 0)\n",
    "df_combined = df_combined.sort_values('Unnamed: 0')\n",
    "\n",
    "output_path = f'{data_folder}/pastel/processed/p={p}_predicted'\n",
    "Path(output_path).mkdir(parents=True, exist_ok=True)\n",
    "df_combined.to_csv(f'{output_path}/{selected_tasks[0]}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc0c332",
   "metadata": {},
   "source": [
    "## gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f7d385",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/jz17d/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/jz17d/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/jz17d/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/jz17d/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/jz17d/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/jz17d/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/jz17d/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/jz17d/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/jz17d/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/jz17d/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "***** Running training *****\n",
      "  Num examples = 6465\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1015\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1015' max='1015' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1015/1015 09:12, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy Gender</th>\n",
       "      <th>F1 Gender</th>\n",
       "      <th>Precision Gender</th>\n",
       "      <th>Recall Gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.276900</td>\n",
       "      <td>0.598843</td>\n",
       "      <td>0.725465</td>\n",
       "      <td>0.429509</td>\n",
       "      <td>0.508646</td>\n",
       "      <td>0.430294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.276900</td>\n",
       "      <td>0.606125</td>\n",
       "      <td>0.724912</td>\n",
       "      <td>0.448950</td>\n",
       "      <td>0.478217</td>\n",
       "      <td>0.445337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.619500</td>\n",
       "      <td>0.662696</td>\n",
       "      <td>0.736909</td>\n",
       "      <td>0.449024</td>\n",
       "      <td>0.503531</td>\n",
       "      <td>0.445597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.619500</td>\n",
       "      <td>0.878510</td>\n",
       "      <td>0.699696</td>\n",
       "      <td>0.449117</td>\n",
       "      <td>0.782673</td>\n",
       "      <td>0.446014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.239700</td>\n",
       "      <td>1.347769</td>\n",
       "      <td>0.706207</td>\n",
       "      <td>0.446347</td>\n",
       "      <td>0.454946</td>\n",
       "      <td>0.444143</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 25341\n",
      "  Batch size = 32\n",
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task gender/20220920-15:53:03/checkpoint-203\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task gender/20220920-15:53:03/checkpoint-203/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task gender/20220920-15:53:03/checkpoint-203/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task gender/20220920-15:53:03/checkpoint-203/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task gender/20220920-15:53:03/checkpoint-203/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25341\n",
      "  Batch size = 32\n",
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task gender/20220920-15:53:03/checkpoint-406\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task gender/20220920-15:53:03/checkpoint-406/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task gender/20220920-15:53:03/checkpoint-406/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task gender/20220920-15:53:03/checkpoint-406/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task gender/20220920-15:53:03/checkpoint-406/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25341\n",
      "  Batch size = 32\n",
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task gender/20220920-15:53:03/checkpoint-609\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task gender/20220920-15:53:03/checkpoint-609/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task gender/20220920-15:53:03/checkpoint-609/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task gender/20220920-15:53:03/checkpoint-609/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task gender/20220920-15:53:03/checkpoint-609/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/PASTEL single task gender/20220920-15:53:03/checkpoint-406] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25341\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task gender/20220920-15:53:03/checkpoint-812\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task gender/20220920-15:53:03/checkpoint-812/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task gender/20220920-15:53:03/checkpoint-812/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task gender/20220920-15:53:03/checkpoint-812/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task gender/20220920-15:53:03/checkpoint-812/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/PASTEL single task gender/20220920-15:53:03/checkpoint-609] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25341\n",
      "  Batch size = 32\n",
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to /scratch/data_jz17d/result/PASTEL single task gender/20220920-15:53:03/checkpoint-1015\n",
      "Configuration saved in /scratch/data_jz17d/result/PASTEL single task gender/20220920-15:53:03/checkpoint-1015/config.json\n",
      "Model weights saved in /scratch/data_jz17d/result/PASTEL single task gender/20220920-15:53:03/checkpoint-1015/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/data_jz17d/result/PASTEL single task gender/20220920-15:53:03/checkpoint-1015/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/data_jz17d/result/PASTEL single task gender/20220920-15:53:03/checkpoint-1015/special_tokens_map.json\n",
      "Deleting older checkpoint [/scratch/data_jz17d/result/PASTEL single task gender/20220920-15:53:03/checkpoint-812] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /scratch/data_jz17d/result/PASTEL single task gender/20220920-15:53:03/checkpoint-203 (score: 0.5988427996635437).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 6465\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='995' max='203' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [203/203 01:11]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "***** Running Prediction *****\n",
      "  Num examples = 25341\n",
      "  Batch size = 32\n",
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "################# settings #################\n",
    "selected_tasks = ['gender']\n",
    "p=0.8\n",
    "############################################\n",
    "# build args\n",
    "my_training_args = MyTrainingArgs(selected_tasks=selected_tasks,\n",
    "                                  model_name=f'PASTEL single task {selected_tasks[0]}',\n",
    "                                  base_model_name='bert-base-uncased',\n",
    "                                  freeze_bert=False,\n",
    "                                  use_pooler=False,\n",
    "                                  num_epoch=5,\n",
    "#                                   data_limit=30000,\n",
    "                                 )\n",
    "hg_training_args = TrainingArguments(\n",
    "    output_dir=my_training_args.model_folder,   # output directory\n",
    "    num_train_epochs=my_training_args.num_epoch,     # total number of training epochs\n",
    "    per_device_train_batch_size=my_training_args.batch_size,  # batch size per device during training\n",
    "    per_device_eval_batch_size=my_training_args.batch_size,   # batch size for evaluation\n",
    "    warmup_steps=my_training_args.num_warmup_steps,    # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir=f\"{my_training_args.model_folder}/logs\",  # directory for storing logs\n",
    "    logging_first_step = True, \n",
    "    evaluation_strategy=\"epoch\",     # either 'epoch' or 'steps'\n",
    "    save_strategy = 'epoch',\n",
    "    save_total_limit = 1,\n",
    "    metric_for_best_model='eval_loss',\n",
    "    greater_is_better=False,\n",
    "    load_best_model_at_end=True,  # by default decide on loss, change it by setting metric_for_best_model\n",
    ")\n",
    "\n",
    "# initialize model and dataset\n",
    "model = init_model(my_training_args)\n",
    "freeze_model(model, my_training_args.freeze_bert)\n",
    "\n",
    "train_dataset = MyDataset(my_training_args, f'p={p}_unmasked_train')\n",
    "val_dataset = MyDataset(my_training_args, f'p={p}_masked_train')\n",
    "\n",
    "trainer = MyTrainer(\n",
    "    model=model,   # the instantiated Transformers model to be trained\n",
    "    args=hg_training_args,                  # training arguments, defined above\n",
    "    tokenizer=model.tokenizer, \n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=val_dataset,          # evaluation dataset\n",
    "    compute_metrics=compute_metrics,     # the callback that computes metrics of interest\n",
    ")\n",
    "\n",
    "# train\n",
    "trainer.train()\n",
    "\n",
    "# get prediction and save to file\n",
    "df_train = predict_logits_labels(trainer, train_dataset, selected_tasks[0])\n",
    "df_val = predict_logits_labels(trainer, val_dataset, selected_tasks[0])\n",
    "df_combined = pd.concat([df_train, df_val], axis = 0)\n",
    "df_combined = df_combined.sort_values('Unnamed: 0')\n",
    "\n",
    "output_path = f'{data_folder}/pastel/processed/p={p}_predicted'\n",
    "Path(output_path).mkdir(parents=True, exist_ok=True)\n",
    "df_combined.to_csv(f'{output_path}/{selected_tasks[0]}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052eb701",
   "metadata": {},
   "source": [
    "# combine all prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f3f12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "p=0.8\n",
    "df_combined = None\n",
    "for task in tasks2labels:\n",
    "    df = pd.read_csv(f'{data_folder}/pastel/processed/p={p}_predicted/{task}.csv')\n",
    "    df = df.rename({'logits': f'logits_{task}'}, axis=1)\n",
    "    if df_combined is None:\n",
    "        df_combined = df\n",
    "    else:\n",
    "        df_combined[task] = df[task]\n",
    "        df_combined[f'logits_{task}'] = df[f'logits_{task}']\n",
    "        df_combined[f'predicted_{task}'] = df[f'predicted_{task}']\n",
    "del df_combined['Unnamed: 0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432db17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined.to_csv(f'{data_folder}/pastel/processed/p={p}_predicted/pastel.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8115373",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>output.sentences</th>\n",
       "      <th>country</th>\n",
       "      <th>logits_country</th>\n",
       "      <th>predicted_country</th>\n",
       "      <th>politics</th>\n",
       "      <th>logits_politics</th>\n",
       "      <th>predicted_politics</th>\n",
       "      <th>tod</th>\n",
       "      <th>logits_tod</th>\n",
       "      <th>predicted_tod</th>\n",
       "      <th>...</th>\n",
       "      <th>predicted_age</th>\n",
       "      <th>education</th>\n",
       "      <th>logits_education</th>\n",
       "      <th>predicted_education</th>\n",
       "      <th>ethnic</th>\n",
       "      <th>logits_ethnic</th>\n",
       "      <th>predicted_ethnic</th>\n",
       "      <th>gender</th>\n",
       "      <th>logits_gender</th>\n",
       "      <th>predicted_gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>He was welcomed and got comfortable very soon</td>\n",
       "      <td>2</td>\n",
       "      <td>[-3.824354410171509, -0.38507792353630066, 3.6...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.24422624707221985, 0.3063298761844635, -0.7...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.21492363512516022, -0.7563967108726501, -0....</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>[2.252073287963867, -1.6805291175842285, 1.095...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.7484460473060608, 4.646738529205322, 0.5143...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[1.5935319662094116, 0.9003511071205139, -2.92...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The person went into the really cute bakery.</td>\n",
       "      <td>2</td>\n",
       "      <td>[-3.9543936252593994, -0.7944300174713135, 4.0...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.4435342252254486, 0.02745579555630684, -0.5...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.8440676927566528, -0.5200585722923279, -0.9...</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[2.4144856929779053, -1.5705946683883667, 0.80...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[1.861870288848877, 4.701307773590088, 0.44426...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[1.9701415300369263, 0.7289540767669678, -2.61...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The woodwinds were particularly great at this ...</td>\n",
       "      <td>2</td>\n",
       "      <td>[-3.8537561893463135, -1.017175316810608, 4.28...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.13127337396144867, 0.29768651723861694, -0...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.3296774923801422, -0.1384560465812683, -0.9...</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>[2.0642471313476562, -1.2309235334396362, 0.73...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.6460480690002441, 4.356095314025879, 0.3705...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[1.630432367324829, 0.5803238749504089, -2.468...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>We saw a cute cat in the window.</td>\n",
       "      <td>2</td>\n",
       "      <td>[-2.963761806488037, 0.06145167350769043, 2.72...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.08225657790899277, -0.026504602283239365, -...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.6835461258888245, -0.5731146931648254, -0.8...</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[2.3273043632507324, -1.320784330368042, 0.806...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[1.3187897205352783, 4.804645538330078, 0.6664...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[1.7107672691345215, 0.5609513521194458, -2.49...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We gathered in a crowded room to celebrate the...</td>\n",
       "      <td>2</td>\n",
       "      <td>[-3.5162606239318848, -0.11993153393268585, 3....</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.15331576764583588, 0.19601358473300934, -0....</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.42055004835128784, -0.32928168773651123, -0...</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>[2.127380847930908, -1.3450191020965576, 0.965...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[1.3346798419952393, 4.704219341278076, 1.0185...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[1.7862259149551392, 0.5271328687667847, -2.20...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31801</th>\n",
       "      <td>I'm always pushing myself to be my last stats.</td>\n",
       "      <td>2</td>\n",
       "      <td>[-3.2892401218414307, -0.13976924121379852, 3....</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.37375155091285706, 0.007765654474496841, -0...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.9421898126602173, -0.4137086272239685, -0.7...</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>[2.1349356174468994, -1.5594700574874878, 0.97...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.9925805330276489, 4.336263179779053, 0.1395...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[2.0534555912017822, 0.6668630242347717, -2.59...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31802</th>\n",
       "      <td>You could tell they had a lot of good impressi...</td>\n",
       "      <td>2</td>\n",
       "      <td>[-3.5324552059173584, -0.8788880705833435, 3.6...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.07646099478006363, 0.16322483122348785, -0....</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>[0.449198454618454, -0.41510307788848877, -0.7...</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>[1.908264398574829, -1.2622565031051636, 1.073...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[1.2664387226104736, 3.647049903869629, -0.026...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[1.618477463722229, 0.5838104486465454, -2.376...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31803</th>\n",
       "      <td>He joined 4 other cars at the starting line, r...</td>\n",
       "      <td>2</td>\n",
       "      <td>[-3.493647813796997, -0.3894329071044922, 3.42...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.20726430416107178, 0.7898072004318237, -0.8...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.5863224267959595, -0.3077000379562378, -0.6...</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>[2.2139570713043213, -1.13866126537323, 0.5666...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[1.4163975715637207, 4.123951435089111, 0.6112...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[1.825916051864624, 0.5409999489784241, -2.128...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31804</th>\n",
       "      <td>I pass by some pumpkins on somebody's porch.</td>\n",
       "      <td>2</td>\n",
       "      <td>[-3.0623326301574707, 0.24642455577850342, 2.6...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.44953158497810364, -0.1441139131784439, -0....</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>[0.903001070022583, -0.642055332660675, -0.909...</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>[2.1015565395355225, -1.4762376546859741, 0.86...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[1.3337857723236084, 4.608504772186279, 0.2960...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[2.02620530128479, 0.37937286496162415, -2.429...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31805</th>\n",
       "      <td>The dog was ready to go.</td>\n",
       "      <td>2</td>\n",
       "      <td>[-3.987136125564575, -1.4081742763519287, 4.46...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>[-1.2837852239608765, 3.4327356815338135, -1.0...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>[-0.5025452971458435, -0.7334327101707458, 0.8...</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>[4.131035804748535, -1.3376038074493408, 0.299...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.4904952943325043, 5.872151851654053, 0.3358...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[1.5236738920211792, 0.586313009262085, -2.377...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31806 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        output.sentences  country  \\\n",
       "0          He was welcomed and got comfortable very soon        2   \n",
       "1           The person went into the really cute bakery.        2   \n",
       "2      The woodwinds were particularly great at this ...        2   \n",
       "3                       We saw a cute cat in the window.        2   \n",
       "4      We gathered in a crowded room to celebrate the...        2   \n",
       "...                                                  ...      ...   \n",
       "31801     I'm always pushing myself to be my last stats.        2   \n",
       "31802  You could tell they had a lot of good impressi...        2   \n",
       "31803  He joined 4 other cars at the starting line, r...        2   \n",
       "31804       I pass by some pumpkins on somebody's porch.        2   \n",
       "31805                           The dog was ready to go.        2   \n",
       "\n",
       "                                          logits_country  predicted_country  \\\n",
       "0      [-3.824354410171509, -0.38507792353630066, 3.6...                  2   \n",
       "1      [-3.9543936252593994, -0.7944300174713135, 4.0...                  2   \n",
       "2      [-3.8537561893463135, -1.017175316810608, 4.28...                  2   \n",
       "3      [-2.963761806488037, 0.06145167350769043, 2.72...                  2   \n",
       "4      [-3.5162606239318848, -0.11993153393268585, 3....                  2   \n",
       "...                                                  ...                ...   \n",
       "31801  [-3.2892401218414307, -0.13976924121379852, 3....                  2   \n",
       "31802  [-3.5324552059173584, -0.8788880705833435, 3.6...                  2   \n",
       "31803  [-3.493647813796997, -0.3894329071044922, 3.42...                  2   \n",
       "31804  [-3.0623326301574707, 0.24642455577850342, 2.6...                  2   \n",
       "31805  [-3.987136125564575, -1.4081742763519287, 4.46...                  2   \n",
       "\n",
       "       politics                                    logits_politics  \\\n",
       "0             0  [0.24422624707221985, 0.3063298761844635, -0.7...   \n",
       "1             0  [0.4435342252254486, 0.02745579555630684, -0.5...   \n",
       "2             0  [-0.13127337396144867, 0.29768651723861694, -0...   \n",
       "3             1  [0.08225657790899277, -0.026504602283239365, -...   \n",
       "4             1  [0.15331576764583588, 0.19601358473300934, -0....   \n",
       "...         ...                                                ...   \n",
       "31801         2  [0.37375155091285706, 0.007765654474496841, -0...   \n",
       "31802         2  [0.07646099478006363, 0.16322483122348785, -0....   \n",
       "31803         0  [0.20726430416107178, 0.7898072004318237, -0.8...   \n",
       "31804         0  [0.44953158497810364, -0.1441139131784439, -0....   \n",
       "31805         1  [-1.2837852239608765, 3.4327356815338135, -1.0...   \n",
       "\n",
       "       predicted_politics  tod  \\\n",
       "0                       1    0   \n",
       "1                       0    0   \n",
       "2                       1    3   \n",
       "3                       0    0   \n",
       "4                       1    0   \n",
       "...                   ...  ...   \n",
       "31801                   0    0   \n",
       "31802                   1    4   \n",
       "31803                   1    0   \n",
       "31804                   0    4   \n",
       "31805                   1    3   \n",
       "\n",
       "                                              logits_tod  predicted_tod  ...  \\\n",
       "0      [0.21492363512516022, -0.7563967108726501, -0....              3  ...   \n",
       "1      [0.8440676927566528, -0.5200585722923279, -0.9...              0  ...   \n",
       "2      [0.3296774923801422, -0.1384560465812683, -0.9...              0  ...   \n",
       "3      [0.6835461258888245, -0.5731146931648254, -0.8...              0  ...   \n",
       "4      [0.42055004835128784, -0.32928168773651123, -0...              0  ...   \n",
       "...                                                  ...            ...  ...   \n",
       "31801  [0.9421898126602173, -0.4137086272239685, -0.7...              0  ...   \n",
       "31802  [0.449198454618454, -0.41510307788848877, -0.7...              0  ...   \n",
       "31803  [0.5863224267959595, -0.3077000379562378, -0.6...              0  ...   \n",
       "31804  [0.903001070022583, -0.642055332660675, -0.909...              0  ...   \n",
       "31805  [-0.5025452971458435, -0.7334327101707458, 0.8...              3  ...   \n",
       "\n",
       "       predicted_age education  \\\n",
       "0                  2         5   \n",
       "1                  1         0   \n",
       "2                  2         0   \n",
       "3                  2         2   \n",
       "4                  2         0   \n",
       "...              ...       ...   \n",
       "31801              1         9   \n",
       "31802              2         5   \n",
       "31803              2         4   \n",
       "31804              2         3   \n",
       "31805              2         0   \n",
       "\n",
       "                                        logits_education  predicted_education  \\\n",
       "0      [2.252073287963867, -1.6805291175842285, 1.095...                    0   \n",
       "1      [2.4144856929779053, -1.5705946683883667, 0.80...                    0   \n",
       "2      [2.0642471313476562, -1.2309235334396362, 0.73...                    0   \n",
       "3      [2.3273043632507324, -1.320784330368042, 0.806...                    0   \n",
       "4      [2.127380847930908, -1.3450191020965576, 0.965...                    0   \n",
       "...                                                  ...                  ...   \n",
       "31801  [2.1349356174468994, -1.5594700574874878, 0.97...                    0   \n",
       "31802  [1.908264398574829, -1.2622565031051636, 1.073...                    0   \n",
       "31803  [2.2139570713043213, -1.13866126537323, 0.5666...                    0   \n",
       "31804  [2.1015565395355225, -1.4762376546859741, 0.86...                    0   \n",
       "31805  [4.131035804748535, -1.3376038074493408, 0.299...                    0   \n",
       "\n",
       "      ethnic                                      logits_ethnic  \\\n",
       "0          1  [0.7484460473060608, 4.646738529205322, 0.5143...   \n",
       "1          1  [1.861870288848877, 4.701307773590088, 0.44426...   \n",
       "2          2  [0.6460480690002441, 4.356095314025879, 0.3705...   \n",
       "3          1  [1.3187897205352783, 4.804645538330078, 0.6664...   \n",
       "4          1  [1.3346798419952393, 4.704219341278076, 1.0185...   \n",
       "...      ...                                                ...   \n",
       "31801      1  [0.9925805330276489, 4.336263179779053, 0.1395...   \n",
       "31802      1  [1.2664387226104736, 3.647049903869629, -0.026...   \n",
       "31803      1  [1.4163975715637207, 4.123951435089111, 0.6112...   \n",
       "31804      1  [1.3337857723236084, 4.608504772186279, 0.2960...   \n",
       "31805      1  [0.4904952943325043, 5.872151851654053, 0.3358...   \n",
       "\n",
       "       predicted_ethnic gender  \\\n",
       "0                     1      0   \n",
       "1                     1      0   \n",
       "2                     1      1   \n",
       "3                     1      0   \n",
       "4                     1      0   \n",
       "...                 ...    ...   \n",
       "31801                 1      0   \n",
       "31802                 1      1   \n",
       "31803                 1      0   \n",
       "31804                 1      0   \n",
       "31805                 1      1   \n",
       "\n",
       "                                           logits_gender  predicted_gender  \n",
       "0      [1.5935319662094116, 0.9003511071205139, -2.92...                 0  \n",
       "1      [1.9701415300369263, 0.7289540767669678, -2.61...                 0  \n",
       "2      [1.630432367324829, 0.5803238749504089, -2.468...                 0  \n",
       "3      [1.7107672691345215, 0.5609513521194458, -2.49...                 0  \n",
       "4      [1.7862259149551392, 0.5271328687667847, -2.20...                 0  \n",
       "...                                                  ...               ...  \n",
       "31801  [2.0534555912017822, 0.6668630242347717, -2.59...                 0  \n",
       "31802  [1.618477463722229, 0.5838104486465454, -2.376...                 0  \n",
       "31803  [1.825916051864624, 0.5409999489784241, -2.128...                 0  \n",
       "31804  [2.02620530128479, 0.37937286496162415, -2.429...                 0  \n",
       "31805  [1.5236738920211792, 0.586313009262085, -2.377...                 0  \n",
       "\n",
       "[31806 rows x 22 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c060d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
