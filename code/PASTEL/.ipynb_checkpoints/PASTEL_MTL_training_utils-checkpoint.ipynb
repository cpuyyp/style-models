{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d7c8831",
   "metadata": {},
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#functions\" data-toc-modified-id=\"functions-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>functions</a></span><ul class=\"toc-item\"><li><span><a href=\"#multi-task-dataset\" data-toc-modified-id=\"multi-task-dataset-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>multi-task dataset</a></span></li><li><span><a href=\"#multi-task-model\" data-toc-modified-id=\"multi-task-model-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>multi-task model</a></span></li><li><span><a href=\"#trainer\" data-toc-modified-id=\"trainer-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>trainer</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5484ef75",
   "metadata": {},
   "source": [
    "# functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fd79d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785f4cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import collections\n",
    "import json\n",
    "from ast import literal_eval\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import Any, Callable, Dict, List, NewType, Optional, Tuple, Union\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torchmetrics\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "from transformers import *\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput, ModelOutput\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5168db",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_folder = os.environ[\"scratch_result_folder\"] if \"scratch_result_folder\" in os.environ else '../result'\n",
    "scratch_data_folder = os.environ[\"scratch_data_folder\"] if \"scratch_data_folder\" in os.environ else None\n",
    "repo_folder = os.environ[\"style_models_repo_folder\"] if \"style_models_repo_folder\" in os.environ else None\n",
    "data_folder = f\"{repo_folder}/data\" if repo_folder else '../../data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d382b23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# https://github.com/huggingface/transformers/issues/5486\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8839076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary: task_name: number_of_labels\n",
    "with open(f'{data_folder}/pastel/pastel_tasks2labels.json', 'r') as f:\n",
    "    tasks2labels = json.load(f)\n",
    "# Dictionary: task_name: task index\n",
    "tasks2idx = {k:i for i,k in enumerate(tasks2labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0deca6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class MyTrainingArgs:\n",
    "    # training args\n",
    "    selected_tasks: List\n",
    "    base_model_name: str \n",
    "    freeze_bert: bool\n",
    "    use_pooler: bool\n",
    "    num_epoch: int\n",
    "    lr: float = 5e-5\n",
    "    num_warmup_steps = 500\n",
    "    model_folder: str = None # if None, this will be inferred based on tasks\n",
    "    model_name: str = None # if provide, use to name model_folder, otherwise use style to name model_folder\n",
    "        \n",
    "    # data loader args\n",
    "    batch_size: int = 32\n",
    "    max_length: int = 64\n",
    "    shuffle: bool = False\n",
    "    num_workers: int = 4\n",
    "    data_limit: int = None # if not None, truncate dataset to keep only top {data_limit} rows\n",
    "    \n",
    "    # post training args\n",
    "    save_best: bool = True\n",
    "    load_best_at_end: bool = True\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        excute_time = datetime.now() \n",
    "        model_name = self.model_name if self.model_name else '+'.join(self.selected_tasks)\n",
    "        model_folder = f\"{result_folder}/{model_name}/{excute_time.now().strftime('%Y%m%d-%H:%M:%S')}\"\n",
    "        self.model_folder = model_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318bd186",
   "metadata": {},
   "source": [
    "## multi-task dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00309940",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset): \n",
    "    # currently it's a Mapping-style dataset. Not sure if a Iterable-style dataset will be better\n",
    "    # this works for standard class indices and also class probilities\n",
    "    # limit: use to truncate dataset. This will drop rows after certain index. May influence label distribution.\n",
    "    def __init__(self, training_args, split, label_prefix = None):\n",
    "        self.tasks = training_args.selected_tasks\n",
    "        self.max_length = training_args.max_length\n",
    "        self.split = split\n",
    "        self.label_prefix = label_prefix\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(training_args.base_model_name)\n",
    "        self.df = pd.read_csv(f\"{data_folder}/pastel/processed/{self.split}/{self.tasks[0] if len(self.tasks)==1 else 'pastel'}.csv\")\n",
    "        self.df = self.df.dropna()\n",
    "        self.df = self.df.reset_index(drop=True)\n",
    "        \n",
    "        # for distill model, logits that written to files need eval to be correctly recognized\n",
    "        # also apply softmax on logits\n",
    "        for task in self.tasks:\n",
    "            if self.label_prefix is not None:\n",
    "                task = self.label_prefix + task\n",
    "            if isinstance(self.df[task][0], str):\n",
    "                self.df[task] = torch.tensor(self.df[task].apply(literal_eval)).softmax(dim=1).numpy().tolist()\n",
    "\n",
    "        if training_args.data_limit:\n",
    "            self.df = self.df.iloc[:training_args.data_limit]\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        dataslice = self.df.iloc[idx]\n",
    "        item = {k: v for k, v in self.tokenizer(dataslice['output.sentences'], truncation=True, padding=True, max_length=self.max_length).items()}\n",
    "        item.update({task: dataslice[task] if self.label_prefix is None else dataslice[self.label_prefix+task] for task in self.tasks}) \n",
    "        return item\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100a28b2",
   "metadata": {},
   "source": [
    "## multi-task model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957b24e7",
   "metadata": {},
   "source": [
    "Given selected tasks, the model will add corresponding classification heads on the top of pretrained bert/(other bert). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed50b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionHead(nn.Module):\n",
    "    def __init__(self, embedding_dim = 768, hidden_dim = 128):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.hidden = nn.Linear(embedding_dim, 1)\n",
    "        \n",
    "        self.loss_fn = nn.MSELoss()\n",
    "    def forward(self, sent_emb, label):\n",
    "        batchsize = sent_emb.shape[0]\n",
    "        output = self.hidden(self.dropout(sent_emb)).squeeze(1)\n",
    "\n",
    "        loss = self.loss_fn(output, label.view(batchsize, -1).squeeze(-1))\n",
    "        return output, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c097a3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationHead(nn.Module):\n",
    "    def __init__(self, num_labels, embedding_dim = 768, hidden_dim = 128):\n",
    "        super().__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.hidden = nn.Linear(embedding_dim, self.num_labels)\n",
    "        \n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "    def forward(self, sent_emb, label):\n",
    "        batchsize = sent_emb.shape[0]\n",
    "        output = self.hidden(self.dropout(sent_emb))\n",
    "        \n",
    "        loss = self.loss_fn(output.view(-1, self.num_labels), label.view(batchsize, -1).squeeze(-1))\n",
    "        return output, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70701880",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class MultiTaskOutput(ModelOutput):\n",
    "    loss: torch.FloatTensor = None\n",
    "    sent_emb: torch.FloatTensor = None\n",
    "    all_logits: Optional[Dict[str, torch.FloatTensor]] = None\n",
    "    bert_hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    bert_attentions: Optional[Tuple[torch.FloatTensor]] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eec4f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskBert(PreTrainedModel):\n",
    "    def __init__(self, config, training_args):\n",
    "        super().__init__(config)\n",
    "#         self.training_args = training_args\n",
    "        self.tasks = training_args.selected_tasks\n",
    "        self.use_pooler = training_args.use_pooler\n",
    "        self.basemodel = AutoModel.from_pretrained(training_args.base_model_name)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(training_args.base_model_name)\n",
    "        self.style_heads = nn.ModuleList()\n",
    "        \n",
    "        for task in self.tasks:\n",
    "            if tasks2labels[task] == 1:\n",
    "                self.style_heads.append(RegressionHead())\n",
    "            else:\n",
    "                self.style_heads.append(ClassificationHead(tasks2labels[task]))\n",
    "                \n",
    "    def forward(self, input_ids, token_type_ids, attention_mask, return_logits=False, return_sent_emb=True, **kwargs):\n",
    "        output = self.basemodel(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)\n",
    "        if self.use_pooler and ('pooler_output' in output):\n",
    "            sent_emb = output['pooler_output']\n",
    "        else:\n",
    "            sent_emb = output['last_hidden_state'][:,0,:]\n",
    "        \n",
    "        total_loss = None\n",
    "        all_logits = None\n",
    "        if return_logits:\n",
    "            all_logits = {}\n",
    "        all_logits = {}\n",
    "        for task in kwargs:\n",
    "            i_task = self.tasks.index(task)\n",
    "            logits, loss = self.style_heads[i_task](sent_emb, kwargs[task]) \n",
    "            if total_loss is None:\n",
    "                total_loss = loss\n",
    "            else:\n",
    "                total_loss += loss\n",
    "            if return_logits:\n",
    "                all_logits[task] = logits.detach()\n",
    "        return MultiTaskOutput(loss=total_loss, sent_emb=sent_emb, all_logits=all_logits, bert_hidden_states=output.hidden_states, bert_attentions=output.attentions)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067bbd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(training_args):\n",
    "    config = AutoConfig.from_pretrained(training_args.base_model_name) \n",
    "    model = MultiTaskBert(config, training_args).to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1302944",
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_model(model, freeze_bert):\n",
    "    '''\n",
    "    if freeze_bert == True, freeze all layer. \n",
    "    if freeze_bert is a positive integer, freeze the bottom {freeze_bert} attention layers\n",
    "    negative integer should also work\n",
    "    '''\n",
    "    if freeze_bert==True:\n",
    "        for param in model.basemodel.parameters():\n",
    "            param.requires_grad = False\n",
    "    elif isinstance(freeze_bert, int):\n",
    "        for layer in model.basemodel.encoder.layer[:freeze_bert]: \n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = False  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03588b88",
   "metadata": {},
   "source": [
    "## trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cbd229",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nested_detach(tensors):\n",
    "    \"Detach `tensors` (even if it's a nested list/tuple of tensors).\"\n",
    "    if isinstance(tensors, (list, tuple)):\n",
    "        return type(tensors)(nested_detach(t) for t in tensors)\n",
    "    if isinstance(tensors, dict):\n",
    "        return {k:nested_detach(tensors[k]) for k in tensors}\n",
    "    return tensors.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cf5b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nested_to(tensors, device):\n",
    "    if isinstance(tensors, (list, tuple)):\n",
    "        return type(tensors)(nested_to(t, device) for t in tensors)\n",
    "    if isinstance(tensors, dict):\n",
    "        return {k: nested_to(tensors[k], device) for k in tensors}\n",
    "    return tensors.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9a1f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTrainer(Trainer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)    \n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        outputs = model(**inputs)\n",
    "        return (outputs.loss, outputs.all_logits) if return_outputs else outputs.loss\n",
    "    \n",
    "    def prediction_step(self,\n",
    "        model: nn.Module,\n",
    "        inputs: Dict[str, Union[torch.Tensor, Any]],\n",
    "        prediction_loss_only: bool,\n",
    "        ignore_keys: Optional[List[str]] = None,\n",
    "    ) -> Tuple[Optional[torch.Tensor], Optional[torch.Tensor], Optional[torch.Tensor]]:\n",
    "        inputs = nested_to(inputs, model.device)\n",
    "        labels = {}\n",
    "        for task in model.tasks:\n",
    "            labels[task] = inputs[task]\n",
    "        outputs = model(**inputs, return_logits=True)\n",
    "        loss = outputs.loss.detach()\n",
    "        \n",
    "        if prediction_loss_only:\n",
    "            return (loss, None, None)\n",
    "        logits = nested_detach(outputs.all_logits)\n",
    "        return (loss, logits, labels)    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00610e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions \n",
    "    res = {}\n",
    "    for task in labels:\n",
    "        if tasks2labels[task] == 2:\n",
    "            average = 'binary'\n",
    "        else:\n",
    "            average = 'macro'\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(labels[task], preds[task].argmax(-1), average=average)\n",
    "        acc = accuracy_score(labels[task], preds[task].argmax(-1))\n",
    "        res.update({\n",
    "            f'accuracy_{task}': acc,\n",
    "            f'f1_{task}': f1,\n",
    "            f'precision_{task}': precision,\n",
    "            f'recall_{task}': recall\n",
    "        })\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c3c5e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
