{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ed7de70",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#multi-task-dataloader\" data-toc-modified-id=\"multi-task-dataloader-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>multi-task dataloader</a></span></li><li><span><a href=\"#multi-task-model\" data-toc-modified-id=\"multi-task-model-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>multi-task model</a></span></li><li><span><a href=\"#train\" data-toc-modified-id=\"train-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>train</a></span><ul class=\"toc-item\"><li><span><a href=\"#single-task\" data-toc-modified-id=\"single-task-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>single task</a></span><ul class=\"toc-item\"><li><span><a href=\"#freeze\" data-toc-modified-id=\"freeze-3.1.1\"><span class=\"toc-item-num\">3.1.1&nbsp;&nbsp;</span>freeze</a></span></li><li><span><a href=\"#unfreeze\" data-toc-modified-id=\"unfreeze-3.1.2\"><span class=\"toc-item-num\">3.1.2&nbsp;&nbsp;</span>unfreeze</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "785f4cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torchmetrics\n",
    "import numpy as np\n",
    "import collections\n",
    "import json\n",
    "from tqdm.auto import tqdm, trange\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import *\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29182ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/huggingface/transformers/issues/5486\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7aaf17a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CLASSES = {\n",
    "    \"bert\": (BertConfig, BertPreTrainedModel, BertModel, BertTokenizer),\n",
    "    \"roberta\": (RobertaConfig, RobertaPreTrainedModel, RobertaModel, RobertaTokenizer),\n",
    "    \"albert\": (AlbertConfig, AlbertPreTrainedModel, AlbertModel, AlbertTokenizer),\n",
    "    \"distilbert\": (DistilBertConfig, DistilBertPreTrainedModel, DistilBertModel, DistilBertTokenizer)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f3fdde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_type = 'distilbert'\n",
    "# model_name = 'distilbert-base-uncased'\n",
    "\n",
    "model_type = 'bert'\n",
    "model_name = 'bert-base-uncased'\n",
    "config_class, pretrained_model_class, model_class, tokenizer_class = MODEL_CLASSES[model_type]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "186facb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8839076",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CrowdFlower': 13,\n",
       " 'DailyDialog': 7,\n",
       " 'EmoBank_Valence': 1,\n",
       " 'EmoBank_Arousal': 1,\n",
       " 'EmoBank_Dominance': 1,\n",
       " 'HateOffensive': 3,\n",
       " 'PASTEL_age': 8,\n",
       " 'PASTEL_country': 2,\n",
       " 'PASTEL_education': 10,\n",
       " 'PASTEL_ethnic': 10,\n",
       " 'PASTEL_gender': 3,\n",
       " 'PASTEL_politics': 3,\n",
       " 'PASTEL_tod': 5,\n",
       " 'SARC': 2,\n",
       " 'SarcasmGhosh': 2,\n",
       " 'SentiTreeBank': 1,\n",
       " 'ShortHumor': 2,\n",
       " 'ShortJokeKaggle': 2,\n",
       " 'ShortRomance': 2,\n",
       " 'StanfordPoliteness': 1,\n",
       " 'TroFi': 2,\n",
       " 'VUA': 2}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../data/xslue/tasks.json', 'r') as f:\n",
    "    tasks = json.load(f)\n",
    "tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02ae7ceb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CrowdFlower': 0,\n",
       " 'DailyDialog': 1,\n",
       " 'EmoBank_Valence': 2,\n",
       " 'EmoBank_Arousal': 3,\n",
       " 'EmoBank_Dominance': 4,\n",
       " 'HateOffensive': 5,\n",
       " 'PASTEL_age': 6,\n",
       " 'PASTEL_country': 7,\n",
       " 'PASTEL_education': 8,\n",
       " 'PASTEL_ethnic': 9,\n",
       " 'PASTEL_gender': 10,\n",
       " 'PASTEL_politics': 11,\n",
       " 'PASTEL_tod': 12,\n",
       " 'SARC': 13,\n",
       " 'SarcasmGhosh': 14,\n",
       " 'SentiTreeBank': 15,\n",
       " 'ShortHumor': 16,\n",
       " 'ShortJokeKaggle': 17,\n",
       " 'ShortRomance': 18,\n",
       " 'StanfordPoliteness': 19,\n",
       " 'TroFi': 20,\n",
       " 'VUA': 21}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tasks2idx = {}\n",
    "for i,k in enumerate(tasks):\n",
    "    tasks2idx[k] = i\n",
    "tasks2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d23f160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# task and their (train) dataset size \n",
    "selected_task = ['PASTEL_country', # 33224\n",
    "#                  'SARC', # 205645\n",
    "                 'SarcasmGhosh', # 39780\n",
    "                 'ShortHumor', # 37801\n",
    "#                  'ShortJokeKaggle', # 406682\n",
    "#                  'ShortRomance', # 1902\n",
    "#                  'TroFi', # 3335\n",
    "                 'VUA', # 15157\n",
    "                ] \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318bd186",
   "metadata": {},
   "source": [
    "# multi-task dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00309940",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset): \n",
    "    # currently it's a Mapping-style dataset. Not sure if a Iterable-style dataset will be better\n",
    "    def __init__(self, tsv_file):\n",
    "        self.df = pd.read_csv(tsv_file, sep='\\t')\n",
    "        self.df = self.df.dropna()\n",
    "        self.df = self.df.reset_index(drop=True)\n",
    "        if self.df['label'].dtype == 'float64':\n",
    "            self.df['label'] = self.df['label'].astype('float32')\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        dataslice = self.df.iloc[idx]\n",
    "        sample = {'text':dataslice['text'], 'label':dataslice['label']}\n",
    "        return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6463e817",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskTrainDataLoader():\n",
    "    '''\n",
    "    Each time, a random integer selects a dataset and load a batch of data {text, label} from it. Return i_task and data\n",
    "    \n",
    "    a iterator\n",
    "    Known issue: large dataset may have not iterate once, small datasets may have been iterated many times\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, selected_task, batch_size, shuffle, num_workers):\n",
    "        self.tasks = selected_task\n",
    "        self.split = 'train'\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.num_workers = num_workers\n",
    "        \n",
    "        self.num_tasks = len(self.tasks)\n",
    "        self.datasets = []\n",
    "        self.dataloaders = []\n",
    "        self.dataloaderiters = []\n",
    "        self.len = 0\n",
    "        for task in self.tasks:\n",
    "            self.datasets.append(MyDataset(f'../data/xslue/processed/{self.split}/{task}.tsv'))\n",
    "            self.dataloaders.append(DataLoader(self.datasets[-1], batch_size=self.batch_size, shuffle=self.shuffle, num_workers=self.num_workers)) \n",
    "            self.dataloaderiters.append(self.dataloaders[-1]._get_iterator())\n",
    "            self.len += len(self.dataloaders[-1])\n",
    "    def __len__(self):   \n",
    "        return self.len\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.n = 0\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        i_task = np.random.randint(self.num_tasks)\n",
    "        if self.n < self.len:\n",
    "            self.n += 1\n",
    "        else:\n",
    "            raise StopIteration\n",
    "        dataloaderiter = self.dataloaderiters[i_task]    \n",
    "        try: \n",
    "            batch = next(dataloaderiter)\n",
    "        except StopIteration:\n",
    "#             self.dataloaderiters[i_task]._reset(self.dataloaders[i_task])\n",
    "#             dataloaderiter = self.dataloaderiters[i_task]\n",
    "            self.dataloaderiters[i_task] = iter(self.dataloaders[i_task])\n",
    "            dataloaderiter = self.dataloaderiters[i_task]\n",
    "            batch = next(dataloaderiter)\n",
    "        return i_task, batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61019a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskTestDataLoader():\n",
    "    '''\n",
    "    For dev and test\n",
    "    \n",
    "    a generator\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, tasks, split, batch_size, shuffle, num_workers):\n",
    "        assert split in ['dev', 'test'], 'not implemented'\n",
    "        self.tasks = tasks\n",
    "        self.split = split\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.num_workers = num_workers\n",
    "        \n",
    "        self.num_tasks = len(tasks)\n",
    "        self.datasets = []\n",
    "        self.dataloaders = []\n",
    "        self.dataloaderiters = []\n",
    "        self.len = 0\n",
    "        for task in tasks:\n",
    "            self.datasets.append(MyDataset(f'../data/xslue/processed/{self.split}/{task}.tsv'))\n",
    "            self.dataloaders.append(DataLoader(self.datasets[-1], batch_size=self.batch_size, shuffle=self.shuffle, num_workers=self.num_workers))\n",
    "            self.len += len(self.dataloaders[-1])\n",
    "        self.i_task = 0\n",
    "    def __len__(self):   \n",
    "        return self.len\n",
    "\n",
    "    def __iter__(self):\n",
    "        for i_task in range(self.num_tasks):\n",
    "            dataloader = self.dataloaders[i_task]\n",
    "            for batch in dataloader:\n",
    "                yield i_task, batch\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100a28b2",
   "metadata": {},
   "source": [
    "# multi-task model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ed50b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionHead(nn.Module):\n",
    "    def __init__(self, embedding_dim = 768, hidden_dim = 128):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.hidden = nn.Linear(embedding_dim, 1)\n",
    "        \n",
    "        self.loss_fn = nn.MSELoss()\n",
    "    def forward(self, sent_emb, label):\n",
    "        output = self.hidden(self.dropout(sent_emb)).squeeze(1)\n",
    "\n",
    "        loss = self.loss_fn(output, label)\n",
    "        return output, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c097a3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationHead(nn.Module):\n",
    "    def __init__(self, num_labels, embedding_dim = 768, hidden_dim = 128):\n",
    "        super().__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.hidden = nn.Linear(embedding_dim, self.num_labels)\n",
    "        \n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "    def forward(self, sent_emb, label):\n",
    "        output = self.hidden(self.dropout(sent_emb))\n",
    "        \n",
    "        loss = self.loss_fn(output.view(-1, self.num_labels), label.view(-1))\n",
    "        return output, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4eec4f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskBert(pretrained_model_class):\n",
    "    def __init__(self, config, selected_task):\n",
    "        super().__init__(config)\n",
    "        self.basemodel = model_class(config)\n",
    "        self.style_heads = nn.ModuleList()\n",
    "        for task in selected_task:\n",
    "            if tasks[task] == 1:\n",
    "                self.style_heads.append(RegressionHead())\n",
    "            else:\n",
    "                self.style_heads.append(ClassificationHead(tasks[task]))\n",
    "    def forward(self, i_task=None, label=None, **kwargs):\n",
    "        output = self.basemodel(**kwargs)\n",
    "        if 'pooler_output' in output:\n",
    "            sent_emb = output['pooler_output']\n",
    "        else:\n",
    "            sent_emb = output['last_hidden_state'][:,0,:]\n",
    "        \n",
    "        output, loss = self.style_heads[i_task](sent_emb, label)\n",
    "        return output, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6acf874a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_loss(losses):\n",
    "    for k in losses:\n",
    "        print(f'{losses[k]:4.4f}', end=' ')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f7439d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, dataloader):\n",
    "    val_loss = collections.defaultdict(float)\n",
    "    val_size = collections.defaultdict(int)\n",
    "    overall_acc = torchmetrics.Accuracy() \n",
    "    task_accs = [torchmetrics.Accuracy() for i in range(len(selected_task))] \n",
    "    \n",
    "    if dataloader == 'train':\n",
    "        mt_dataloader = MultiTaskTrainDataLoader(selected_task, 32, False, 4)\n",
    "    else:\n",
    "        mt_dataloader = MultiTaskTestDataLoader(selected_task, 'dev', 32, False, 4)\n",
    "    model.eval()\n",
    "    for data in tqdm(mt_dataloader, leave=False):  \n",
    "        i_task, batch = data\n",
    "        label = batch['label'].to(device)\n",
    "        size = len(label)\n",
    "        del batch['label']\n",
    "        tokens = tokenizer(**batch, return_tensors='pt', padding=True, truncation=True, max_length=64).to(device)\n",
    "        output, loss = model(**tokens, i_task=i_task,  label=label)\n",
    "        overall_acc.update(output.to('cpu').detach(), label.to('cpu').detach())\n",
    "        task_accs[i_task].update(output.to('cpu').detach(), label.to('cpu').detach())\n",
    "        tokens = None\n",
    "        output = None\n",
    "        val_loss[i_task] += loss.detach().item()*size\n",
    "        val_size[i_task] += size\n",
    "    \n",
    "    accs = []\n",
    "    for i_task in val_loss:\n",
    "        val_loss[i_task] /= val_size[i_task]\n",
    "        accs.append(task_accs[i_task].compute())\n",
    "    model.train()\n",
    "    \n",
    "    \n",
    "    return val_loss, overall_acc.compute(), accs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d73059",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f6fdc2",
   "metadata": {},
   "source": [
    "## single task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1639a3b",
   "metadata": {},
   "source": [
    "### freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "801bd0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_task = [\n",
    "                 'ShortHumor', \n",
    "#                  'VUA',\n",
    "                ] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8edd545e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = MultiTaskTrainDataLoader(selected_task, 32, False, 4)\n",
    "\n",
    "config = config_class.from_pretrained(model_name)\n",
    "tokenizer = tokenizer_class.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6225c252",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56137a9fc30342c1932e8e6b6bc34cba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1182 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02287bcf5b5f48cc92eae1e8ceb1069f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1182 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86bee986c0004fdcaeb82bf6dc30af13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/62 [00:20<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i_epoch': 0, 'train_loss': 0.6652952216453065, 'train_acc': 0.6093754172325134, 'val_loss': 0.6672794208403976, 'val_acc': 0.6132122874259949}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95be7d183cf7466bb8489a19e110d345",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1182 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5ba3ff4bd894b03af5ce8d89190806c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1182 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64bdc73439c64ce390ac4392ab9e4691",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/62 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i_epoch': 1, 'train_loss': 0.6327887569712298, 'train_acc': 0.6503795981407166, 'val_loss': 0.6334584649319728, 'val_acc': 0.656580924987793}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa15ed06582741458851f9fbeb30b6ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1182 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28da7e17c3054cfba7969b43651d2f8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1182 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac33d422ee5d43cdab3928f2377753b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/62 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i_epoch': 2, 'train_loss': 0.6194691281748059, 'train_acc': 0.6582100987434387, 'val_loss': 0.6192063390457203, 'val_acc': 0.6591023802757263}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6284ddcf9d4d4b65939ae9cdd4384275",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1182 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ca326efcaf24b838a6092b2ce098ef3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1182 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a314148908d54e918adaa02ee22a9ccc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/62 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i_epoch': 3, 'train_loss': 0.6095730139999105, 'train_acc': 0.6636861562728882, 'val_loss': 0.608214221545319, 'val_acc': 0.6606152057647705}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52cc920b52a34a7c8ae68e014af0b388",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1182 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb624eb81bee48b5a3afe061703b49b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1182 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9107b647c40b4a11bf532da6cea73d2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/62 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fa3a0772f70>Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fa3a0772f70><function _MultiProcessingDataLoaderIter.__del__ at 0x7fa3a0772f70>\n",
      "\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "\n",
      "Exception ignored in: Traceback (most recent call last):\n",
      "<function _MultiProcessingDataLoaderIter.__del__ at 0x7fa3a0772f70>  File \"/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "  File \"/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "\n",
      "        self._shutdown_workers()self._shutdown_workers()Traceback (most recent call last):\n",
      "    \n",
      "  File \"/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "  File \"/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "\n",
      "self._shutdown_workers()\n",
      "      File \"/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "  File \"/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    self._shutdown_workers()\n",
      "      File \"/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "        if w.is_alive():\n",
      "  File \"/home/jz17d/anaconda3/envs/torch/lib/python3.9/multiprocessing/process.py\", line 160, in is_alive\n",
      "if w.is_alive():if w.is_alive():\n",
      "if w.is_alive():\n",
      "    \n",
      "  File \"/home/jz17d/anaconda3/envs/torch/lib/python3.9/multiprocessing/process.py\", line 160, in is_alive\n",
      "  File \"/home/jz17d/anaconda3/envs/torch/lib/python3.9/multiprocessing/process.py\", line 160, in is_alive\n",
      "      File \"/home/jz17d/anaconda3/envs/torch/lib/python3.9/multiprocessing/process.py\", line 160, in is_alive\n",
      "        assert self._parent_pid == os.getpid(), 'can only test a child process'assert self._parent_pid == os.getpid(), 'can only test a child process'assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "\n",
      "\n",
      "AssertionErrorassert self._parent_pid == os.getpid(), 'can only test a child process'AssertionError: can only test a child processAssertionError: \n",
      "\n",
      "can only test a child processAssertionError\n",
      ": : can only test a child processcan only test a child process\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i_epoch': 4, 'train_loss': 0.6090051889946207, 'train_acc': 0.6636067628860474, 'val_loss': 0.607591244228188, 'val_acc': 0.6611195206642151}\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "num_epoch = 5\n",
    "\n",
    "mt_model = MultiTaskBert(config, selected_task).to(device)\n",
    "for param in mt_model.basemodel.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "optimizer = optim.AdamW(mt_model.parameters(), lr=5e-5)\n",
    "# optimizer = optim.SGD(mt_model.parameters(), lr=0.001)\n",
    "\n",
    "# scheduler = lr_scheduler.LinearLR(optimizer, start_factor=0.01, total_iters=500)\n",
    "scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=5e-5, total_steps=num_epoch*len(train_dataloader)) \n",
    "\n",
    "df = pd.DataFrame(columns=['i_iter', 'train_loss', 'train_acc', 'val_loss', 'val_acc'])\n",
    "\n",
    "for i_epoch in range(num_epoch):\n",
    "    for i_iter, data in enumerate(tqdm(train_dataloader)):  \n",
    "\n",
    "        i_task, batch = data\n",
    "        optimizer.zero_grad()\n",
    "        label = batch['label'].to(device)\n",
    "        del batch['label']\n",
    "        tokens = tokenizer(**batch, return_tensors='pt', padding=True, truncation=True, max_length=64).to(device)\n",
    "        output, loss = mt_model(**tokens, i_task=i_task,  label=label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "#         if i_iter in [0, 10, 25, 50, 100]:\n",
    "#             train_loss, train_overall_acc, train_task_accs = validate(mt_model, 'train')\n",
    "#             val_loss, val_overall_acc, val_task_accs = validate(mt_model, 'val')\n",
    "#             result = {'i_iter':i_iter, 'train_loss':train_loss[0], 'train_acc':train_overall_acc.item(), 'val_loss':val_loss[0], 'val_acc':val_overall_acc.item()}\n",
    "#             df = df.append(result , ignore_index=True)\n",
    "#             print(result)\n",
    "#         if i_iter>100:\n",
    "#             break\n",
    "\n",
    "    train_loss, train_overall_acc, train_task_accs = validate(mt_model, 'train')\n",
    "    val_loss, val_overall_acc, val_task_accs = validate(mt_model, 'val')\n",
    "    result = {'i_epoch':i_epoch, 'train_loss':train_loss[0], 'train_acc':train_overall_acc.item(), 'val_loss':val_loss[0], 'val_acc':val_overall_acc.item()}\n",
    "    df = df.append(result , ignore_index=True)\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7fa4fefc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>i_iter</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>i_epoch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.665295</td>\n",
       "      <td>0.609375</td>\n",
       "      <td>0.667279</td>\n",
       "      <td>0.613212</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.632789</td>\n",
       "      <td>0.650380</td>\n",
       "      <td>0.633458</td>\n",
       "      <td>0.656581</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.619469</td>\n",
       "      <td>0.658210</td>\n",
       "      <td>0.619206</td>\n",
       "      <td>0.659102</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.609573</td>\n",
       "      <td>0.663686</td>\n",
       "      <td>0.608214</td>\n",
       "      <td>0.660615</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.609005</td>\n",
       "      <td>0.663607</td>\n",
       "      <td>0.607591</td>\n",
       "      <td>0.661120</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   i_iter  train_loss  train_acc  val_loss   val_acc  i_epoch\n",
       "0     NaN    0.665295   0.609375  0.667279  0.613212      0.0\n",
       "1     NaN    0.632789   0.650380  0.633458  0.656581      1.0\n",
       "2     NaN    0.619469   0.658210  0.619206  0.659102      2.0\n",
       "3     NaN    0.609573   0.663686  0.608214  0.660615      3.0\n",
       "4     NaN    0.609005   0.663607  0.607591  0.661120      4.0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with onecycle lr_scheduler\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fd64c12e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>i_iter</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>i_epoch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.635168</td>\n",
       "      <td>0.649798</td>\n",
       "      <td>0.633450</td>\n",
       "      <td>0.663137</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.624064</td>\n",
       "      <td>0.655088</td>\n",
       "      <td>0.621221</td>\n",
       "      <td>0.664145</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.616705</td>\n",
       "      <td>0.658263</td>\n",
       "      <td>0.613418</td>\n",
       "      <td>0.665154</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.614793</td>\n",
       "      <td>0.657972</td>\n",
       "      <td>0.611683</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.610469</td>\n",
       "      <td>0.660088</td>\n",
       "      <td>0.607097</td>\n",
       "      <td>0.668684</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   i_iter  train_loss  train_acc  val_loss   val_acc  i_epoch\n",
       "0     NaN    0.635168   0.649798  0.633450  0.663137      0.0\n",
       "1     NaN    0.624064   0.655088  0.621221  0.664145      1.0\n",
       "2     NaN    0.616705   0.658263  0.613418  0.665154      2.0\n",
       "3     NaN    0.614793   0.657972  0.611683  0.666667      3.0\n",
       "4     NaN    0.610469   0.660088  0.607097  0.668684      4.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with 500 warm up linear lr_scheduler\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662d7e74",
   "metadata": {},
   "source": [
    "### unfreeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8b2667c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_task = [\n",
    "                 'ShortHumor', \n",
    "#                  'VUA',\n",
    "                ] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c1c14bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = MultiTaskTrainDataLoader(selected_task, 32, False, 4)\n",
    "\n",
    "config = config_class.from_pretrained(model_name)\n",
    "tokenizer = tokenizer_class.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12e3920",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b450927177bc47939882e3ab00e24fa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1182 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1182 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/62 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i_epoch': 0, 'train_loss': 0.36868295300647363, 'train_acc': 0.8496600389480591, 'val_loss': 0.38348085836395135, 'val_acc': 0.8350983262062073}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe26aabfc57d48bcbea258c4916ef108",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1182 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "num_epoch = 10\n",
    "\n",
    "mt_model = MultiTaskBert(config, selected_task).to(device)\n",
    "# for param in mt_model.basemodel.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "optimizer = optim.AdamW(mt_model.parameters(), lr=5e-5)\n",
    "# optimizer = optim.SGD(mt_model.parameters(), lr=0.001)\n",
    "\n",
    "# scheduler = lr_scheduler.LinearLR(optimizer, start_factor=0.01, total_iters=500)\n",
    "scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=5e-5, total_steps=num_epoch*len(train_dataloader)) \n",
    "\n",
    "df = pd.DataFrame(columns=['i_iter', 'train_loss', 'train_acc', 'val_loss', 'val_acc'])\n",
    "\n",
    "for i_epoch in range(num_epoch):\n",
    "    for i_iter, data in enumerate(tqdm(train_dataloader)):  \n",
    "\n",
    "        i_task, batch = data\n",
    "        optimizer.zero_grad()\n",
    "        label = batch['label'].to(device)\n",
    "        del batch['label']\n",
    "        tokens = tokenizer(**batch, return_tensors='pt', padding=True, truncation=True, max_length=64).to(device)\n",
    "        output, loss = mt_model(**tokens, i_task=i_task,  label=label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "#         if i_iter in [0, 10, 25, 50, 100]:\n",
    "#             train_loss, train_overall_acc, train_task_accs = validate(mt_model, 'train')\n",
    "#             val_loss, val_overall_acc, val_task_accs = validate(mt_model, 'val')\n",
    "#             result = {'i_iter':i_iter, 'train_loss':train_loss[0], 'train_acc':train_overall_acc.item(), 'val_loss':val_loss[0], 'val_acc':val_overall_acc.item()}\n",
    "#             df = df.append(result , ignore_index=True)\n",
    "#             print(result)\n",
    "#         if i_iter>100:\n",
    "#             break\n",
    "\n",
    "    train_loss, train_overall_acc, train_task_accs = validate(mt_model, 'train')\n",
    "    val_loss, val_overall_acc, val_task_accs = validate(mt_model, 'val')\n",
    "    result = {'i_epoch':i_epoch, 'train_loss':train_loss[0], 'train_acc':train_overall_acc.item(), 'val_loss':val_loss[0], 'val_acc':val_overall_acc.item()}\n",
    "    df = df.append(result , ignore_index=True)\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ef240b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8f259d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "372.4px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
