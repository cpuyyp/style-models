{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ed7de70",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#multi-task-dataloader\" data-toc-modified-id=\"multi-task-dataloader-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>multi-task dataloader</a></span></li><li><span><a href=\"#multi-task-model\" data-toc-modified-id=\"multi-task-model-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>multi-task model</a></span></li><li><span><a href=\"#train\" data-toc-modified-id=\"train-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>train</a></span><ul class=\"toc-item\"><li><span><a href=\"#single-task\" data-toc-modified-id=\"single-task-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>single task</a></span><ul class=\"toc-item\"><li><span><a href=\"#ShortHumor\" data-toc-modified-id=\"ShortHumor-3.1.1\"><span class=\"toc-item-num\">3.1.1&nbsp;&nbsp;</span>ShortHumor</a></span><ul class=\"toc-item\"><li><span><a href=\"#freeze\" data-toc-modified-id=\"freeze-3.1.1.1\"><span class=\"toc-item-num\">3.1.1.1&nbsp;&nbsp;</span>freeze</a></span></li><li><span><a href=\"#unfreeze\" data-toc-modified-id=\"unfreeze-3.1.1.2\"><span class=\"toc-item-num\">3.1.1.2&nbsp;&nbsp;</span>unfreeze</a></span></li></ul></li><li><span><a href=\"#SARC\" data-toc-modified-id=\"SARC-3.1.2\"><span class=\"toc-item-num\">3.1.2&nbsp;&nbsp;</span>SARC</a></span><ul class=\"toc-item\"><li><span><a href=\"#freeze\" data-toc-modified-id=\"freeze-3.1.2.1\"><span class=\"toc-item-num\">3.1.2.1&nbsp;&nbsp;</span>freeze</a></span></li><li><span><a href=\"#unfreeze\" data-toc-modified-id=\"unfreeze-3.1.2.2\"><span class=\"toc-item-num\">3.1.2.2&nbsp;&nbsp;</span>unfreeze</a></span></li></ul></li></ul></li><li><span><a href=\"#multi-task\" data-toc-modified-id=\"multi-task-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>multi task</a></span><ul class=\"toc-item\"><li><span><a href=\"#ShortHumor-+-SARC-freeze\" data-toc-modified-id=\"ShortHumor-+-SARC-freeze-3.2.1\"><span class=\"toc-item-num\">3.2.1&nbsp;&nbsp;</span>ShortHumor + SARC freeze</a></span></li><li><span><a href=\"#ShortHumor-+-SARC-unfreeze\" data-toc-modified-id=\"ShortHumor-+-SARC-unfreeze-3.2.2\"><span class=\"toc-item-num\">3.2.2&nbsp;&nbsp;</span>ShortHumor + SARC unfreeze</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "785f4cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torchmetrics\n",
    "import numpy as np\n",
    "import collections\n",
    "import json\n",
    "from tqdm.auto import tqdm, trange\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import *\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29182ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/huggingface/transformers/issues/5486\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24897b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CLASSES = {\n",
    "    \"bert\": (BertConfig, BertPreTrainedModel, BertModel, BertTokenizer),\n",
    "    \"roberta\": (RobertaConfig, RobertaPreTrainedModel, RobertaModel, RobertaTokenizer),\n",
    "    \"albert\": (AlbertConfig, AlbertPreTrainedModel, AlbertModel, AlbertTokenizer),\n",
    "    \"distilbert\": (DistilBertConfig, DistilBertPreTrainedModel, DistilBertModel, DistilBertTokenizer)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb757688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_type = 'distilbert'\n",
    "# model_name = 'distilbert-base-uncased'\n",
    "\n",
    "model_type = 'bert'\n",
    "model_name = 'bert-base-uncased'\n",
    "config_class, pretrained_model_class, model_class, tokenizer_class = MODEL_CLASSES[model_type]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "186facb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8839076",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CrowdFlower': 13,\n",
       " 'DailyDialog': 7,\n",
       " 'EmoBank_Valence': 1,\n",
       " 'EmoBank_Arousal': 1,\n",
       " 'EmoBank_Dominance': 1,\n",
       " 'HateOffensive': 3,\n",
       " 'PASTEL_age': 8,\n",
       " 'PASTEL_country': 2,\n",
       " 'PASTEL_education': 10,\n",
       " 'PASTEL_ethnic': 10,\n",
       " 'PASTEL_gender': 3,\n",
       " 'PASTEL_politics': 3,\n",
       " 'PASTEL_tod': 5,\n",
       " 'SARC': 2,\n",
       " 'SarcasmGhosh': 2,\n",
       " 'SentiTreeBank': 1,\n",
       " 'ShortHumor': 2,\n",
       " 'ShortJokeKaggle': 2,\n",
       " 'ShortRomance': 2,\n",
       " 'StanfordPoliteness': 1,\n",
       " 'TroFi': 2,\n",
       " 'VUA': 2}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dictionary: task_name: number_of_labels\n",
    "with open('../data/xslue/tasks.json', 'r') as f:\n",
    "    tasks = json.load(f)\n",
    "tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02ae7ceb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CrowdFlower': 0,\n",
       " 'DailyDialog': 1,\n",
       " 'EmoBank_Valence': 2,\n",
       " 'EmoBank_Arousal': 3,\n",
       " 'EmoBank_Dominance': 4,\n",
       " 'HateOffensive': 5,\n",
       " 'PASTEL_age': 6,\n",
       " 'PASTEL_country': 7,\n",
       " 'PASTEL_education': 8,\n",
       " 'PASTEL_ethnic': 9,\n",
       " 'PASTEL_gender': 10,\n",
       " 'PASTEL_politics': 11,\n",
       " 'PASTEL_tod': 12,\n",
       " 'SARC': 13,\n",
       " 'SarcasmGhosh': 14,\n",
       " 'SentiTreeBank': 15,\n",
       " 'ShortHumor': 16,\n",
       " 'ShortJokeKaggle': 17,\n",
       " 'ShortRomance': 18,\n",
       " 'StanfordPoliteness': 19,\n",
       " 'TroFi': 20,\n",
       " 'VUA': 21}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dictionary: task_name: task index\n",
    "tasks2idx = {k:i for i,k in enumerate(tasks)}\n",
    "tasks2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d23f160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# task and their (train) dataset size \n",
    "selected_task = ['PASTEL_country', # 33224\n",
    "#                  'SARC', # 205645\n",
    "                 'SarcasmGhosh', # 39780\n",
    "                 'ShortHumor', # 37801\n",
    "#                  'ShortJokeKaggle', # 406682\n",
    "#                  'ShortRomance', # 1902\n",
    "#                  'TroFi', # 3335\n",
    "                 'VUA', # 15157\n",
    "                ] \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318bd186",
   "metadata": {},
   "source": [
    "# multi-task dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834ecfbf",
   "metadata": {},
   "source": [
    "Test/validation dataloader consume dataset one by one, where as the train dataloader do it randomly. So the train dataloader is more complicated than test/validation dataloader. It must be able to reset a dataset once it is exhausted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00309940",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset): \n",
    "    # currently it's a Mapping-style dataset. Not sure if a Iterable-style dataset will be better\n",
    "    # limit: use to truncate dataset. This will drop rows after certain index. May influence label distribution.\n",
    "    def __init__(self, tsv_file, limit=None):\n",
    "        self.df = pd.read_csv(tsv_file, sep='\\t')\n",
    "        self.df = self.df.dropna()\n",
    "        self.df = self.df.reset_index(drop=True)\n",
    "        if self.df['label'].dtype == 'float64':\n",
    "            self.df['label'] = self.df['label'].astype('float32')\n",
    "        if limit:\n",
    "            self.df = self.df.iloc[:limit]\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        dataslice = self.df.iloc[idx]\n",
    "        sample = {'text':dataslice['text'], 'label':dataslice['label']}\n",
    "        return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6463e817",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskTrainDataLoader():\n",
    "    '''\n",
    "    Each time, a random integer selects a dataset and load a batch of data {text, label} from it. Return i_task and data\n",
    "    \n",
    "    a iterator\n",
    "    Known issue: large dataset may have not iterate once, small datasets may have been iterated many times\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, selected_task, batch_size, shuffle, num_workers, limit=None):\n",
    "        self.tasks = selected_task\n",
    "        self.split = 'train'\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.num_workers = num_workers\n",
    "        \n",
    "        self.num_tasks = len(self.tasks)\n",
    "        self.datasets = []\n",
    "        self.dataloaders = []\n",
    "        self.dataloaderiters = []\n",
    "        self.len = 0\n",
    "        for task in self.tasks:\n",
    "            self.datasets.append(MyDataset(f'../data/xslue/processed/{self.split}/{task}.tsv', limit=limit))\n",
    "            self.dataloaders.append(DataLoader(self.datasets[-1], batch_size=self.batch_size, shuffle=self.shuffle, num_workers=self.num_workers)) \n",
    "            self.dataloaderiters.append(self.dataloaders[-1]._get_iterator())\n",
    "            self.len += len(self.dataloaders[-1])\n",
    "    def __len__(self):   \n",
    "        return self.len\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.n = 0\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        i_task = np.random.randint(self.num_tasks)\n",
    "        if self.n < self.len:\n",
    "            self.n += 1\n",
    "        else:\n",
    "            raise StopIteration\n",
    "        dataloaderiter = self.dataloaderiters[i_task]    \n",
    "        try: \n",
    "            batch = next(dataloaderiter)\n",
    "        except StopIteration:\n",
    "#             self.dataloaderiters[i_task]._reset(self.dataloaders[i_task])\n",
    "#             dataloaderiter = self.dataloaderiters[i_task]\n",
    "            self.dataloaderiters[i_task] = iter(self.dataloaders[i_task])\n",
    "            dataloaderiter = self.dataloaderiters[i_task]\n",
    "            batch = next(dataloaderiter)\n",
    "        return i_task, batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61019a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskTestDataLoader():\n",
    "    '''\n",
    "    For dev and test\n",
    "    \n",
    "    a generator\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, tasks, split, batch_size, shuffle, num_workers, limit=None):\n",
    "        assert split in ['dev', 'test'], 'not implemented'\n",
    "        self.tasks = tasks\n",
    "        self.split = split\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.num_workers = num_workers\n",
    "        \n",
    "        self.num_tasks = len(tasks)\n",
    "        self.datasets = []\n",
    "        self.dataloaders = []\n",
    "        self.dataloaderiters = []\n",
    "        self.len = 0\n",
    "        for task in tasks:\n",
    "            self.datasets.append(MyDataset(f'../data/xslue/processed/{self.split}/{task}.tsv', limit=limit))\n",
    "            self.dataloaders.append(DataLoader(self.datasets[-1], batch_size=self.batch_size, shuffle=self.shuffle, num_workers=self.num_workers))\n",
    "            self.len += len(self.dataloaders[-1])\n",
    "        self.i_task = 0\n",
    "    def __len__(self):   \n",
    "        return self.len\n",
    "\n",
    "    def __iter__(self):\n",
    "        for i_task in range(self.num_tasks):\n",
    "            dataloader = self.dataloaders[i_task]\n",
    "            for batch in dataloader:\n",
    "                yield i_task, batch\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100a28b2",
   "metadata": {},
   "source": [
    "# multi-task model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2c748e",
   "metadata": {},
   "source": [
    "Given selected tasks, the model will add corresponding classification heads on the top of pretrained bert/(other bert). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ed50b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionHead(nn.Module):\n",
    "    def __init__(self, embedding_dim = 768, hidden_dim = 128):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.hidden = nn.Linear(embedding_dim, 1)\n",
    "        \n",
    "        self.loss_fn = nn.MSELoss()\n",
    "    def forward(self, sent_emb, label):\n",
    "        output = self.hidden(self.dropout(sent_emb)).squeeze(1)\n",
    "\n",
    "        loss = self.loss_fn(output, label)\n",
    "        return output, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c097a3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationHead(nn.Module):\n",
    "    def __init__(self, num_labels, embedding_dim = 768, hidden_dim = 128):\n",
    "        super().__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.hidden = nn.Linear(embedding_dim, self.num_labels)\n",
    "        \n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "    def forward(self, sent_emb, label):\n",
    "        output = self.hidden(self.dropout(sent_emb))\n",
    "        \n",
    "        loss = self.loss_fn(output.view(-1, self.num_labels), label.view(-1))\n",
    "        return output, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4eec4f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskBert(pretrained_model_class):\n",
    "    def __init__(self, config, selected_task):\n",
    "        super().__init__(config)\n",
    "        self.basemodel = model_class.from_pretrained(model_name)\n",
    "        self.style_heads = nn.ModuleList()\n",
    "        for task in selected_task:\n",
    "            if tasks[task] == 1:\n",
    "                self.style_heads.append(RegressionHead())\n",
    "            else:\n",
    "                self.style_heads.append(ClassificationHead(tasks[task]))\n",
    "    def forward(self, i_task=None, label=None, **kwargs):\n",
    "        output = self.basemodel(**kwargs)\n",
    "        if 'pooler_output' in output:\n",
    "            sent_emb = output['pooler_output']\n",
    "        else:\n",
    "            sent_emb = output['last_hidden_state'][:,0,:]\n",
    "        \n",
    "        output, loss = self.style_heads[i_task](sent_emb, label) \n",
    "        return output, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6acf874a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_loss(losses):\n",
    "    for k in losses:\n",
    "        print(f'{losses[k]:4.4f}', end=' ')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f7439d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, selected_task, split, limit=None):\n",
    "    val_loss = collections.defaultdict(float)\n",
    "    val_size = collections.defaultdict(int)\n",
    "    overall_acc = torchmetrics.Accuracy() \n",
    "    task_accs = [torchmetrics.Accuracy() for i in range(len(selected_task))] \n",
    "    \n",
    "    if split == 'train':\n",
    "        mt_dataloader = MultiTaskTrainDataLoader(selected_task, 32, False, 4, limit=limit)\n",
    "    else:\n",
    "        mt_dataloader = MultiTaskTestDataLoader(selected_task, 'dev', 32, False, 4, limit=limit)\n",
    "        \n",
    "    model.eval()\n",
    "    for data in tqdm(mt_dataloader, leave=False):  \n",
    "        i_task, batch = data\n",
    "        label = batch['label'].to(device)\n",
    "        size = len(label)\n",
    "        del batch['label']\n",
    "        tokens = tokenizer(**batch, return_tensors='pt', padding=True, truncation=True, max_length=64).to(device)\n",
    "        output, loss = model(**tokens, i_task=i_task,  label=label)\n",
    "        overall_acc.update(output.to('cpu').detach(), label.to('cpu').detach())\n",
    "        task_accs[i_task].update(output.to('cpu').detach(), label.to('cpu').detach())\n",
    "        tokens = None\n",
    "        output = None\n",
    "        val_loss[i_task] += loss.detach().item()*size\n",
    "        val_size[i_task] += size\n",
    "    \n",
    "    accs = []\n",
    "    for i_task in val_loss:\n",
    "        val_loss[i_task] /= val_size[i_task]\n",
    "        accs.append(task_accs[i_task].compute())\n",
    "    model.train()\n",
    "    \n",
    "    \n",
    "    return val_loss, overall_acc.compute(), accs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d73059",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f6fdc2",
   "metadata": {},
   "source": [
    "## single task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd7d750",
   "metadata": {},
   "source": [
    "### ShortHumor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d84d7e",
   "metadata": {},
   "source": [
    "#### freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "801bd0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_task = [\n",
    "                 'ShortHumor', \n",
    "#                  'VUA',\n",
    "                ] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8edd545e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = MultiTaskTrainDataLoader(selected_task, 32, False, 4)\n",
    "\n",
    "config = config_class.from_pretrained(model_name)\n",
    "tokenizer = tokenizer_class.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6225c252",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "327851f1facb48928c6da65fd9df5f3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1182 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1182 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/62 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i_epoch': 0, 'train_loss': 0.6093603514596295, 'train_acc': 0.6768603920936584, 'val_loss': 0.6037419830616592, 'val_acc': 0.6787695288658142}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f773cbd96f1464eaeb51eea492b96be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1182 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1182 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/62 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i_epoch': 1, 'train_loss': 0.5694271674443419, 'train_acc': 0.6997169256210327, 'val_loss': 0.5666932542697986, 'val_acc': 0.7009581327438354}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b89f9d15df2c4093a7533bed82f3dc58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1182 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1182 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/62 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i_epoch': 2, 'train_loss': 0.542740888546511, 'train_acc': 0.7273352742195129, 'val_loss': 0.5414822714171262, 'val_acc': 0.7216339111328125}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db90ff53549c4b0db4dcf65b89ea9a30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1182 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1182 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/62 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i_epoch': 3, 'train_loss': 0.5291925388148391, 'train_acc': 0.7452183961868286, 'val_loss': 0.5293197461408133, 'val_acc': 0.7367624640464783}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed6ffbe6f79045bca1def22ea7263751",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1182 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1182 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/62 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i_epoch': 4, 'train_loss': 0.5281198126989516, 'train_acc': 0.7454035878181458, 'val_loss': 0.5283215008657266, 'val_acc': 0.738779604434967}\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "num_epoch = 5\n",
    "\n",
    "mt_model = MultiTaskBert(config, selected_task).to(device)\n",
    "for param in mt_model.basemodel.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "optimizer = optim.AdamW(mt_model.parameters(), lr=5e-5)\n",
    "# optimizer = optim.SGD(mt_model.parameters(), lr=0.001)\n",
    "\n",
    "# scheduler = lr_scheduler.LinearLR(optimizer, start_factor=0.01, total_iters=500)\n",
    "scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=5e-5, total_steps=num_epoch*len(train_dataloader)) \n",
    "\n",
    "df = pd.DataFrame(columns=['i_iter', 'train_loss', 'train_acc', 'val_loss', 'val_acc'])\n",
    "\n",
    "for i_epoch in range(num_epoch):\n",
    "    for i_iter, data in enumerate(tqdm(train_dataloader)):  \n",
    "\n",
    "        i_task, batch = data\n",
    "        optimizer.zero_grad()\n",
    "        label = batch['label'].to(device)\n",
    "        del batch['label']\n",
    "        tokens = tokenizer(**batch, return_tensors='pt', padding=True, truncation=True, max_length=64).to(device)\n",
    "        output, loss = mt_model(**tokens, i_task=i_task,  label=label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "#         if i_iter in [0, 10, 25, 50, 100]:\n",
    "#             train_loss, train_overall_acc, train_task_accs = validate(mt_model, 'train')\n",
    "#             val_loss, val_overall_acc, val_task_accs = validate(mt_model, 'val')\n",
    "#             result = {'i_iter':i_iter, 'train_loss':train_loss[0], 'train_acc':train_overall_acc.item(), 'val_loss':val_loss[0], 'val_acc':val_overall_acc.item()}\n",
    "#             df = df.append(result , ignore_index=True)\n",
    "#             print(result)\n",
    "#         if i_iter>100:\n",
    "#             break\n",
    "\n",
    "    train_loss, train_overall_acc, train_task_accs = validate(mt_model, 'train')\n",
    "    val_loss, val_overall_acc, val_task_accs = validate(mt_model, 'val')\n",
    "    result = {'i_epoch':i_epoch, 'train_loss':train_loss[0], 'train_acc':train_overall_acc.item(), 'val_loss':val_loss[0], 'val_acc':val_overall_acc.item()}\n",
    "    df = df.append(result , ignore_index=True)\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b4a15fa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>i_iter</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>i_epoch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.609360</td>\n",
       "      <td>0.676860</td>\n",
       "      <td>0.603742</td>\n",
       "      <td>0.678770</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.569427</td>\n",
       "      <td>0.699717</td>\n",
       "      <td>0.566693</td>\n",
       "      <td>0.700958</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.542741</td>\n",
       "      <td>0.727335</td>\n",
       "      <td>0.541482</td>\n",
       "      <td>0.721634</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.529193</td>\n",
       "      <td>0.745218</td>\n",
       "      <td>0.529320</td>\n",
       "      <td>0.736762</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.528120</td>\n",
       "      <td>0.745404</td>\n",
       "      <td>0.528322</td>\n",
       "      <td>0.738780</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   i_iter  train_loss  train_acc  val_loss   val_acc  i_epoch\n",
       "0     NaN    0.609360   0.676860  0.603742  0.678770      0.0\n",
       "1     NaN    0.569427   0.699717  0.566693  0.700958      1.0\n",
       "2     NaN    0.542741   0.727335  0.541482  0.721634      2.0\n",
       "3     NaN    0.529193   0.745218  0.529320  0.736762      3.0\n",
       "4     NaN    0.528120   0.745404  0.528322  0.738780      4.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with onecycle lr_scheduler\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ba0332",
   "metadata": {},
   "source": [
    "#### unfreeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "68d4e2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_task = [\n",
    "                 'ShortHumor', \n",
    "#                  'VUA',\n",
    "                ] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f18b570f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = MultiTaskTrainDataLoader(selected_task, 32, False, 4)\n",
    "\n",
    "config = config_class.from_pretrained(model_name)\n",
    "tokenizer = tokenizer_class.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dc3bd767",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1182 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1182 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/62 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i_epoch': 0, 'train_loss': 0.07624205425341254, 'train_acc': 0.9742070436477661, 'val_loss': 0.1591183666442298, 'val_acc': 0.940494179725647}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1182 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1182 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/62 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f19a2515550>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/home/jz17d/anaconda3/envs/torch/lib/python3.9/multiprocessing/process.py\", line 160, in is_alive\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f19a2515550>Exception ignored in: \n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f19a2515550><function _MultiProcessingDataLoaderIter.__del__ at 0x7f19a2515550>Traceback (most recent call last):\n",
      "\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()  File \"/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "\n",
      "          File \"/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "self._shutdown_workers()\n",
      "  File \"/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "if w.is_alive():\n",
      "        if w.is_alive():if w.is_alive():\n",
      "    \n",
      "assert self._parent_pid == os.getpid(), 'can only test a child process'  File \"/home/jz17d/anaconda3/envs/torch/lib/python3.9/multiprocessing/process.py\", line 160, in is_alive\n",
      "  File \"/home/jz17d/anaconda3/envs/torch/lib/python3.9/multiprocessing/process.py\", line 160, in is_alive\n",
      "\n",
      "  File \"/home/jz17d/anaconda3/envs/torch/lib/python3.9/multiprocessing/process.py\", line 160, in is_alive\n",
      "AssertionError            assert self._parent_pid == os.getpid(), 'can only test a child process'assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "\n",
      ": AssertionErrorcan only test a child process: \n",
      "AssertionError: can only test a child processcan only test a child process\n",
      "assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "\n",
      "AssertionError: can only test a child process\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i_epoch': 1, 'train_loss': 0.030277462488580556, 'train_acc': 0.9911113381385803, 'val_loss': 0.14881271859926823, 'val_acc': 0.9520927667617798}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1182 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1182 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/62 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i_epoch': 2, 'train_loss': 0.011882821528125562, 'train_acc': 0.9964551329612732, 'val_loss': 0.15233089646162015, 'val_acc': 0.9546142220497131}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1182 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1182 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/62 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i_epoch': 3, 'train_loss': 0.0017386663505057035, 'train_acc': 0.9996032118797302, 'val_loss': 0.14110398142522484, 'val_acc': 0.9667171239852905}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1182 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1182 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/62 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i_epoch': 4, 'train_loss': 0.000905192531714256, 'train_acc': 0.9998412728309631, 'val_loss': 0.15679824659722963, 'val_acc': 0.9636913537979126}\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "num_epoch = 5\n",
    "\n",
    "mt_model = MultiTaskBert(config, selected_task).to(device)\n",
    "# for param in mt_model.basemodel.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "optimizer = optim.AdamW(mt_model.parameters(), lr=5e-5)\n",
    "# optimizer = optim.SGD(mt_model.parameters(), lr=0.001)\n",
    "\n",
    "# scheduler = lr_scheduler.LinearLR(optimizer, start_factor=0.01, total_iters=500)\n",
    "scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=5e-5, total_steps=num_epoch*len(train_dataloader)) \n",
    "\n",
    "df = pd.DataFrame(columns=['i_iter', 'train_loss', 'train_acc', 'val_loss', 'val_acc'])\n",
    "\n",
    "for i_epoch in range(num_epoch):\n",
    "    for i_iter, data in enumerate(tqdm(train_dataloader)):  \n",
    "\n",
    "        i_task, batch = data\n",
    "        optimizer.zero_grad()\n",
    "        label = batch['label'].to(device)\n",
    "        del batch['label']\n",
    "        tokens = tokenizer(**batch, return_tensors='pt', padding=True, truncation=True, max_length=64).to(device)\n",
    "        output, loss = mt_model(**tokens, i_task=i_task,  label=label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "#         if i_iter in [0, 10, 25, 50, 100]:\n",
    "#             train_loss, train_overall_acc, train_task_accs = validate(mt_model, 'train')\n",
    "#             val_loss, val_overall_acc, val_task_accs = validate(mt_model, 'val')\n",
    "#             result = {'i_iter':i_iter, 'train_loss':train_loss[0], 'train_acc':train_overall_acc.item(), 'val_loss':val_loss[0], 'val_acc':val_overall_acc.item()}\n",
    "#             df = df.append(result , ignore_index=True)\n",
    "#             print(result)\n",
    "#         if i_iter>100:\n",
    "#             break\n",
    "\n",
    "    train_loss, train_overall_acc, train_task_accs = validate(mt_model, 'train')\n",
    "    val_loss, val_overall_acc, val_task_accs = validate(mt_model, 'val')\n",
    "    result = {'i_epoch':i_epoch, 'train_loss':train_loss[0], 'train_acc':train_overall_acc.item(), 'val_loss':val_loss[0], 'val_acc':val_overall_acc.item()}\n",
    "    df = df.append(result , ignore_index=True)\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2a3833e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>i_iter</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>i_epoch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.076242</td>\n",
       "      <td>0.974207</td>\n",
       "      <td>0.159118</td>\n",
       "      <td>0.940494</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.030277</td>\n",
       "      <td>0.991111</td>\n",
       "      <td>0.148813</td>\n",
       "      <td>0.952093</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.011883</td>\n",
       "      <td>0.996455</td>\n",
       "      <td>0.152331</td>\n",
       "      <td>0.954614</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001739</td>\n",
       "      <td>0.999603</td>\n",
       "      <td>0.141104</td>\n",
       "      <td>0.966717</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000905</td>\n",
       "      <td>0.999841</td>\n",
       "      <td>0.156798</td>\n",
       "      <td>0.963691</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   i_iter  train_loss  train_acc  val_loss   val_acc  i_epoch\n",
       "0     NaN    0.076242   0.974207  0.159118  0.940494      0.0\n",
       "1     NaN    0.030277   0.991111  0.148813  0.952093      1.0\n",
       "2     NaN    0.011883   0.996455  0.152331  0.954614      2.0\n",
       "3     NaN    0.001739   0.999603  0.141104  0.966717      3.0\n",
       "4     NaN    0.000905   0.999841  0.156798  0.963691      4.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df # ShortHumor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011dd416",
   "metadata": {},
   "source": [
    "### SARC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b7395c",
   "metadata": {},
   "source": [
    "#### freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "47b0a08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_task = [\n",
    "#                 'SarcasmGhosh',\n",
    "                'SARC',\n",
    "                ] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fb299026",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = MultiTaskTrainDataLoader(selected_task, 32, False, 4)\n",
    "\n",
    "config = config_class.from_pretrained(model_name)\n",
    "tokenizer = tokenizer_class.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ba77368d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adb4457bba044b0db9d126e414900eeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6427 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6427 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1607 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i_epoch': 0, 'train_loss': 0.6752687707076435, 'train_acc': 0.5900925993919373, 'val_loss': 0.6748735675782087, 'val_acc': 0.5901769995689392}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "789b2181eaa646918958c0875954be9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6427 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6427 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1607 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i_epoch': 1, 'train_loss': 0.6693607444583342, 'train_acc': 0.5988796353340149, 'val_loss': 0.6689359912007867, 'val_acc': 0.6004862785339355}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8d2e11a2e0e4787bd30d94afeb98f21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6427 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6427 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1607 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i_epoch': 2, 'train_loss': 0.6668882590044554, 'train_acc': 0.6024197340011597, 'val_loss': 0.6664740512445728, 'val_acc': 0.6041431427001953}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d157dba33a89401c9616bfc91aa8bf93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6427 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6427 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1607 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i_epoch': 3, 'train_loss': 0.6650835614176025, 'train_acc': 0.6039320230484009, 'val_loss': 0.6646254126094839, 'val_acc': 0.6058354377746582}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04415966ecad40609bbc9d6acc6df7a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6427 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6427 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1607 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ff312b82d30><function _MultiProcessingDataLoaderIter.__del__ at 0x7ff312b82d30>\n",
      "\n",
      "Traceback (most recent call last):\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ff312b82d30>  File \"/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "          File \"/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "self._shutdown_workers()self._shutdown_workers()    \n",
      "\n",
      "self._shutdown_workers()  File \"/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "  File \"/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "\n",
      "  File \"/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "        if w.is_alive():\n",
      "if w.is_alive():  File \"/home/jz17d/anaconda3/envs/torch/lib/python3.9/multiprocessing/process.py\", line 160, in is_alive\n",
      "    if w.is_alive():    \n",
      "assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "\n",
      "AssertionError  File \"/home/jz17d/anaconda3/envs/torch/lib/python3.9/multiprocessing/process.py\", line 160, in is_alive\n",
      "      File \"/home/jz17d/anaconda3/envs/torch/lib/python3.9/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process': \n",
      "AssertionError: can only test a child process\n",
      "can only test a child processassert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "\n",
      "AssertionError: can only test a child process\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i_epoch': 4, 'train_loss': 0.6644202125413103, 'train_acc': 0.6025412678718567, 'val_loss': 0.6639701425878484, 'val_acc': 0.6020035147666931}\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "num_epoch = 5\n",
    "\n",
    "mt_model = MultiTaskBert(config, selected_task).to(device)\n",
    "for param in mt_model.basemodel.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "optimizer = optim.AdamW(mt_model.parameters(), lr=5e-5)\n",
    "# optimizer = optim.SGD(mt_model.parameters(), lr=0.001)\n",
    "\n",
    "num_warmup_steps=500\n",
    "num_training_steps=num_epoch*len(train_dataloader)\n",
    "def lr_lambda(current_step: int):\n",
    "    if current_step < num_warmup_steps:\n",
    "        return float(current_step) / float(max(1, num_warmup_steps))\n",
    "    return max(\n",
    "        0.0, float(num_training_steps - current_step) / float(max(1, num_training_steps - num_warmup_steps))\n",
    "    )\n",
    "scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda, -1)\n",
    "# scheduler = lr_scheduler.LinearLR(optimizer, start_factor=0.01, total_iters=500)\n",
    "# scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=5e-5, total_steps=num_epoch*len(train_dataloader)) \n",
    "\n",
    "df = pd.DataFrame(columns=['i_iter', 'train_loss', 'train_acc', 'val_loss', 'val_acc'])\n",
    "\n",
    "for i_epoch in range(num_epoch):\n",
    "    for i_iter, data in enumerate(tqdm(train_dataloader)):  \n",
    "\n",
    "        i_task, batch = data\n",
    "        optimizer.zero_grad()\n",
    "        label = batch['label'].to(device)\n",
    "        del batch['label']\n",
    "        tokens = tokenizer(**batch, return_tensors='pt', padding=True, truncation=True, max_length=64).to(device)\n",
    "        output, loss = mt_model(**tokens, i_task=i_task,  label=label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "#         if i_iter in [0, 10, 25, 50, 100]:\n",
    "#             train_loss, train_overall_acc, train_task_accs = validate(mt_model, 'train')\n",
    "#             val_loss, val_overall_acc, val_task_accs = validate(mt_model, 'val')\n",
    "#             result = {'i_iter':i_iter, 'train_loss':train_loss[0], 'train_acc':train_overall_acc.item(), 'val_loss':val_loss[0], 'val_acc':val_overall_acc.item()}\n",
    "#             df = df.append(result , ignore_index=True)\n",
    "#             print(result)\n",
    "#         if i_iter>100:\n",
    "#             break\n",
    "\n",
    "    train_loss, train_overall_acc, train_task_accs = validate(mt_model, selected_task, split='train')\n",
    "    val_loss, val_overall_acc, val_task_accs = validate(mt_model, selected_task, split='val')\n",
    "    result = {'i_epoch':i_epoch, 'train_loss':train_loss[0], 'train_acc':train_overall_acc.item(), 'val_loss':val_loss[0], 'val_acc':val_overall_acc.item()}\n",
    "    df = df.append(result , ignore_index=True)\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e7a30b21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>i_iter</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>i_epoch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.675269</td>\n",
       "      <td>0.590093</td>\n",
       "      <td>0.674874</td>\n",
       "      <td>0.590177</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.669361</td>\n",
       "      <td>0.598880</td>\n",
       "      <td>0.668936</td>\n",
       "      <td>0.600486</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.666888</td>\n",
       "      <td>0.602420</td>\n",
       "      <td>0.666474</td>\n",
       "      <td>0.604143</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.665084</td>\n",
       "      <td>0.603932</td>\n",
       "      <td>0.664625</td>\n",
       "      <td>0.605835</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.664420</td>\n",
       "      <td>0.602541</td>\n",
       "      <td>0.663970</td>\n",
       "      <td>0.602004</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   i_iter  train_loss  train_acc  val_loss   val_acc  i_epoch\n",
       "0     NaN    0.675269   0.590093  0.674874  0.590177      0.0\n",
       "1     NaN    0.669361   0.598880  0.668936  0.600486      1.0\n",
       "2     NaN    0.666888   0.602420  0.666474  0.604143      2.0\n",
       "3     NaN    0.665084   0.603932  0.664625  0.605835      3.0\n",
       "4     NaN    0.664420   0.602541  0.663970  0.602004      4.0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23b52d5",
   "metadata": {},
   "source": [
    "#### unfreeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "36273d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_task = [\n",
    "#                 'SarcasmGhosh',\n",
    "                'SARC',\n",
    "    \n",
    "                ] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2129ae2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = MultiTaskTrainDataLoader(selected_task, 32, False, 4)\n",
    "\n",
    "config = config_class.from_pretrained(model_name)\n",
    "tokenizer = tokenizer_class.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8f6e5129",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a52942543c6e493b9aeaa143250c35d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6427 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6427 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1607 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i_epoch': 0, 'train_loss': 0.5204895184541661, 'train_acc': 0.7440722584724426, 'val_loss': 0.5617923329948242, 'val_acc': 0.7100952863693237}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2004021697c4c91a604ecdb9384318b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6427 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6427 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1607 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i_epoch': 1, 'train_loss': 0.4135907814818228, 'train_acc': 0.8152292370796204, 'val_loss': 0.5866683719014871, 'val_acc': 0.7142579555511475}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d8aec73d1a043f7bce25cf936bbab24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6427 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6427 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1607 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i_epoch': 2, 'train_loss': 0.26873430512040186, 'train_acc': 0.8927320837974548, 'val_loss': 0.713898151434713, 'val_acc': 0.7076444029808044}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a388af445ac445d38483d4ed7207084d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6427 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6427 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1607 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i_epoch': 3, 'train_loss': 0.1376631868924303, 'train_acc': 0.9499280452728271, 'val_loss': 0.8818678910805079, 'val_acc': 0.7074499130249023}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "368d2a97f3434c67b4dca3fa421af15a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6427 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6427 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1607 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f43cc4f3d30>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "Exception ignored in: Exception ignored in:     self._shutdown_workers()\n",
      "<function _MultiProcessingDataLoaderIter.__del__ at 0x7f43cc4f3d30>  File \"/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "<function _MultiProcessingDataLoaderIter.__del__ at 0x7f43cc4f3d30>\n",
      "\n",
      "Traceback (most recent call last):\n",
      "    if w.is_alive():\n",
      "  File \"/home/jz17d/anaconda3/envs/torch/lib/python3.9/multiprocessing/process.py\", line 160, in is_alive\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "          File \"/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'self._shutdown_workers()\n",
      "\n",
      "  File \"/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "AssertionError    : if w.is_alive():can only test a child process\n",
      "\n",
      "  File \"/home/jz17d/anaconda3/envs/torch/lib/python3.9/multiprocessing/process.py\", line 160, in is_alive\n",
      "self._shutdown_workers()    \n",
      "assert self._parent_pid == os.getpid(), 'can only test a child process'  File \"/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "\n",
      "    : AssertionErrorcan only test a child processif w.is_alive():\n",
      "\n",
      "  File \"/home/jz17d/anaconda3/envs/torch/lib/python3.9/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i_epoch': 4, 'train_loss': 0.08787503195361171, 'train_acc': 0.9715479016304016, 'val_loss': 0.9523796078389406, 'val_acc': 0.7084224820137024}\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "num_epoch = 5\n",
    "\n",
    "mt_model = MultiTaskBert(config, selected_task).to(device)\n",
    "# for param in mt_model.basemodel.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "optimizer = optim.AdamW(mt_model.parameters(), lr=5e-5)\n",
    "# optimizer = optim.SGD(mt_model.parameters(), lr=0.001)\n",
    "\n",
    "# scheduler = lr_scheduler.LinearLR(optimizer, start_factor=0.01, total_iters=500)\n",
    "scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=5e-5, total_steps=num_epoch*len(train_dataloader)) \n",
    "\n",
    "df = pd.DataFrame(columns=['i_iter', 'train_loss', 'train_acc', 'val_loss', 'val_acc'])\n",
    "\n",
    "for i_epoch in range(num_epoch):\n",
    "    for i_iter, data in enumerate(tqdm(train_dataloader)):  \n",
    "\n",
    "        i_task, batch = data\n",
    "        optimizer.zero_grad()\n",
    "        label = batch['label'].to(device)\n",
    "        del batch['label']\n",
    "        tokens = tokenizer(**batch, return_tensors='pt', padding=True, truncation=True, max_length=64).to(device)\n",
    "        output, loss = mt_model(**tokens, i_task=i_task,  label=label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "#         if i_iter in [0, 10, 25, 50, 100]:\n",
    "#             train_loss, train_overall_acc, train_task_accs = validate(mt_model, 'train')\n",
    "#             val_loss, val_overall_acc, val_task_accs = validate(mt_model, 'val')\n",
    "#             result = {'i_iter':i_iter, 'train_loss':train_loss[0], 'train_acc':train_overall_acc.item(), 'val_loss':val_loss[0], 'val_acc':val_overall_acc.item()}\n",
    "#             df = df.append(result , ignore_index=True)\n",
    "#             print(result)\n",
    "#         if i_iter>100:\n",
    "#             break\n",
    "\n",
    "    train_loss, train_overall_acc, train_task_accs = validate(mt_model, selected_task,'train')\n",
    "    val_loss, val_overall_acc, val_task_accs = validate(mt_model, selected_task, 'val')\n",
    "    result = {'i_epoch':i_epoch, 'train_loss':train_loss[0], 'train_acc':train_overall_acc.item(), 'val_loss':val_loss[0], 'val_acc':val_overall_acc.item()}\n",
    "    df = df.append(result , ignore_index=True)\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e9ae8887",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>i_iter</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>i_epoch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.520490</td>\n",
       "      <td>0.744072</td>\n",
       "      <td>0.561792</td>\n",
       "      <td>0.710095</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.413591</td>\n",
       "      <td>0.815229</td>\n",
       "      <td>0.586668</td>\n",
       "      <td>0.714258</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.268734</td>\n",
       "      <td>0.892732</td>\n",
       "      <td>0.713898</td>\n",
       "      <td>0.707644</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.137663</td>\n",
       "      <td>0.949928</td>\n",
       "      <td>0.881868</td>\n",
       "      <td>0.707450</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.087875</td>\n",
       "      <td>0.971548</td>\n",
       "      <td>0.952380</td>\n",
       "      <td>0.708422</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   i_iter  train_loss  train_acc  val_loss   val_acc  i_epoch\n",
       "0     NaN    0.520490   0.744072  0.561792  0.710095      0.0\n",
       "1     NaN    0.413591   0.815229  0.586668  0.714258      1.0\n",
       "2     NaN    0.268734   0.892732  0.713898  0.707644      2.0\n",
       "3     NaN    0.137663   0.949928  0.881868  0.707450      3.0\n",
       "4     NaN    0.087875   0.971548  0.952380  0.708422      4.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4df04d0",
   "metadata": {},
   "source": [
    "## multi task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21a965a",
   "metadata": {},
   "source": [
    "### ShortHumor + SARC freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fe1f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d67dc12248674931a5bab13d59f6cee7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1876 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##################### settings #####################\n",
    "selected_task = [\n",
    "                'ShortHumor',\n",
    "                'SARC',\n",
    "                ] \n",
    "num_epoch = 5\n",
    "freeze_bert = True\n",
    "limit = 30000 # set the maximum number of data in each dataset \n",
    "####################################################\n",
    "\n",
    "train_dataloader = MultiTaskTrainDataLoader(selected_task, 32, False, 4, limit = limit)\n",
    "config = config_class.from_pretrained(model_name)\n",
    "tokenizer = tokenizer_class.from_pretrained(model_name)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "mt_model = MultiTaskBert(config, selected_task).to(device)\n",
    "if freeze_bert:\n",
    "    for param in mt_model.basemodel.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "optimizer = optim.AdamW(mt_model.parameters(), lr=5e-5)\n",
    "\n",
    "scheduler = get_scheduler(\"linear\",\n",
    "                            optimizer=optimizer,\n",
    "                            num_warmup_steps=500,\n",
    "                            num_training_steps=num_epoch*len(train_dataloader))\n",
    "\n",
    "# create df to save metrics\n",
    "columns = ['i_epoch', 'train_loss'] + [f'train_loss_{selected_task[i]}' for i in range(len(selected_task))]\n",
    "columns += ['train_acc'] + [f'train_acc_{selected_task[i]}' for i in range(len(selected_task))]\n",
    "columns += ['val_loss'] + [f'val_loss{selected_task[i]}' for i in range(len(selected_task))]\n",
    "columns += ['val_acc'] + [f'val_acc{selected_task[i]}' for i in range(len(selected_task))]\n",
    "df = pd.DataFrame(columns=columns)\n",
    "\n",
    "for i_epoch in range(num_epoch):\n",
    "    for i_iter, data in enumerate(tqdm(train_dataloader)):  \n",
    "        i_task, batch = data\n",
    "        optimizer.zero_grad()\n",
    "        label = batch['label'].to(device)\n",
    "        del batch['label']\n",
    "        tokens = tokenizer(**batch, return_tensors='pt', padding=True, truncation=True, max_length=64).to(device)\n",
    "        output, loss = mt_model(**tokens, i_task=i_task,  label=label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "#         if i_iter in [0, 10, 25, 50, 100]:\n",
    "#             train_loss, train_overall_acc, train_task_accs = validate(mt_model, 'train')\n",
    "#             val_loss, val_overall_acc, val_task_accs = validate(mt_model, 'val')\n",
    "#             result = {'i_iter':i_iter, 'train_loss':train_loss[0], 'train_acc':train_overall_acc.item(), 'val_loss':val_loss[0], 'val_acc':val_overall_acc.item()}\n",
    "#             df = df.append(result , ignore_index=True)\n",
    "#             print(result)\n",
    "#         if i_iter>100:\n",
    "#             break\n",
    "    \n",
    "    # run evaluation on train and validation set \n",
    "    train_loss, train_overall_acc, train_task_accs = validate(mt_model, selected_task,'train', limit = limit)\n",
    "    val_loss, val_overall_acc, val_task_accs = validate(mt_model, selected_task, 'val', limit = limit)\n",
    "    \n",
    "    # collect result\n",
    "    result = {'i_epoch':i_epoch, f'train_loss':sum(train_loss.values()), 'train_acc':train_overall_acc.item(), 'val_loss':sum(val_loss.values()), 'val_acc':val_overall_acc.item()}\n",
    "    result.update({f'train_loss_{selected_task[i]}':train_loss[i] for i in train_loss})\n",
    "    result.update({f'train_acc_{selected_task[i]}':train_task_accs[i].item() for i in range(len(train_task_accs))})   \n",
    "    result.update({f'val_loss_{selected_task[i]}':val_loss[i] for i in val_loss})\n",
    "    result.update({f'val_acc_{selected_task[i]}':val_task_accs[i].item() for i in range(len(val_task_accs))})   \n",
    "    df = df.append(result , ignore_index=True)\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c558bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2314c8b",
   "metadata": {},
   "source": [
    "### ShortHumor + SARC unfreeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc5689f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################### settings #####################\n",
    "selected_task = [\n",
    "                'ShortHumor',\n",
    "                'SARC',\n",
    "                ] \n",
    "num_epoch = 5\n",
    "freeze_bert = False\n",
    "limit = 30000 # set the maximum number of data in each dataset \n",
    "####################################################\n",
    "\n",
    "train_dataloader = MultiTaskTrainDataLoader(selected_task, 32, False, 4, limit = limit)\n",
    "config = config_class.from_pretrained(model_name)\n",
    "tokenizer = tokenizer_class.from_pretrained(model_name)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "mt_model = MultiTaskBert(config, selected_task).to(device)\n",
    "if freeze_bert:\n",
    "    for param in mt_model.basemodel.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "optimizer = optim.AdamW(mt_model.parameters(), lr=5e-5)\n",
    "\n",
    "scheduler = get_scheduler(\"linear\",\n",
    "                            optimizer=optimizer,\n",
    "                            num_warmup_steps=500,\n",
    "                            num_training_steps=num_epoch*len(train_dataloader))\n",
    "\n",
    "# create df to save metrics\n",
    "columns = ['i_epoch', 'train_loss'] + [f'train_loss_{selected_task[i]}' for i in range(len(selected_task))]\n",
    "columns += ['train_acc'] + [f'train_acc_{selected_task[i]}' for i in range(len(selected_task))]\n",
    "columns += ['val_loss'] + [f'val_loss{selected_task[i]}' for i in range(len(selected_task))]\n",
    "columns += ['val_acc'] + [f'val_acc{selected_task[i]}' for i in range(len(selected_task))]\n",
    "df = pd.DataFrame(columns=columns)\n",
    "\n",
    "for i_epoch in range(num_epoch):\n",
    "    for i_iter, data in enumerate(tqdm(train_dataloader)):  \n",
    "        i_task, batch = data\n",
    "        optimizer.zero_grad()\n",
    "        label = batch['label'].to(device)\n",
    "        del batch['label']\n",
    "        tokens = tokenizer(**batch, return_tensors='pt', padding=True, truncation=True, max_length=64).to(device)\n",
    "        output, loss = mt_model(**tokens, i_task=i_task,  label=label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "#         if i_iter in [0, 10, 25, 50, 100]:\n",
    "#             train_loss, train_overall_acc, train_task_accs = validate(mt_model, 'train')\n",
    "#             val_loss, val_overall_acc, val_task_accs = validate(mt_model, 'val')\n",
    "#             result = {'i_iter':i_iter, 'train_loss':train_loss[0], 'train_acc':train_overall_acc.item(), 'val_loss':val_loss[0], 'val_acc':val_overall_acc.item()}\n",
    "#             df = df.append(result , ignore_index=True)\n",
    "#             print(result)\n",
    "#         if i_iter>100:\n",
    "#             break\n",
    "    \n",
    "    # run evaluation on train and validation set \n",
    "    train_loss, train_overall_acc, train_task_accs = validate(mt_model, selected_task,'train', limit = limit)\n",
    "    val_loss, val_overall_acc, val_task_accs = validate(mt_model, selected_task, 'val', limit = limit)\n",
    "    \n",
    "    # collect result\n",
    "    result = {'i_epoch':i_epoch, f'train_loss':sum(train_loss.values()), 'train_acc':train_overall_acc.item(), 'val_loss':sum(val_loss.values()), 'val_acc':val_overall_acc.item()}\n",
    "    result.update({{f'train_loss_{selected_task[i]}':train_loss[i] for i in train_loss}})\n",
    "    result.update({f'train_acc_{selected_task[i]}':train_task_accs[i].item() for i in range(len(train_task_accs))})   \n",
    "    result.update({{f'val_loss_{selected_task[i]}':val_loss[i] for i in val_loss}})\n",
    "    result.update({f'val_acc_{selected_task[i]}':val_task_accs[i].item() for i in range(len(val_task_accs))})   \n",
    "    df = df.append(result , ignore_index=True)\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d489cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45d55fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "372.4px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
