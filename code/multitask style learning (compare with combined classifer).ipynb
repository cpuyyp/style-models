{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5eddc9da",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#multi-task-dataloader\" data-toc-modified-id=\"multi-task-dataloader-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>multi-task dataloader</a></span></li><li><span><a href=\"#multi-task-model\" data-toc-modified-id=\"multi-task-model-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>multi-task model</a></span></li><li><span><a href=\"#train\" data-toc-modified-id=\"train-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>train</a></span></li><li><span><a href=\"#check-model-output\" data-toc-modified-id=\"check-model-output-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>check model output</a></span></li><li><span><a href=\"#check-bert-output\" data-toc-modified-id=\"check-bert-output-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>check bert output</a></span></li><li><span><a href=\"#senteval\" data-toc-modified-id=\"senteval-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>senteval</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "785f4cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torchmetrics\n",
    "import numpy as np\n",
    "import collections\n",
    "import json\n",
    "from tqdm.auto import tqdm, trange\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoConfig, AutoTokenizer, BertModel, RobertaModel\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29182ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/huggingface/transformers/issues/5486\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "186facb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8839076",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CrowdFlower': 13,\n",
       " 'DailyDialog': 7,\n",
       " 'EmoBank_Valence': 1,\n",
       " 'EmoBank_Arousal': 1,\n",
       " 'EmoBank_Dominance': 1,\n",
       " 'HateOffensive': 3,\n",
       " 'PASTEL_age': 8,\n",
       " 'PASTEL_country': 2,\n",
       " 'PASTEL_education': 10,\n",
       " 'PASTEL_ethnic': 10,\n",
       " 'PASTEL_gender': 3,\n",
       " 'PASTEL_politics': 3,\n",
       " 'PASTEL_tod': 5,\n",
       " 'SARC': 2,\n",
       " 'SarcasmGhosh': 2,\n",
       " 'SentiTreeBank': 1,\n",
       " 'ShortHumor': 2,\n",
       " 'ShortJokeKaggle': 2,\n",
       " 'ShortRomance': 2,\n",
       " 'StanfordPoliteness': 1,\n",
       " 'TroFi': 2,\n",
       " 'VUA': 2}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../data/xslue/tasks.json', 'r') as f:\n",
    "    tasks = json.load(f)\n",
    "tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76efeb82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CrowdFlower': 0,\n",
       " 'DailyDialog': 1,\n",
       " 'EmoBank_Valence': 2,\n",
       " 'EmoBank_Arousal': 3,\n",
       " 'EmoBank_Dominance': 4,\n",
       " 'HateOffensive': 5,\n",
       " 'PASTEL_age': 6,\n",
       " 'PASTEL_country': 7,\n",
       " 'PASTEL_education': 8,\n",
       " 'PASTEL_ethnic': 9,\n",
       " 'PASTEL_gender': 10,\n",
       " 'PASTEL_politics': 11,\n",
       " 'PASTEL_tod': 12,\n",
       " 'SARC': 13,\n",
       " 'SarcasmGhosh': 14,\n",
       " 'SentiTreeBank': 15,\n",
       " 'ShortHumor': 16,\n",
       " 'ShortJokeKaggle': 17,\n",
       " 'ShortRomance': 18,\n",
       " 'StanfordPoliteness': 19,\n",
       " 'TroFi': 20,\n",
       " 'VUA': 21}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tasks2idx = {}\n",
    "for i,k in enumerate(tasks):\n",
    "    tasks2idx[k] = i\n",
    "tasks2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d23f160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# task and their (train) dataset size \n",
    "selected_task = ['PASTEL_country', # 33224\n",
    "#                  'SARC', # 205645\n",
    "                 'SarcasmGhosh', # 39780\n",
    "                 'ShortHumor', # 37801\n",
    "#                  'ShortJokeKaggle', # 406682\n",
    "#                  'ShortRomance', # 1902\n",
    "#                  'TroFi', # 3335\n",
    "                 'VUA', # 15157\n",
    "                ] \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318bd186",
   "metadata": {},
   "source": [
    "# multi-task dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00309940",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset): \n",
    "    # currently it's a Mapping-style dataset. Not sure if a Iterable-style dataset will be better\n",
    "    def __init__(self, tsv_file):\n",
    "        self.df = pd.read_csv(tsv_file, sep='\\t')\n",
    "        self.df = self.df.dropna()\n",
    "        self.df = self.df.reset_index(drop=True)\n",
    "        if self.df['label'].dtype == 'float64':\n",
    "            self.df['label'] = self.df['label'].astype('float32')\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        dataslice = self.df.iloc[idx]\n",
    "        sample = {'text':dataslice['text'], 'label':dataslice['label']}\n",
    "        return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6463e817",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskTrainDataLoader():\n",
    "    '''\n",
    "    Each time, a random integer selects a dataset and load a batch of data {text, label} from it. Return i_task and data\n",
    "    \n",
    "    a iterator\n",
    "    Known issue: large dataset may have not iterate once, small datasets may have been iterated many times\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, tasks, batch_size, shuffle, num_workers):\n",
    "        self.tasks = tasks\n",
    "        self.split = 'train'\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.num_workers = num_workers\n",
    "        \n",
    "        self.num_tasks = len(tasks)\n",
    "        self.datasets = []\n",
    "        self.dataloaders = []\n",
    "        self.dataloaderiters = []\n",
    "        self.len = 0\n",
    "        for task in tasks:\n",
    "            self.datasets.append(MyDataset(f'../data/xslue/processed/{self.split}/{task}.tsv'))\n",
    "            self.dataloaders.append(DataLoader(self.datasets[-1], batch_size=self.batch_size, shuffle=self.shuffle, num_workers=self.num_workers)) \n",
    "            self.dataloaderiters.append(self.dataloaders[-1]._get_iterator())\n",
    "            self.len += len(self.dataloaders[-1])\n",
    "    def __len__(self):   \n",
    "        return self.len\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.n = 0\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        i_task = np.random.randint(self.num_tasks)\n",
    "        if self.n < self.len:\n",
    "            self.n += 1\n",
    "        else:\n",
    "            raise StopIteration\n",
    "        dataloaderiter = self.dataloaderiters[i_task]    \n",
    "        try: \n",
    "            batch = next(dataloaderiter)\n",
    "        except StopIteration:\n",
    "#             self.dataloaderiters[i_task]._reset(self.dataloaders[i_task])\n",
    "#             dataloaderiter = self.dataloaderiters[i_task]\n",
    "            self.dataloaderiters[i_task] = iter(self.dataloaders[i_task])\n",
    "            dataloaderiter = self.dataloaderiters[i_task]\n",
    "            batch = next(dataloaderiter)\n",
    "        return i_task, batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61019a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskTestDataLoader():\n",
    "    '''\n",
    "    For dev and test\n",
    "    \n",
    "    a generator\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, tasks, split, batch_size, shuffle, num_workers):\n",
    "        assert split in ['dev', 'test'], 'not implemented'\n",
    "        self.tasks = tasks\n",
    "        self.split = split\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.num_workers = num_workers\n",
    "        \n",
    "        self.num_tasks = len(tasks)\n",
    "        self.datasets = []\n",
    "        self.dataloaders = []\n",
    "        self.dataloaderiters = []\n",
    "        self.len = 0\n",
    "        for task in tasks:\n",
    "            self.datasets.append(MyDataset(f'../data/xslue/processed/{self.split}/{task}.tsv'))\n",
    "            self.dataloaders.append(DataLoader(self.datasets[-1], batch_size=self.batch_size, shuffle=self.shuffle, num_workers=self.num_workers))\n",
    "            self.len += len(self.dataloaders[-1])\n",
    "        self.i_task = 0\n",
    "    def __len__(self):   \n",
    "        return self.len\n",
    "\n",
    "    def __iter__(self):\n",
    "        for i_task in range(self.num_tasks):\n",
    "            dataloader = self.dataloaders[i_task]\n",
    "            for batch in dataloader:\n",
    "                yield i_task, batch\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100a28b2",
   "metadata": {},
   "source": [
    "# multi-task model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9c05444",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.bert.modeling_bert import BertPreTrainedModel\n",
    "from transformers.models.roberta.modeling_roberta import RobertaPreTrainedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ed50b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionHead(nn.Module):\n",
    "    def __init__(self, embedding_dim = 768, hidden_dim = 128):\n",
    "        super().__init__()\n",
    "#         self.hidden1 = nn.Linear(embedding_dim, hidden_dim)\n",
    "#         self.hidden2 = nn.Linear(hidden_dim, 1)\n",
    "        self.hidden = nn.Linear(embedding_dim, 1)\n",
    "        self.activation = nn.Sigmoid()\n",
    "        \n",
    "        self.loss_fn = nn.MSELoss()\n",
    "    def forward(self, sent_emb, label):\n",
    "#         output = self.activation(self.hidden2(self.hidden1(sent_emb))).squeeze(1)\n",
    "        output = self.activation(self.hidden(sent_emb)).squeeze(1)\n",
    "        \n",
    "        loss = self.loss_fn(output, label)\n",
    "        return output, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c097a3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationHead(nn.Module):\n",
    "    def __init__(self, num_labels, embedding_dim = 768, hidden_dim = 128):\n",
    "        super().__init__()\n",
    "#         self.hidden1 = nn.Linear(embedding_dim, hidden_dim)\n",
    "#         self.hidden2 = nn.Linear(hidden_dim, num_labels)\n",
    "        self.hidden = nn.Linear(embedding_dim, num_labels)\n",
    "        self.activation = nn.Softmax(dim=1)\n",
    "        \n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "    def forward(self, sent_emb, label):\n",
    "#         output = self.activation(self.hidden2(self.hidden1(sent_emb)))\n",
    "        output = self.activation(self.hidden(sent_emb))\n",
    "        \n",
    "        loss = self.loss_fn(output, label)\n",
    "        return output, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4eec4f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskBert(BertPreTrainedModel):\n",
    "    def __init__(self, config, selected_task, use_pooler=True):\n",
    "        super().__init__(config)\n",
    "        self.use_pooler = use_pooler\n",
    "        self.basemodel = BertModel(config)\n",
    "        self.style_heads = nn.ModuleList()\n",
    "        for task in selected_task:\n",
    "            if tasks[task] == 1:\n",
    "                self.style_heads.append(RegressionHead())\n",
    "            else:\n",
    "                self.style_heads.append(ClassificationHead(tasks[task]))\n",
    "    def forward(self, input_ids, token_type_ids, attention_mask, i_task=None, label=None, return_sent_emb=False):\n",
    "        output = self.basemodel(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)\n",
    "        if self.use_pooler:\n",
    "            sent_emb = output['pooler_output']\n",
    "        else:\n",
    "            sent_emb = output['last_hidden_state'][:,0,:]\n",
    "        \n",
    "        if return_sent_emb:\n",
    "            return sent_emb  \n",
    "        output, loss = self.style_heads[i_task](sent_emb, label)\n",
    "        return output, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "605330aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskRoberta(RobertaPreTrainedModel):\n",
    "    def __init__(self, config, selected_task, use_pooler=True):\n",
    "        super().__init__(config)\n",
    "        self.use_pooler = use_pooler\n",
    "        self.basemodel = RobertaModel(config)\n",
    "        self.style_heads = nn.ModuleList()\n",
    "        for task in selected_task:\n",
    "            if tasks[task] == 1:\n",
    "                self.style_heads.append(RegressionHead())\n",
    "            else:\n",
    "                self.style_heads.append(ClassificationHead(tasks[task]))\n",
    "    def forward(self, input_ids, attention_mask, i_task=None, label=None, return_sent_emb=False):\n",
    "        output = self.basemodel(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        if self.use_pooler:\n",
    "            sent_emb = output['pooler_output']\n",
    "        else:\n",
    "            sent_emb = output['last_hidden_state'][:,0,:]\n",
    "        if return_sent_emb:\n",
    "            return sent_emb\n",
    "        \n",
    "        output, loss = self.style_heads[i_task](sent_emb, label)\n",
    "        \n",
    "        return output, loss, sent_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6acf874a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_loss(losses):\n",
    "    for k in losses:\n",
    "        print(f'{losses[k]:4.4f}', end=' ')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "07c0cc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(mt_val_dataloader):\n",
    "    val_loss = collections.defaultdict(float)\n",
    "    val_size = collections.defaultdict(int)\n",
    "    acc = torchmetrics.Accuracy() # todo\n",
    "    mt_model.eval()\n",
    "    for data in tqdm(mt_val_dataloader):  \n",
    "        i_task, batch = data\n",
    "        label = batch['label'].to(device)\n",
    "        size = len(label)\n",
    "        del batch['label']\n",
    "        tokens = tokenizer(**batch, return_tensors='pt', padding=True, truncation=True, max_length=64).to(device)\n",
    "        output, loss = mt_model(**tokens, i_task=i_task,  label=label)\n",
    "#         acc.update(preds, target)\n",
    "        tokens = None\n",
    "        output = None\n",
    "        val_loss[i_task] += loss.detach().item()*size\n",
    "        val_size[i_task] += size\n",
    "    for i_task in val_loss:\n",
    "        val_loss[i_task] /= val_size[i_task]\n",
    "    mt_model.train()\n",
    "\n",
    "    return val_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d73059",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4e1fda3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# larger batch_size will definitely lead to memory issue\n",
    "mt_dataloader = MultiTaskTrainDataLoader(selected_task, batch_size = 16, shuffle = True, num_workers = 4)\n",
    "mt_dev_dataloader = MultiTaskTestDataLoader(selected_task, split='dev', batch_size = 16, shuffle = True, num_workers = 4)\n",
    "mt_test_dataloader = MultiTaskTestDataLoader(selected_task, split='test', batch_size = 16, shuffle = True, num_workers = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4ec272dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = \"bert-base-uncased\"\n",
    "# base_model = 'roberta-base'\n",
    "\n",
    "config = AutoConfig.from_pretrained(base_model)\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "\n",
    "mt_model = MultiTaskBert(config, selected_task, use_pooler=False).to(device)\n",
    "# mt_model = MultiTaskRoberta(config, tasks).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a516c64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in mt_model.basemodel.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "884c578c",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(mt_model.parameters(), lr=0.001)\n",
    "scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=0.001, total_steps=len(mt_dataloader)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "654eadc1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ef16354ab574525982d4dae7a5403f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31370d2596384b6f877551b5bfac7f52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/586 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####training iter 300/7875\n",
      "0.3372 0.6045 0.6850 0.5837 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dfdf67911f942d98bcbff513b0d9931",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/586 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####training iter 600/7875\n",
      "0.3370 0.5008 0.7140 0.5840 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b63a221a490445c0a6af157157ade260",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/586 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####training iter 900/7875\n",
      "0.3369 0.4808 0.6568 0.5837 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79db386f71a04d5fb541336548dd1622",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/586 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####training iter 1200/7875\n",
      "0.3369 1.2763 0.8150 0.5837 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55497352e86946c9aa70840b9ee0111f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/586 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####training iter 1500/7875\n",
      "0.3369 1.2763 0.8150 0.5837 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_355187/3626777905.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0macc_objs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi_task\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mlosses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi_task\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/torchmetrics/metric.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    265\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_called\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_enable_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/torchmetrics/classification/accuracy.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, preds, target)\u001b[0m\n\u001b[1;32m    226\u001b[0m         \"\"\"\n\u001b[1;32m    227\u001b[0m         \u001b[0;34m\"\"\" returns the mode of the data (binary, multi label, multi class, multi-dim multi class) \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m         \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtop_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmulticlass\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/torchmetrics/functional/classification/accuracy.py\u001b[0m in \u001b[0;36m_mode\u001b[0;34m(preds, target, threshold, top_k, num_classes, multiclass)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \"\"\"\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     mode = _check_classification_inputs(\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtop_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulticlass\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmulticlass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     )\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/torchmetrics/utilities/checks.py\u001b[0m in \u001b[0;36m_check_classification_inputs\u001b[0;34m(preds, target, threshold, num_classes, multiclass, top_k)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;31m# Basic validation (that does not need case/type information)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     \u001b[0m_basic_input_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulticlass\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m     \u001b[0;31m# Check that shape/types fall into one of the cases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/torchmetrics/utilities/checks.py\u001b[0m in \u001b[0;36m_basic_input_validation\u001b[0;34m(preds, target, threshold, multiclass)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The `target` has to be an integer tensor.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The `target` has to be a non-negative tensor.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "losses = collections.defaultdict(list)\n",
    "df_dev_loss = pd.DataFrame(columns=np.arange(0,len(selected_task)))\n",
    "acc_objs = [torchmetrics.Accuracy() for i in range(len(selected_task))]\n",
    "\n",
    "accs = collections.defaultdict(list) # collect per 300 steps\n",
    "\n",
    "test_embs = []\n",
    "\n",
    "for i_iter, data in enumerate(tqdm(mt_dataloader)):  \n",
    "    if i_iter == 1000:\n",
    "        for param in mt_model.basemodel.parameters():\n",
    "            param.requires_grad = True\n",
    "    i_task, batch = data\n",
    "    optimizer.zero_grad()\n",
    "    label = batch['label'].to(device)\n",
    "    del batch['label']\n",
    "    tokens = tokenizer(**batch, return_tensors='pt', padding=True, truncation=True, max_length=64).to(device)\n",
    "    output, loss = mt_model(**tokens, i_task=i_task,  label=label)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    acc_objs[i_task].update(output.to('cpu').detach(), label.to('cpu').detach())\n",
    "    \n",
    "    losses[i_task].append(loss.detach().item())\n",
    "    \n",
    "    if i_iter in [0,10,100,200,210,300,400,500]:\n",
    "        sent_emb = mt_model(**tokens, i_task=i_task,  label=label, return_sent_emb=True)\n",
    "        test_embs.append(sent_emb)\n",
    "        \n",
    "#     if i_iter > 1500:\n",
    "#         break\n",
    "\n",
    "    if i_iter%300 == 0 and i_iter != 0:\n",
    "        for i_task in range(4):\n",
    "            accs[i_task].append(acc_objs[i_task].compute().detach().item())\n",
    "        acc_objs = [torchmetrics.Accuracy() for i in range(len(selected_task))]\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        dev_loss = validate(mt_dev_dataloader)\n",
    "        df_dev_loss = df_dev_loss.append(dev_loss , ignore_index=True)\n",
    "        print(f'#####training iter {i_iter}/{len(mt_dataloader)}')\n",
    "        print_loss(dev_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6f9c32f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {0: [0.9794303774833679,\n",
       "              0.9725000262260437,\n",
       "              0.9756944179534912,\n",
       "              0.9754746556282043,\n",
       "              0.9795082211494446],\n",
       "             1: [0.5719085931777954,\n",
       "              0.6089527010917664,\n",
       "              0.663281261920929,\n",
       "              0.5308098793029785,\n",
       "              0.45892858505249023],\n",
       "             2: [0.5615234375,\n",
       "              0.5859375,\n",
       "              0.5838607549667358,\n",
       "              0.5496794581413269,\n",
       "              0.5157967209815979],\n",
       "             3: [0.7038461565971375,\n",
       "              0.7053571343421936,\n",
       "              0.7273550629615784,\n",
       "              0.7196180820465088,\n",
       "              0.7259615659713745]})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "aae1ad9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "accc = torchmetrics.Accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1b6639a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "accc.update(output.to('cpu').detach(), label.to('cpu').detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0f1b982d",
   "metadata": {},
   "outputs": [],
   "source": [
    "accc.update(torch.Tensor([1.,1,1,0,0]), torch.LongTensor([1,1,1,0,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8cb32427",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6250)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accc.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "859407b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PASTEL_country</th>\n",
       "      <th>SarcasmGhosh</th>\n",
       "      <th>ShortHumor</th>\n",
       "      <th>VUA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.336860</td>\n",
       "      <td>1.276343</td>\n",
       "      <td>0.811491</td>\n",
       "      <td>0.583713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.336859</td>\n",
       "      <td>1.276343</td>\n",
       "      <td>0.811497</td>\n",
       "      <td>0.583713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.336859</td>\n",
       "      <td>1.276343</td>\n",
       "      <td>0.811497</td>\n",
       "      <td>0.583713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.289664</td>\n",
       "      <td>0.350181</td>\n",
       "      <td>0.811497</td>\n",
       "      <td>0.583714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.289664</td>\n",
       "      <td>0.350181</td>\n",
       "      <td>0.811497</td>\n",
       "      <td>0.583714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.289664</td>\n",
       "      <td>0.350181</td>\n",
       "      <td>0.811497</td>\n",
       "      <td>0.583714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.289664</td>\n",
       "      <td>0.350181</td>\n",
       "      <td>0.811497</td>\n",
       "      <td>0.583714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.289664</td>\n",
       "      <td>0.350181</td>\n",
       "      <td>0.811497</td>\n",
       "      <td>0.583714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.289664</td>\n",
       "      <td>0.350181</td>\n",
       "      <td>0.811497</td>\n",
       "      <td>0.583714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.289664</td>\n",
       "      <td>0.350181</td>\n",
       "      <td>0.811497</td>\n",
       "      <td>0.583714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.289664</td>\n",
       "      <td>0.350181</td>\n",
       "      <td>0.811497</td>\n",
       "      <td>0.583714</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    PASTEL_country  SarcasmGhosh  ShortHumor       VUA\n",
       "0         0.336860      1.276343    0.811491  0.583713\n",
       "1         0.336859      1.276343    0.811497  0.583713\n",
       "2         0.336859      1.276343    0.811497  0.583713\n",
       "3         1.289664      0.350181    0.811497  0.583714\n",
       "4         1.289664      0.350181    0.811497  0.583714\n",
       "5         1.289664      0.350181    0.811497  0.583714\n",
       "6         1.289664      0.350181    0.811497  0.583714\n",
       "7         1.289664      0.350181    0.811497  0.583714\n",
       "8         1.289664      0.350181    0.811497  0.583714\n",
       "9         1.289664      0.350181    0.811497  0.583714\n",
       "10        1.289664      0.350181    0.811497  0.583714"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev_loss.columns = selected_task\n",
    "df_dev_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb215de",
   "metadata": {},
   "source": [
    "# check model output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5d99a925",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5625,\n",
       " 0.4375,\n",
       " 0.5,\n",
       " 0.4375,\n",
       " 0.6875,\n",
       " 0.5625,\n",
       " 0.625,\n",
       " 0.9375,\n",
       " 0.5625,\n",
       " 0.5625,\n",
       " 0.8125,\n",
       " 0.6875,\n",
       " 0.8125,\n",
       " 0.6875,\n",
       " 0.75,\n",
       " 0.8125,\n",
       " 0.8125,\n",
       " 0.8125,\n",
       " 0.75,\n",
       " 0.5625,\n",
       " 0.6875,\n",
       " 0.625,\n",
       " 0.75,\n",
       " 0.6875,\n",
       " 0.6875,\n",
       " 0.625,\n",
       " 0.75,\n",
       " 0.6875,\n",
       " 0.5625,\n",
       " 0.6875,\n",
       " 0.9375,\n",
       " 0.75,\n",
       " 0.75,\n",
       " 0.75,\n",
       " 0.6875,\n",
       " 0.5625,\n",
       " 0.625,\n",
       " 0.6875,\n",
       " 0.625,\n",
       " 0.9375,\n",
       " 0.625,\n",
       " 0.6875,\n",
       " 0.6875,\n",
       " 0.75,\n",
       " 0.6875,\n",
       " 0.6875,\n",
       " 0.75,\n",
       " 0.6875,\n",
       " 0.6875,\n",
       " 0.6875,\n",
       " 0.75,\n",
       " 0.8125,\n",
       " 0.75,\n",
       " 0.8125,\n",
       " 0.625,\n",
       " 0.6875,\n",
       " 0.75,\n",
       " 0.6875,\n",
       " 0.75,\n",
       " 0.75,\n",
       " 0.6875,\n",
       " 0.75,\n",
       " 0.6875,\n",
       " 0.8125,\n",
       " 0.6875,\n",
       " 0.5625,\n",
       " 0.75,\n",
       " 0.6875,\n",
       " 0.8125,\n",
       " 0.5,\n",
       " 0.875,\n",
       " 0.75,\n",
       " 0.625,\n",
       " 0.75,\n",
       " 0.75,\n",
       " 0.6875,\n",
       " 0.6875,\n",
       " 0.8125,\n",
       " 0.75,\n",
       " 0.8125,\n",
       " 0.75,\n",
       " 0.6875,\n",
       " 0.75,\n",
       " 0.75,\n",
       " 0.6875,\n",
       " 0.8125,\n",
       " 0.6875,\n",
       " 0.75,\n",
       " 0.6875,\n",
       " 0.6875,\n",
       " 0.75,\n",
       " 0.625,\n",
       " 0.875,\n",
       " 0.75,\n",
       " 0.8125,\n",
       " 0.5625,\n",
       " 0.625,\n",
       " 0.6875,\n",
       " 0.75,\n",
       " 0.6875,\n",
       " 0.9375,\n",
       " 0.5625,\n",
       " 0.6875,\n",
       " 0.8125,\n",
       " 0.625,\n",
       " 0.6875,\n",
       " 0.8125,\n",
       " 0.6875,\n",
       " 1.0,\n",
       " 0.5,\n",
       " 0.8125,\n",
       " 0.8125,\n",
       " 0.8125,\n",
       " 0.75,\n",
       " 0.75,\n",
       " 0.9375,\n",
       " 0.8125,\n",
       " 0.8125,\n",
       " 0.5,\n",
       " 0.6875,\n",
       " 0.6875,\n",
       " 0.8125,\n",
       " 0.8125,\n",
       " 0.8125,\n",
       " 0.8125,\n",
       " 0.5625,\n",
       " 0.75,\n",
       " 0.8125,\n",
       " 0.5,\n",
       " 0.875,\n",
       " 0.6875,\n",
       " 0.6875,\n",
       " 0.875,\n",
       " 0.625,\n",
       " 0.8125,\n",
       " 0.6875,\n",
       " 0.6875,\n",
       " 0.6875,\n",
       " 0.75,\n",
       " 0.8125,\n",
       " 0.6875,\n",
       " 0.6875,\n",
       " 0.5,\n",
       " 0.6875,\n",
       " 0.8125,\n",
       " 0.625,\n",
       " 0.8125,\n",
       " 0.625,\n",
       " 0.6875,\n",
       " 0.875,\n",
       " 0.9375,\n",
       " 0.625,\n",
       " 0.5,\n",
       " 0.6875,\n",
       " 0.75,\n",
       " 0.8125,\n",
       " 0.8125,\n",
       " 0.75,\n",
       " 0.6875,\n",
       " 0.625,\n",
       " 0.625,\n",
       " 0.6875,\n",
       " 0.875,\n",
       " 0.625,\n",
       " 0.5625,\n",
       " 0.8125,\n",
       " 0.875,\n",
       " 0.6875,\n",
       " 0.875,\n",
       " 0.75,\n",
       " 0.5625,\n",
       " 0.5625,\n",
       " 0.6875,\n",
       " 0.6875,\n",
       " 0.9375,\n",
       " 0.6875,\n",
       " 0.8125,\n",
       " 0.75,\n",
       " 0.6875,\n",
       " 0.5625,\n",
       " 0.875,\n",
       " 0.625,\n",
       " 0.75,\n",
       " 0.875,\n",
       " 0.8125,\n",
       " 0.9375,\n",
       " 0.6875,\n",
       " 0.8125,\n",
       " 0.6875,\n",
       " 0.5,\n",
       " 0.6875,\n",
       " 0.75,\n",
       " 0.6875,\n",
       " 0.625,\n",
       " 0.75,\n",
       " 0.875,\n",
       " 0.6875,\n",
       " 0.8125,\n",
       " 0.6875,\n",
       " 0.625,\n",
       " 0.75,\n",
       " 0.75,\n",
       " 0.5625,\n",
       " 0.8125,\n",
       " 0.8125,\n",
       " 0.5625,\n",
       " 0.6875,\n",
       " 0.625,\n",
       " 0.75,\n",
       " 0.875,\n",
       " 0.75,\n",
       " 0.8125,\n",
       " 0.8125,\n",
       " 0.6875,\n",
       " 0.75,\n",
       " 0.75,\n",
       " 0.5625,\n",
       " 0.75,\n",
       " 0.625,\n",
       " 0.5625,\n",
       " 0.8125,\n",
       " 0.6875,\n",
       " 0.875,\n",
       " 0.8125,\n",
       " 0.625,\n",
       " 0.75,\n",
       " 0.75,\n",
       " 0.6875,\n",
       " 0.625,\n",
       " 0.6875,\n",
       " 0.6875,\n",
       " 0.5,\n",
       " 0.875,\n",
       " 0.6875,\n",
       " 0.625,\n",
       " 0.6875,\n",
       " 0.75,\n",
       " 0.6875,\n",
       " 0.6875,\n",
       " 0.8125,\n",
       " 0.8125,\n",
       " 0.75,\n",
       " 0.8125,\n",
       " 0.5625,\n",
       " 0.5625,\n",
       " 0.75,\n",
       " 0.8125,\n",
       " 0.6875,\n",
       " 0.75,\n",
       " 0.5,\n",
       " 0.8125,\n",
       " 0.625,\n",
       " 0.75,\n",
       " 0.6875,\n",
       " 0.75,\n",
       " 0.75,\n",
       " 0.5625,\n",
       " 0.625,\n",
       " 0.5,\n",
       " 0.8125,\n",
       " 0.6875,\n",
       " 0.8125,\n",
       " 0.875,\n",
       " 0.8125,\n",
       " 0.4375,\n",
       " 0.625,\n",
       " 0.6875,\n",
       " 0.6875,\n",
       " 0.6875,\n",
       " 0.75]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accs[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "432c07b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9551)\n",
      "tensor(0.5574)\n",
      "tensor(0.5461)\n",
      "tensor(0.7150)\n"
     ]
    }
   ],
   "source": [
    "for acc in acc_objs:\n",
    "    print(acc.compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1e4ffd09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9789)\n",
      "tensor(0.5158)\n",
      "tensor(0.4825)\n",
      "tensor(0.7243)\n"
     ]
    }
   ],
   "source": [
    "for acc in acc_objs:\n",
    "    print(acc.compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d422c204",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiTaskBert(\n",
       "  (basemodel): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (style_heads): ModuleList(\n",
       "    (0): ClassificationHead(\n",
       "      (hidden): Linear(in_features=768, out_features=2, bias=True)\n",
       "      (activation): Softmax(dim=1)\n",
       "      (loss_fn): CrossEntropyLoss()\n",
       "    )\n",
       "    (1): ClassificationHead(\n",
       "      (hidden): Linear(in_features=768, out_features=2, bias=True)\n",
       "      (activation): Softmax(dim=1)\n",
       "      (loss_fn): CrossEntropyLoss()\n",
       "    )\n",
       "    (2): ClassificationHead(\n",
       "      (hidden): Linear(in_features=768, out_features=2, bias=True)\n",
       "      (activation): Softmax(dim=1)\n",
       "      (loss_fn): CrossEntropyLoss()\n",
       "    )\n",
       "    (3): ClassificationHead(\n",
       "      (hidden): Linear(in_features=768, out_features=2, bias=True)\n",
       "      (activation): Softmax(dim=1)\n",
       "      (loss_fn): CrossEntropyLoss()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mt_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dd2c16f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits, loss = mt_model(**tokens, i_task=i_task,  label=label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5515308b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1063, 0.8937],\n",
       "        [0.1063, 0.8937],\n",
       "        [0.1063, 0.8937],\n",
       "        [0.1063, 0.8937],\n",
       "        [0.1063, 0.8937],\n",
       "        [0.1063, 0.8937],\n",
       "        [0.1063, 0.8937],\n",
       "        [0.1063, 0.8937],\n",
       "        [0.1063, 0.8937],\n",
       "        [0.1063, 0.8937],\n",
       "        [0.1063, 0.8937],\n",
       "        [0.1063, 0.8937],\n",
       "        [0.1063, 0.8937],\n",
       "        [0.1063, 0.8937],\n",
       "        [0.1063, 0.8937],\n",
       "        [0.1063, 0.8937]], device='cuda:0', grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5ec1096b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0], device='cuda:0')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "286f34e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8756, device='cuda:0', grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.CrossEntropyLoss()(logits, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c31c0ce9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8756, device='cuda:0', grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7fdfca",
   "metadata": {},
   "source": [
    "# check bert output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "92330d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = mt_model.basemodel(**tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "eb812a5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.1633, -0.6806,  0.7816,  ..., -0.4631, -0.7018, -1.1974],\n",
       "         [-0.1623, -0.6788,  0.7804,  ..., -0.4576, -0.7000, -1.1939],\n",
       "         [-0.1627, -0.6795,  0.7822,  ..., -0.4595, -0.6993, -1.1916],\n",
       "         ...,\n",
       "         [-0.1626, -0.6791,  0.7818,  ..., -0.4588, -0.6999, -1.1938],\n",
       "         [-0.1611, -0.6782,  0.7817,  ..., -0.4582, -0.6971, -1.1924],\n",
       "         [-0.1617, -0.6792,  0.7808,  ..., -0.4583, -0.6982, -1.1930]],\n",
       "\n",
       "        [[-0.1632, -0.6810,  0.7818,  ..., -0.4628, -0.7019, -1.1979],\n",
       "         [-0.1630, -0.6798,  0.7813,  ..., -0.4565, -0.6979, -1.1942],\n",
       "         [-0.1622, -0.6793,  0.7822,  ..., -0.4578, -0.7010, -1.1931],\n",
       "         ...,\n",
       "         [-0.1626, -0.6796,  0.7821,  ..., -0.4586, -0.7001, -1.1943],\n",
       "         [-0.1610, -0.6787,  0.7821,  ..., -0.4581, -0.6973, -1.1930],\n",
       "         [-0.1617, -0.6798,  0.7812,  ..., -0.4581, -0.6984, -1.1936]],\n",
       "\n",
       "        [[-0.1632, -0.6806,  0.7818,  ..., -0.4629, -0.7018, -1.1975],\n",
       "         [-0.1617, -0.6802,  0.7802,  ..., -0.4571, -0.6989, -1.1929],\n",
       "         [-0.1635, -0.6781,  0.7810,  ..., -0.4574, -0.6978, -1.1918],\n",
       "         ...,\n",
       "         [-0.1631, -0.6792,  0.7803,  ..., -0.4574, -0.6984, -1.1931],\n",
       "         [-0.1623, -0.6785,  0.7810,  ..., -0.4577, -0.6956, -1.1918],\n",
       "         [-0.1620, -0.6790,  0.7801,  ..., -0.4569, -0.6975, -1.1925]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.1634, -0.6808,  0.7818,  ..., -0.4629, -0.7016, -1.1979],\n",
       "         [-0.1622, -0.6788,  0.7820,  ..., -0.4583, -0.7002, -1.1946],\n",
       "         [-0.1624, -0.6793,  0.7820,  ..., -0.4585, -0.7010, -1.1927],\n",
       "         ...,\n",
       "         [-0.1628, -0.6794,  0.7821,  ..., -0.4586, -0.6998, -1.1944],\n",
       "         [-0.1612, -0.6785,  0.7821,  ..., -0.4581, -0.6971, -1.1930],\n",
       "         [-0.1619, -0.6796,  0.7811,  ..., -0.4581, -0.6982, -1.1936]],\n",
       "\n",
       "        [[-0.1633, -0.6806,  0.7818,  ..., -0.4630, -0.7018, -1.1977],\n",
       "         [-0.1628, -0.6805,  0.7827,  ..., -0.4563, -0.7002, -1.1952],\n",
       "         [-0.1617, -0.6806,  0.7813,  ..., -0.4577, -0.7003, -1.1929],\n",
       "         ...,\n",
       "         [-0.1627, -0.6792,  0.7821,  ..., -0.4587, -0.6999, -1.1941],\n",
       "         [-0.1611, -0.6783,  0.7820,  ..., -0.4581, -0.6971, -1.1927],\n",
       "         [-0.1617, -0.6794,  0.7811,  ..., -0.4582, -0.6982, -1.1933]],\n",
       "\n",
       "        [[-0.1635, -0.6816,  0.7820,  ..., -0.4629, -0.7017, -1.1979],\n",
       "         [-0.1616, -0.6802,  0.7817,  ..., -0.4581, -0.6987, -1.1937],\n",
       "         [-0.1626, -0.6798,  0.7810,  ..., -0.4570, -0.6993, -1.1917],\n",
       "         ...,\n",
       "         [-0.1627, -0.6802,  0.7824,  ..., -0.4586, -0.6998, -1.1942],\n",
       "         [-0.1611, -0.6793,  0.7823,  ..., -0.4581, -0.6971, -1.1928],\n",
       "         [-0.1618, -0.6804,  0.7813,  ..., -0.4580, -0.6982, -1.1935]]],\n",
       "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[ 0.2139,  0.3472,  0.4103,  ...,  0.2303, -0.8231, -0.0569],\n",
       "        [ 0.2141,  0.3471,  0.4101,  ...,  0.2304, -0.8231, -0.0570],\n",
       "        [ 0.2139,  0.3471,  0.4102,  ...,  0.2304, -0.8231, -0.0569],\n",
       "        ...,\n",
       "        [ 0.2138,  0.3471,  0.4101,  ...,  0.2305, -0.8231, -0.0570],\n",
       "        [ 0.2138,  0.3472,  0.4103,  ...,  0.2304, -0.8232, -0.0568],\n",
       "        [ 0.2137,  0.3472,  0.4103,  ...,  0.2305, -0.8231, -0.0571]],\n",
       "       device='cuda:0', grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8c08b2f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0], device='cuda:0')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b3f9ef6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[3.4035e-04, 9.9966e-01],\n",
       "         [3.4037e-04, 9.9966e-01],\n",
       "         [3.4035e-04, 9.9966e-01],\n",
       "         [3.4037e-04, 9.9966e-01],\n",
       "         [3.4031e-04, 9.9966e-01],\n",
       "         [3.4029e-04, 9.9966e-01],\n",
       "         [3.4035e-04, 9.9966e-01],\n",
       "         [3.4036e-04, 9.9966e-01],\n",
       "         [3.4035e-04, 9.9966e-01],\n",
       "         [3.4033e-04, 9.9966e-01],\n",
       "         [3.4035e-04, 9.9966e-01],\n",
       "         [3.4032e-04, 9.9966e-01],\n",
       "         [3.4036e-04, 9.9966e-01],\n",
       "         [3.4035e-04, 9.9966e-01],\n",
       "         [3.4031e-04, 9.9966e-01],\n",
       "         [3.4030e-04, 9.9966e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>),\n",
       " tensor(0.8756, device='cuda:0', grad_fn=<NllLossBackward0>))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mt_model.style_heads[i_task](output['last_hidden_state'][:,0,:], label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "321621d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits, loss = mt_model(**tokens, i_task=i_task,  label=label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "57cbe6ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        device='cuda:0', grad_fn=<SqueezeBackward1>),\n",
       " tensor(0.2813, device='cuda:0', grad_fn=<MseLossBackward0>))"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e0e63e1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 8, 12, 12, 12,  7, 10,  7, 10, 12, 10], device='cuda:0')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e4adf29f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5.0471e-01,  1.2167e+00, -4.7918e-01,  ..., -1.3093e-01,\n",
       "         -4.7092e-03,  1.0579e+00],\n",
       "        [ 4.8265e-01,  1.1955e+00, -4.8833e-01,  ..., -9.8464e-02,\n",
       "         -2.6491e-02,  1.0363e+00],\n",
       "        [ 4.9495e-01,  1.2084e+00, -4.8237e-01,  ..., -8.0255e-02,\n",
       "          1.3555e-02,  1.0508e+00],\n",
       "        ...,\n",
       "        [ 5.0307e-01,  3.3286e-01, -4.7845e-01,  ..., -1.3443e-01,\n",
       "          1.0501e-02,  1.0487e+00],\n",
       "        [ 5.2614e-01,  1.2204e+00, -4.6653e-01,  ..., -7.9352e-02,\n",
       "         -9.1928e-03,  1.0598e+00],\n",
       "        [ 5.0942e-01,  1.2224e+00, -4.7927e-01,  ..., -1.2814e-01,\n",
       "          1.0450e-03,  1.0637e+00]], device='cuda:0', grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_emb = mt_model(**tokens, i_task=i_task,  label=label, return_sent_emb=True)\n",
    "sent_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4a7e7f78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 0.5493, -1.0068,  0.6620,  ...,  0.4401, -0.6234, -1.8955],\n",
       "         [ 0.4866, -0.3081,  1.4383,  ...,  0.5462, -0.6992, -1.7907],\n",
       "         [ 0.8709, -0.7770,  1.1853,  ..., -0.2183, -0.8382, -1.6414],\n",
       "         ...,\n",
       "         [ 0.0543, -0.6668,  1.2341,  ...,  1.3756, -0.6340, -1.6918],\n",
       "         [-0.3885, -0.9061,  1.8967,  ...,  0.3281, -0.7763, -1.8667],\n",
       "         [ 0.4483, -1.2293,  1.1639,  ...,  0.1616, -0.7883, -1.9392]],\n",
       "        device='cuda:0', grad_fn=<SliceBackward0>),\n",
       " tensor([[ 1.1720,  0.1778,  0.5938,  ..., -0.9601, -0.2259, -0.2412],\n",
       "         [ 1.1684,  0.2329,  0.4642,  ..., -1.0706,  0.3184, -0.2005],\n",
       "         [ 0.6869, -0.0212,  0.8063,  ..., -0.9281, -0.1159, -0.3850],\n",
       "         ...,\n",
       "         [ 1.3084,  0.0476,  0.4350,  ..., -0.6799, -0.5986, -0.4706],\n",
       "         [ 1.2512,  0.1888,  0.0546,  ..., -0.9272, -0.0723, -0.0089],\n",
       "         [ 0.7005,  0.3047,  0.6693,  ..., -0.7450, -0.3109, -0.1338]],\n",
       "        device='cuda:0', grad_fn=<SliceBackward0>),\n",
       " tensor([[ 0.8474, -0.1315,  0.5076,  ..., -0.1054,  0.2948,  0.4735],\n",
       "         [ 0.9569, -0.2143,  0.4118,  ...,  0.0679,  0.1802,  0.4096],\n",
       "         [ 0.8603, -0.3856,  0.9567,  ...,  0.1996,  0.3205,  0.2971],\n",
       "         ...,\n",
       "         [ 0.9252, -0.6147,  0.8478,  ...,  0.2346,  0.1292,  0.1476],\n",
       "         [ 0.8677, -0.0188,  0.7083,  ...,  0.0995,  0.0269,  0.1256],\n",
       "         [ 0.9453, -0.1700,  0.8537,  ...,  0.0542,  0.5242,  0.3638]],\n",
       "        device='cuda:0', grad_fn=<SliceBackward0>),\n",
       " tensor([[ 0.3133, -0.9638,  0.8201,  ..., -0.1618, -0.2524, -0.5329],\n",
       "         [ 0.4655, -0.8799,  0.8549,  ..., -0.1259, -0.0410, -0.4395],\n",
       "         [ 0.3188, -1.0275,  0.9918,  ..., -0.1584, -0.2829, -0.5497],\n",
       "         ...,\n",
       "         [ 0.3571, -0.8881,  0.9315,  ..., -0.1218, -0.3150, -0.5195],\n",
       "         [ 0.3747, -0.7517,  0.8914,  ..., -0.0396, -0.3002, -0.1501],\n",
       "         [ 0.4433, -0.4670,  0.2905,  ...,  0.0471, -0.3275, -0.4233]],\n",
       "        device='cuda:0', grad_fn=<SliceBackward0>),\n",
       " tensor([[-0.1752, -0.6623,  0.6047,  ..., -0.0210, -0.4767, -0.8514],\n",
       "         [-0.1806, -0.7220,  0.6183,  ...,  0.1229, -0.3124, -0.9439],\n",
       "         [ 0.0032, -0.6092,  0.4913,  ...,  0.0734, -0.5151, -0.7514],\n",
       "         ...,\n",
       "         [-0.1393, -0.5928,  0.4955,  ...,  0.1073, -0.7804, -0.6297],\n",
       "         [-0.1453, -0.3091,  0.4589,  ...,  0.0620, -0.5424, -0.6814],\n",
       "         [-0.0426, -0.6592,  0.5560,  ...,  0.0127, -0.7249, -0.7634]],\n",
       "        device='cuda:0', grad_fn=<SliceBackward0>),\n",
       " tensor([[-0.9918, -0.1409, -0.2455,  ...,  0.2049, -0.4020, -0.8627],\n",
       "         [-0.3396, -0.0317, -0.0602,  ...,  0.0376, -0.8518, -1.0211],\n",
       "         [-0.9966, -0.0734, -0.2494,  ..., -0.0094, -0.9163, -0.9201],\n",
       "         ...,\n",
       "         [-1.0274, -0.2484, -0.0075,  ...,  0.2574, -1.0244, -0.8665],\n",
       "         [-1.0400, -0.2900, -0.1526,  ...,  0.1590, -0.7790, -0.9805],\n",
       "         [-0.8526, -0.1004, -0.0797,  ...,  0.0414, -1.0905, -0.9513]],\n",
       "        device='cuda:0', grad_fn=<SliceBackward0>),\n",
       " tensor([[-0.4080, -0.6840,  0.7006,  ..., -0.1536, -0.9619, -0.7916],\n",
       "         [-0.2602, -0.5867,  0.5658,  ..., -0.9451, -0.9655, -0.8125],\n",
       "         [-0.2634, -0.6068,  0.1988,  ..., -0.8038, -0.9678, -0.0882],\n",
       "         ...,\n",
       "         [-0.3125, -0.6705,  0.7005,  ..., -0.8689, -1.0968, -0.8154],\n",
       "         [-0.2960, -0.6718,  0.6909,  ..., -0.8266, -0.9306, -0.0954],\n",
       "         [-0.3048, -0.0556,  0.5772,  ..., -0.8071, -0.9315, -0.9198]],\n",
       "        device='cuda:0', grad_fn=<SliceBackward0>),\n",
       " tensor([[-0.2812, -0.6284,  0.6829,  ..., -0.7827, -0.1884, -0.8925],\n",
       "         [-0.2614, -0.6151,  0.6393,  ..., -0.9038, -0.9776, -0.8132],\n",
       "         [ 0.0708, -0.6207,  0.6385,  ..., -0.8706, -1.0405, -0.9432],\n",
       "         ...,\n",
       "         [ 0.1107, -0.6585,  0.6455,  ..., -0.7271, -1.0414, -0.8955],\n",
       "         [-0.2423, -0.6000,  0.6819,  ..., -0.8808, -0.9801, -0.7775],\n",
       "         [-0.2585, -0.6727,  0.0939,  ..., -0.9408, -0.2411, -0.7374]],\n",
       "        device='cuda:0', grad_fn=<SliceBackward0>)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_embs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77eb8f9",
   "metadata": {},
   "source": [
    "torch metrics\n",
    "https://torchmetrics.readthedocs.io/en/stable/pages/overview.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8b20b98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = torchmetrics.Accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4ef7de68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1875"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = metric(torch.softmax(output, dim=-1).cpu(), torch.LongTensor([6, 7,  7, 12,  5, 12, 10,  5,  8,  8, 8, 8,  9,  5,  7, 3]))\n",
    "acc.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "029949b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_train_accuracy = metric.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "45b4e2d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0833)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_train_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fc1c5a33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>CrowdFlower</th>\n",
       "      <th>DailyDialog</th>\n",
       "      <th>EmoBank_Valence</th>\n",
       "      <th>EmoBank_Arousal</th>\n",
       "      <th>EmoBank_Dominance</th>\n",
       "      <th>HateOffensive</th>\n",
       "      <th>PASTEL_age</th>\n",
       "      <th>PASTEL_country</th>\n",
       "      <th>PASTEL_education</th>\n",
       "      <th>PASTEL_ethnic</th>\n",
       "      <th>...</th>\n",
       "      <th>PASTEL_tod</th>\n",
       "      <th>SARC</th>\n",
       "      <th>SarcasmGhosh</th>\n",
       "      <th>SentiTreeBank</th>\n",
       "      <th>ShortHumor</th>\n",
       "      <th>ShortJokeKaggle</th>\n",
       "      <th>ShortRomance</th>\n",
       "      <th>StanfordPoliteness</th>\n",
       "      <th>TroFi</th>\n",
       "      <th>VUA</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>13</th>\n",
       "      <th>7</th>\n",
       "      <th>1</th>\n",
       "      <th>1</th>\n",
       "      <th>1</th>\n",
       "      <th>3</th>\n",
       "      <th>8</th>\n",
       "      <th>2</th>\n",
       "      <th>10</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>5</th>\n",
       "      <th>2</th>\n",
       "      <th>2</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>2</th>\n",
       "      <th>2</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.312776</td>\n",
       "      <td>0.832633</td>\n",
       "      <td>0.017660</td>\n",
       "      <td>0.064081</td>\n",
       "      <td>0.007568</td>\n",
       "      <td>0.701575</td>\n",
       "      <td>1.832837</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>2.570806</td>\n",
       "      <td>1.628639</td>\n",
       "      <td>...</td>\n",
       "      <td>1.576885</td>\n",
       "      <td>1.125411</td>\n",
       "      <td>0.200766</td>\n",
       "      <td>0.065751</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.012265</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.312776</td>\n",
       "      <td>0.832633</td>\n",
       "      <td>0.035017</td>\n",
       "      <td>0.012292</td>\n",
       "      <td>0.026210</td>\n",
       "      <td>0.701575</td>\n",
       "      <td>1.832837</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>2.570806</td>\n",
       "      <td>1.628639</td>\n",
       "      <td>...</td>\n",
       "      <td>1.576885</td>\n",
       "      <td>1.125411</td>\n",
       "      <td>0.200766</td>\n",
       "      <td>1.380987</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.023418</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.312776</td>\n",
       "      <td>0.832633</td>\n",
       "      <td>0.016358</td>\n",
       "      <td>0.056178</td>\n",
       "      <td>0.019477</td>\n",
       "      <td>0.701575</td>\n",
       "      <td>1.832837</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>2.570802</td>\n",
       "      <td>1.628639</td>\n",
       "      <td>...</td>\n",
       "      <td>1.576885</td>\n",
       "      <td>1.125411</td>\n",
       "      <td>0.200766</td>\n",
       "      <td>0.075151</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.012740</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.281408</td>\n",
       "      <td>3.575869</td>\n",
       "      <td>10356.683170</td>\n",
       "      <td>3.350560</td>\n",
       "      <td>202.949844</td>\n",
       "      <td>2.296593</td>\n",
       "      <td>2.876076</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>2.906558</td>\n",
       "      <td>1.622863</td>\n",
       "      <td>...</td>\n",
       "      <td>1.576885</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.243174</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>117.785290</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>1.586025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.199740</td>\n",
       "      <td>3.575869</td>\n",
       "      <td>0.013024</td>\n",
       "      <td>0.009631</td>\n",
       "      <td>0.009826</td>\n",
       "      <td>2.296593</td>\n",
       "      <td>2.929388</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>2.869238</td>\n",
       "      <td>1.804569</td>\n",
       "      <td>...</td>\n",
       "      <td>1.576885</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.073756</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.012265</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3.199740</td>\n",
       "      <td>3.575869</td>\n",
       "      <td>0.017520</td>\n",
       "      <td>0.009645</td>\n",
       "      <td>0.007379</td>\n",
       "      <td>2.296593</td>\n",
       "      <td>2.929388</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>2.869238</td>\n",
       "      <td>1.804569</td>\n",
       "      <td>...</td>\n",
       "      <td>1.576885</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.082852</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.021920</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3.199740</td>\n",
       "      <td>3.575869</td>\n",
       "      <td>44.343632</td>\n",
       "      <td>39.389981</td>\n",
       "      <td>0.009247</td>\n",
       "      <td>2.296593</td>\n",
       "      <td>2.929388</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>2.869238</td>\n",
       "      <td>1.804569</td>\n",
       "      <td>...</td>\n",
       "      <td>1.576885</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.066447</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.040102</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3.199740</td>\n",
       "      <td>3.575869</td>\n",
       "      <td>0.013002</td>\n",
       "      <td>0.009748</td>\n",
       "      <td>0.008249</td>\n",
       "      <td>2.296593</td>\n",
       "      <td>2.929388</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>2.869238</td>\n",
       "      <td>1.804569</td>\n",
       "      <td>...</td>\n",
       "      <td>1.576885</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>243.021282</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.014671</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.531922</td>\n",
       "      <td>3.575869</td>\n",
       "      <td>0.014539</td>\n",
       "      <td>0.009767</td>\n",
       "      <td>0.007498</td>\n",
       "      <td>1.902214</td>\n",
       "      <td>2.563523</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>2.282474</td>\n",
       "      <td>1.289514</td>\n",
       "      <td>...</td>\n",
       "      <td>1.970627</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.063148</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>1.125329</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.011814</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2.531922</td>\n",
       "      <td>3.056301</td>\n",
       "      <td>0.013438</td>\n",
       "      <td>0.010019</td>\n",
       "      <td>0.008480</td>\n",
       "      <td>1.902214</td>\n",
       "      <td>2.563523</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>2.282474</td>\n",
       "      <td>1.289514</td>\n",
       "      <td>...</td>\n",
       "      <td>1.970627</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.063210</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>1.125329</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.012274</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2.531922</td>\n",
       "      <td>3.056301</td>\n",
       "      <td>0.013254</td>\n",
       "      <td>0.009790</td>\n",
       "      <td>0.007389</td>\n",
       "      <td>1.902214</td>\n",
       "      <td>2.553995</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>2.282474</td>\n",
       "      <td>1.289514</td>\n",
       "      <td>...</td>\n",
       "      <td>1.970627</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.063161</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>1.125329</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.014559</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2.531922</td>\n",
       "      <td>3.056301</td>\n",
       "      <td>0.031359</td>\n",
       "      <td>0.010579</td>\n",
       "      <td>0.012248</td>\n",
       "      <td>1.902214</td>\n",
       "      <td>2.291181</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>2.282474</td>\n",
       "      <td>1.289514</td>\n",
       "      <td>...</td>\n",
       "      <td>1.970627</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.063349</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>1.125329</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.012060</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2.669394</td>\n",
       "      <td>3.056301</td>\n",
       "      <td>33.257728</td>\n",
       "      <td>0.413607</td>\n",
       "      <td>0.007814</td>\n",
       "      <td>1.902214</td>\n",
       "      <td>2.291181</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>2.282474</td>\n",
       "      <td>1.289514</td>\n",
       "      <td>...</td>\n",
       "      <td>1.970627</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.063479</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>1.125329</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.247142</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2.669394</td>\n",
       "      <td>2.795953</td>\n",
       "      <td>0.013106</td>\n",
       "      <td>0.009670</td>\n",
       "      <td>0.007399</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>1.722609</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>2.682214</td>\n",
       "      <td>1.805775</td>\n",
       "      <td>...</td>\n",
       "      <td>1.779532</td>\n",
       "      <td>1.128445</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.064578</td>\n",
       "      <td>1.130458</td>\n",
       "      <td>1.125329</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.011947</td>\n",
       "      <td>1.201214</td>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2.593174</td>\n",
       "      <td>2.795953</td>\n",
       "      <td>0.013773</td>\n",
       "      <td>0.009716</td>\n",
       "      <td>0.008857</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>1.722609</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>2.682214</td>\n",
       "      <td>1.805775</td>\n",
       "      <td>...</td>\n",
       "      <td>1.779532</td>\n",
       "      <td>1.128445</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.064000</td>\n",
       "      <td>1.130458</td>\n",
       "      <td>1.125329</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.012716</td>\n",
       "      <td>1.201214</td>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3.166203</td>\n",
       "      <td>3.433892</td>\n",
       "      <td>0.013319</td>\n",
       "      <td>0.009545</td>\n",
       "      <td>0.008774</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>1.722609</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>2.682214</td>\n",
       "      <td>1.805775</td>\n",
       "      <td>...</td>\n",
       "      <td>1.718815</td>\n",
       "      <td>1.128445</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.063461</td>\n",
       "      <td>1.123398</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.012515</td>\n",
       "      <td>1.201214</td>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3.166203</td>\n",
       "      <td>3.433892</td>\n",
       "      <td>0.013053</td>\n",
       "      <td>0.009650</td>\n",
       "      <td>0.007941</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>1.722609</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>2.463943</td>\n",
       "      <td>1.805775</td>\n",
       "      <td>...</td>\n",
       "      <td>1.718815</td>\n",
       "      <td>1.128445</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.069324</td>\n",
       "      <td>1.123398</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.011902</td>\n",
       "      <td>1.201214</td>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2.741549</td>\n",
       "      <td>1.313582</td>\n",
       "      <td>0.020966</td>\n",
       "      <td>0.009784</td>\n",
       "      <td>0.009704</td>\n",
       "      <td>1.902214</td>\n",
       "      <td>2.201590</td>\n",
       "      <td>0.174123</td>\n",
       "      <td>2.618898</td>\n",
       "      <td>3.365728</td>\n",
       "      <td>...</td>\n",
       "      <td>2.133407</td>\n",
       "      <td>1.125411</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.063156</td>\n",
       "      <td>1.130458</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>1.126928</td>\n",
       "      <td>0.022923</td>\n",
       "      <td>1.201214</td>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2.675492</td>\n",
       "      <td>1.053234</td>\n",
       "      <td>0.014367</td>\n",
       "      <td>0.010479</td>\n",
       "      <td>0.014078</td>\n",
       "      <td>1.902214</td>\n",
       "      <td>2.201590</td>\n",
       "      <td>0.174123</td>\n",
       "      <td>2.618898</td>\n",
       "      <td>1.755472</td>\n",
       "      <td>...</td>\n",
       "      <td>2.283431</td>\n",
       "      <td>1.125411</td>\n",
       "      <td>0.200766</td>\n",
       "      <td>0.067555</td>\n",
       "      <td>1.130458</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>1.126928</td>\n",
       "      <td>0.014723</td>\n",
       "      <td>1.201214</td>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2.538020</td>\n",
       "      <td>1.053234</td>\n",
       "      <td>0.013331</td>\n",
       "      <td>0.009674</td>\n",
       "      <td>0.025462</td>\n",
       "      <td>1.902214</td>\n",
       "      <td>2.201590</td>\n",
       "      <td>0.174123</td>\n",
       "      <td>2.618898</td>\n",
       "      <td>1.579542</td>\n",
       "      <td>...</td>\n",
       "      <td>2.169905</td>\n",
       "      <td>1.125411</td>\n",
       "      <td>0.200766</td>\n",
       "      <td>0.080893</td>\n",
       "      <td>1.130458</td>\n",
       "      <td>1.128527</td>\n",
       "      <td>1.126928</td>\n",
       "      <td>0.012073</td>\n",
       "      <td>1.201214</td>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>3.515313</td>\n",
       "      <td>1.851276</td>\n",
       "      <td>0.013314</td>\n",
       "      <td>5.145812</td>\n",
       "      <td>0.017289</td>\n",
       "      <td>1.902214</td>\n",
       "      <td>2.464783</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>3.112872</td>\n",
       "      <td>2.084602</td>\n",
       "      <td>...</td>\n",
       "      <td>2.156836</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>2.053090</td>\n",
       "      <td>477.005259</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>1.125329</td>\n",
       "      <td>1.126928</td>\n",
       "      <td>0.014370</td>\n",
       "      <td>1.052642</td>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>3.515313</td>\n",
       "      <td>1.851276</td>\n",
       "      <td>0.013080</td>\n",
       "      <td>0.014636</td>\n",
       "      <td>0.024433</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>2.464783</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>3.112872</td>\n",
       "      <td>2.084602</td>\n",
       "      <td>...</td>\n",
       "      <td>1.970627</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>2.053090</td>\n",
       "      <td>0.071535</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>1.125329</td>\n",
       "      <td>1.126928</td>\n",
       "      <td>0.014299</td>\n",
       "      <td>1.052642</td>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>3.360841</td>\n",
       "      <td>1.851276</td>\n",
       "      <td>0.013085</td>\n",
       "      <td>0.019438</td>\n",
       "      <td>0.040487</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>1.986006</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>2.918075</td>\n",
       "      <td>2.084602</td>\n",
       "      <td>...</td>\n",
       "      <td>1.690410</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.063559</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>1.125329</td>\n",
       "      <td>1.126928</td>\n",
       "      <td>0.012019</td>\n",
       "      <td>1.052642</td>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>3.360841</td>\n",
       "      <td>1.851276</td>\n",
       "      <td>0.015409</td>\n",
       "      <td>0.028597</td>\n",
       "      <td>0.068424</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>2.038050</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>2.918075</td>\n",
       "      <td>2.084602</td>\n",
       "      <td>...</td>\n",
       "      <td>1.576885</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>167.689251</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>1.126928</td>\n",
       "      <td>0.012074</td>\n",
       "      <td>1.052642</td>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>3.195473</td>\n",
       "      <td>1.983602</td>\n",
       "      <td>0.018592</td>\n",
       "      <td>0.059705</td>\n",
       "      <td>0.112257</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>1.894972</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>2.093787</td>\n",
       "      <td>2.064906</td>\n",
       "      <td>...</td>\n",
       "      <td>2.006813</td>\n",
       "      <td>1.125411</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.066633</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>1.128527</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.011939</td>\n",
       "      <td>1.052642</td>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>3.248536</td>\n",
       "      <td>1.983602</td>\n",
       "      <td>0.017954</td>\n",
       "      <td>0.054340</td>\n",
       "      <td>0.093445</td>\n",
       "      <td>0.701575</td>\n",
       "      <td>1.849361</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>1.979928</td>\n",
       "      <td>2.064906</td>\n",
       "      <td>...</td>\n",
       "      <td>2.006813</td>\n",
       "      <td>1.125411</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.128055</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>1.128527</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.015505</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>3.236341</td>\n",
       "      <td>3.234514</td>\n",
       "      <td>0.016057</td>\n",
       "      <td>0.041509</td>\n",
       "      <td>0.061102</td>\n",
       "      <td>0.701575</td>\n",
       "      <td>1.849361</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>1.979928</td>\n",
       "      <td>2.064906</td>\n",
       "      <td>...</td>\n",
       "      <td>2.006813</td>\n",
       "      <td>1.125411</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.137261</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>1.128527</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.012144</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2.893061</td>\n",
       "      <td>1.512960</td>\n",
       "      <td>0.013178</td>\n",
       "      <td>0.052041</td>\n",
       "      <td>0.063899</td>\n",
       "      <td>0.701575</td>\n",
       "      <td>1.849361</td>\n",
       "      <td>0.174123</td>\n",
       "      <td>3.273791</td>\n",
       "      <td>3.384352</td>\n",
       "      <td>...</td>\n",
       "      <td>1.576885</td>\n",
       "      <td>1.128445</td>\n",
       "      <td>0.200766</td>\n",
       "      <td>8.340908</td>\n",
       "      <td>1.130458</td>\n",
       "      <td>1.128527</td>\n",
       "      <td>1.126928</td>\n",
       "      <td>6066.250663</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>3.465798</td>\n",
       "      <td>1.512960</td>\n",
       "      <td>8964.222899</td>\n",
       "      <td>0.019575</td>\n",
       "      <td>0.017272</td>\n",
       "      <td>0.701575</td>\n",
       "      <td>1.894972</td>\n",
       "      <td>0.174123</td>\n",
       "      <td>3.273791</td>\n",
       "      <td>3.384352</td>\n",
       "      <td>...</td>\n",
       "      <td>1.576885</td>\n",
       "      <td>1.128445</td>\n",
       "      <td>0.200766</td>\n",
       "      <td>0.063543</td>\n",
       "      <td>1.130458</td>\n",
       "      <td>1.128527</td>\n",
       "      <td>1.126928</td>\n",
       "      <td>0.016995</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>3.465798</td>\n",
       "      <td>1.512960</td>\n",
       "      <td>0.016301</td>\n",
       "      <td>0.018517</td>\n",
       "      <td>0.010876</td>\n",
       "      <td>0.701575</td>\n",
       "      <td>1.849361</td>\n",
       "      <td>0.174123</td>\n",
       "      <td>3.273791</td>\n",
       "      <td>1.886398</td>\n",
       "      <td>...</td>\n",
       "      <td>1.576885</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.200766</td>\n",
       "      <td>0.120865</td>\n",
       "      <td>1.130458</td>\n",
       "      <td>1.128527</td>\n",
       "      <td>1.126928</td>\n",
       "      <td>0.012610</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30 rows  22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   CrowdFlower DailyDialog EmoBank_Valence EmoBank_Arousal EmoBank_Dominance  \\\n",
       "            13          7               1               1                 1    \n",
       "0     2.312776    0.832633        0.017660        0.064081          0.007568   \n",
       "1     2.312776    0.832633        0.035017        0.012292          0.026210   \n",
       "2     2.312776    0.832633        0.016358        0.056178          0.019477   \n",
       "3     3.281408    3.575869    10356.683170        3.350560        202.949844   \n",
       "4     3.199740    3.575869        0.013024        0.009631          0.009826   \n",
       "5     3.199740    3.575869        0.017520        0.009645          0.007379   \n",
       "6     3.199740    3.575869       44.343632       39.389981          0.009247   \n",
       "7     3.199740    3.575869        0.013002        0.009748          0.008249   \n",
       "8     2.531922    3.575869        0.014539        0.009767          0.007498   \n",
       "9     2.531922    3.056301        0.013438        0.010019          0.008480   \n",
       "10    2.531922    3.056301        0.013254        0.009790          0.007389   \n",
       "11    2.531922    3.056301        0.031359        0.010579          0.012248   \n",
       "12    2.669394    3.056301       33.257728        0.413607          0.007814   \n",
       "13    2.669394    2.795953        0.013106        0.009670          0.007399   \n",
       "14    2.593174    2.795953        0.013773        0.009716          0.008857   \n",
       "15    3.166203    3.433892        0.013319        0.009545          0.008774   \n",
       "16    3.166203    3.433892        0.013053        0.009650          0.007941   \n",
       "17    2.741549    1.313582        0.020966        0.009784          0.009704   \n",
       "18    2.675492    1.053234        0.014367        0.010479          0.014078   \n",
       "19    2.538020    1.053234        0.013331        0.009674          0.025462   \n",
       "20    3.515313    1.851276        0.013314        5.145812          0.017289   \n",
       "21    3.515313    1.851276        0.013080        0.014636          0.024433   \n",
       "22    3.360841    1.851276        0.013085        0.019438          0.040487   \n",
       "23    3.360841    1.851276        0.015409        0.028597          0.068424   \n",
       "24    3.195473    1.983602        0.018592        0.059705          0.112257   \n",
       "25    3.248536    1.983602        0.017954        0.054340          0.093445   \n",
       "26    3.236341    3.234514        0.016057        0.041509          0.061102   \n",
       "27    2.893061    1.512960        0.013178        0.052041          0.063899   \n",
       "28    3.465798    1.512960     8964.222899        0.019575          0.017272   \n",
       "29    3.465798    1.512960        0.016301        0.018517          0.010876   \n",
       "\n",
       "   HateOffensive PASTEL_age PASTEL_country PASTEL_education PASTEL_ethnic  \\\n",
       "              3          8              2                10            10   \n",
       "0       0.701575   1.832837       0.693147         2.570806      1.628639   \n",
       "1       0.701575   1.832837       0.693147         2.570806      1.628639   \n",
       "2       0.701575   1.832837       0.693147         2.570802      1.628639   \n",
       "3       2.296593   2.876076       0.693147         2.906558      1.622863   \n",
       "4       2.296593   2.929388       0.693147         2.869238      1.804569   \n",
       "5       2.296593   2.929388       0.693147         2.869238      1.804569   \n",
       "6       2.296593   2.929388       0.693147         2.869238      1.804569   \n",
       "7       2.296593   2.929388       0.693147         2.869238      1.804569   \n",
       "8       1.902214   2.563523       0.693147         2.282474      1.289514   \n",
       "9       1.902214   2.563523       0.693147         2.282474      1.289514   \n",
       "10      1.902214   2.553995       0.693147         2.282474      1.289514   \n",
       "11      1.902214   2.291181       0.693147         2.282474      1.289514   \n",
       "12      1.902214   2.291181       0.693147         2.282474      1.289514   \n",
       "13      1.098612   1.722609       0.693147         2.682214      1.805775   \n",
       "14      1.098612   1.722609       0.693147         2.682214      1.805775   \n",
       "15      1.098612   1.722609       0.693147         2.682214      1.805775   \n",
       "16      1.098612   1.722609       0.693147         2.463943      1.805775   \n",
       "17      1.902214   2.201590       0.174123         2.618898      3.365728   \n",
       "18      1.902214   2.201590       0.174123         2.618898      1.755472   \n",
       "19      1.902214   2.201590       0.174123         2.618898      1.579542   \n",
       "20      1.902214   2.464783       0.693147         3.112872      2.084602   \n",
       "21      1.098612   2.464783       0.693147         3.112872      2.084602   \n",
       "22      1.098612   1.986006       0.693147         2.918075      2.084602   \n",
       "23      1.098612   2.038050       0.693147         2.918075      2.084602   \n",
       "24      1.098612   1.894972       0.693147         2.093787      2.064906   \n",
       "25      0.701575   1.849361       0.693147         1.979928      2.064906   \n",
       "26      0.701575   1.849361       0.693147         1.979928      2.064906   \n",
       "27      0.701575   1.849361       0.174123         3.273791      3.384352   \n",
       "28      0.701575   1.894972       0.174123         3.273791      3.384352   \n",
       "29      0.701575   1.849361       0.174123         3.273791      1.886398   \n",
       "\n",
       "    ... PASTEL_tod      SARC SarcasmGhosh SentiTreeBank ShortHumor  \\\n",
       "    ...         5         2            2             1          2    \n",
       "0   ...   1.576885  1.125411     0.200766      0.065751   0.693147   \n",
       "1   ...   1.576885  1.125411     0.200766      1.380987   0.693147   \n",
       "2   ...   1.576885  1.125411     0.200766      0.075151   0.693147   \n",
       "3   ...   1.576885  0.693147     0.693147      0.243174   0.693147   \n",
       "4   ...   1.576885  0.693147     0.693147      0.073756   0.693147   \n",
       "5   ...   1.576885  0.693147     0.693147      0.082852   0.693147   \n",
       "6   ...   1.576885  0.693147     0.693147      0.066447   0.693147   \n",
       "7   ...   1.576885  0.693147     0.693147    243.021282   0.693147   \n",
       "8   ...   1.970627  0.693147     0.693147      0.063148   0.693147   \n",
       "9   ...   1.970627  0.693147     0.693147      0.063210   0.693147   \n",
       "10  ...   1.970627  0.693147     0.693147      0.063161   0.693147   \n",
       "11  ...   1.970627  0.693147     0.693147      0.063349   0.693147   \n",
       "12  ...   1.970627  0.693147     0.693147      0.063479   0.693147   \n",
       "13  ...   1.779532  1.128445     0.693147      0.064578   1.130458   \n",
       "14  ...   1.779532  1.128445     0.693147      0.064000   1.130458   \n",
       "15  ...   1.718815  1.128445     0.693147      0.063461   1.123398   \n",
       "16  ...   1.718815  1.128445     0.693147      0.069324   1.123398   \n",
       "17  ...   2.133407  1.125411     0.693147      0.063156   1.130458   \n",
       "18  ...   2.283431  1.125411     0.200766      0.067555   1.130458   \n",
       "19  ...   2.169905  1.125411     0.200766      0.080893   1.130458   \n",
       "20  ...   2.156836  0.693147     2.053090    477.005259   0.693147   \n",
       "21  ...   1.970627  0.693147     2.053090      0.071535   0.693147   \n",
       "22  ...   1.690410  0.693147     0.693147      0.063559   0.693147   \n",
       "23  ...   1.576885  0.693147     0.693147    167.689251   0.693147   \n",
       "24  ...   2.006813  1.125411     0.693147      0.066633   0.693147   \n",
       "25  ...   2.006813  1.125411     0.693147      0.128055   0.693147   \n",
       "26  ...   2.006813  1.125411     0.693147      0.137261   0.693147   \n",
       "27  ...   1.576885  1.128445     0.200766      8.340908   1.130458   \n",
       "28  ...   1.576885  1.128445     0.200766      0.063543   1.130458   \n",
       "29  ...   1.576885  0.693147     0.200766      0.120865   1.130458   \n",
       "\n",
       "   ShortJokeKaggle ShortRomance StanfordPoliteness     TroFi       VUA  \n",
       "                2            2                  1         2         2   \n",
       "0         0.693147     0.693147           0.012265  0.693147  0.693147  \n",
       "1         0.693147     0.693147           0.023418  0.693147  0.693147  \n",
       "2         0.693147     0.693147           0.012740  0.693147  0.693147  \n",
       "3         0.693147     0.693147         117.785290  0.693147  1.586025  \n",
       "4         0.693147     0.693147           0.012265  0.693147  0.693147  \n",
       "5         0.693147     0.693147           0.021920  0.693147  0.693147  \n",
       "6         0.693147     0.693147           0.040102  0.693147  0.693147  \n",
       "7         0.693147     0.693147           0.014671  0.693147  0.693147  \n",
       "8         1.125329     0.693147           0.011814  0.693147  0.693147  \n",
       "9         1.125329     0.693147           0.012274  0.693147  0.693147  \n",
       "10        1.125329     0.693147           0.014559  0.693147  0.693147  \n",
       "11        1.125329     0.693147           0.012060  0.693147  0.693147  \n",
       "12        1.125329     0.693147           0.247142  0.693147  0.693147  \n",
       "13        1.125329     0.693147           0.011947  1.201214  0.693147  \n",
       "14        1.125329     0.693147           0.012716  1.201214  0.693147  \n",
       "15        0.693147     0.693147           0.012515  1.201214  0.693147  \n",
       "16        0.693147     0.693147           0.011902  1.201214  0.693147  \n",
       "17        0.693147     1.126928           0.022923  1.201214  0.693147  \n",
       "18        0.693147     1.126928           0.014723  1.201214  0.693147  \n",
       "19        1.128527     1.126928           0.012073  1.201214  0.693147  \n",
       "20        1.125329     1.126928           0.014370  1.052642  0.693147  \n",
       "21        1.125329     1.126928           0.014299  1.052642  0.693147  \n",
       "22        1.125329     1.126928           0.012019  1.052642  0.693147  \n",
       "23        0.693147     1.126928           0.012074  1.052642  0.693147  \n",
       "24        1.128527     0.693147           0.011939  1.052642  0.693147  \n",
       "25        1.128527     0.693147           0.015505  0.693147  0.693147  \n",
       "26        1.128527     0.693147           0.012144  0.693147  0.693147  \n",
       "27        1.128527     1.126928        6066.250663  0.693147  0.693147  \n",
       "28        1.128527     1.126928           0.016995  0.693147  0.693147  \n",
       "29        1.128527     1.126928           0.012610  0.693147  0.693147  \n",
       "\n",
       "[30 rows x 22 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev.columns = tasks.items()\n",
    "df_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5f97e20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './mt_model_runs/mt_2.bin'\n",
    "torch.save(mt_model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6424252b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('losses.json', 'w') as f:\n",
    "    json.dump(losses, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0376c53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dev.to_csv('dev_losses.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b5087e",
   "metadata": {},
   "source": [
    "# senteval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783d6959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sys.path.append(f'{os.getcwd()}/../SentEval')\n",
    "# PATH_TO_DATA = f'{os.getcwd()}/../SentEval/data'\n",
    "\n",
    "# # Import SentEval\n",
    "# import senteval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "faa83f5b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def prepare(params, samples):\n",
    "    return\n",
    "def batcher(params, batch):\n",
    "    sentences = [' '.join(s) for s in batch]\n",
    "    batch = tokenizer(\n",
    "        sentences,\n",
    "        return_tensors='pt',\n",
    "        padding=True,\n",
    "    )\n",
    "    \n",
    "    for k in batch:\n",
    "        batch[k] = batch[k].to(device)\n",
    "    with torch.no_grad():\n",
    "        sent_emb = mt_model(**batch, return_sent_emb=True)\n",
    "    \n",
    "    return sent_emb.cpu()\n",
    "\n",
    "# Set params for SentEval (fastmode)\n",
    "params = {'task_path': PATH_TO_DATA, 'usepytorch': True, 'kfold': 5}\n",
    "params['classifier'] = {'nhid': 0, 'optim': 'rmsprop', 'batch_size': 128,\n",
    "                                    'tenacity': 3, 'epoch_size': 2}\n",
    "\n",
    "task_set = 'sts'\n",
    "if task_set == 'sts':\n",
    "    senteval_tasks = ['STS12', 'STS13', 'STS14', 'STS15', 'STS16', 'STSBenchmark', 'SICKRelatedness']\n",
    "elif task_set == 'transfer':\n",
    "    senteval_tasks = ['MR', 'CR', 'MPQA', 'SUBJ', 'SST2', 'TREC', 'MRPC']\n",
    "elif task_set == 'full':\n",
    "    senteval_tasks = ['STS12', 'STS13', 'STS14', 'STS15', 'STS16', 'STSBenchmark', 'SICKRelatedness']\n",
    "    senteval_tasks += ['MR', 'CR', 'MPQA', 'SUBJ', 'SST2', 'TREC', 'MRPC']\n",
    "\n",
    "se = senteval.engine.SE(params, batcher, prepare)\n",
    "mt_model.eval()\n",
    "results = se.eval(senteval_tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f0585c45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'STS12': {'MSRpar': {'pearson': (0.04511842111176674, 0.21713343634765625),\n",
       "   'spearman': SpearmanrResult(correlation=0.05829879048588996, pvalue=0.11065140077710642),\n",
       "   'nsamples': 750},\n",
       "  'MSRvid': {'pearson': (-0.04719690809332701, 0.19666754119304142),\n",
       "   'spearman': SpearmanrResult(correlation=-0.05791055536831769, pvalue=0.11305028382924873),\n",
       "   'nsamples': 750},\n",
       "  'SMTeuroparl': {'pearson': (0.05465063166493182, 0.24259241634429948),\n",
       "   'spearman': SpearmanrResult(correlation=0.04601455241561682, pvalue=0.3252817731161748),\n",
       "   'nsamples': 459},\n",
       "  'surprise.OnWN': {'pearson': (0.028684333440108387, 0.43280310929667104),\n",
       "   'spearman': SpearmanrResult(correlation=0.0486239359795535, pvalue=0.183456625977699),\n",
       "   'nsamples': 750},\n",
       "  'surprise.SMTnews': {'pearson': (0.004681029491783591, 0.9257362156479326),\n",
       "   'spearman': SpearmanrResult(correlation=-0.013892916608515183, pvalue=0.7820461848374392),\n",
       "   'nsamples': 399},\n",
       "  'all': {'pearson': {'all': 0.014868264700974198,\n",
       "    'mean': 0.017187501523052706,\n",
       "    'wmean': 0.01509226368897569},\n",
       "   'spearman': {'all': 0.015054354649698811,\n",
       "    'mean': 0.01622676138084548,\n",
       "    'wmean': 0.016839296703608394}}},\n",
       " 'STS13': {'FNWN': {'pearson': (0.06002909117909498, 0.4119145612631916),\n",
       "   'spearman': SpearmanrResult(correlation=0.10666278073835941, pvalue=0.14406661717560476),\n",
       "   'nsamples': 189},\n",
       "  'headlines': {'pearson': (-0.016041462614685892, 0.6609448622666173),\n",
       "   'spearman': SpearmanrResult(correlation=-0.013468861132827664, pvalue=0.7126783172726778),\n",
       "   'nsamples': 750},\n",
       "  'OnWN': {'pearson': (-0.07275485896320083, 0.08512943410489214),\n",
       "   'spearman': SpearmanrResult(correlation=-0.10822762814993239, pvalue=0.010310445835506298),\n",
       "   'nsamples': 561},\n",
       "  'all': {'pearson': {'all': -0.033342285143532845,\n",
       "    'mean': -0.009589076799597246,\n",
       "    'wmean': -0.027667383071014087},\n",
       "   'spearman': {'all': -0.04431797498580879,\n",
       "    'mean': -0.005011236181466881,\n",
       "    'wmean': -0.03377205312145526}}},\n",
       " 'STS14': {'deft-forum': {'pearson': (-0.021190909391096264,\n",
       "    0.6539171213268393),\n",
       "   'spearman': SpearmanrResult(correlation=0.0029060908432702397, pvalue=0.9509799970865339),\n",
       "   'nsamples': 450},\n",
       "  'deft-news': {'pearson': (-0.04926721632365719, 0.3951653745618607),\n",
       "   'spearman': SpearmanrResult(correlation=-0.015566161047035513, pvalue=0.7883109827723784),\n",
       "   'nsamples': 300},\n",
       "  'headlines': {'pearson': (-0.036271895856946464, 0.3211895362760735),\n",
       "   'spearman': SpearmanrResult(correlation=-0.01334434447258084, pvalue=0.7152184385155735),\n",
       "   'nsamples': 750},\n",
       "  'images': {'pearson': (0.04866819404814243, 0.18305765622334177),\n",
       "   'spearman': SpearmanrResult(correlation=0.05836358393088991, pvalue=0.11025498096668766),\n",
       "   'nsamples': 750},\n",
       "  'OnWN': {'pearson': (0.012467595309804187, 0.7331917338366095),\n",
       "   'spearman': SpearmanrResult(correlation=0.0052956944913676056, pvalue=0.8848785081393861),\n",
       "   'nsamples': 750},\n",
       "  'tweet-news': {'pearson': (-0.0484903037253082, 0.18466517099205706),\n",
       "   'spearman': SpearmanrResult(correlation=-0.060365555567131204, pvalue=0.09854921142261619),\n",
       "   'nsamples': 750},\n",
       "  'all': {'pearson': {'all': -0.00430607701796145,\n",
       "    'mean': -0.015680755989843583,\n",
       "    'wmean': -0.011209568477685734},\n",
       "   'spearman': {'all': 0.0022572859515260823,\n",
       "    'mean': -0.003785115303536634,\n",
       "    'wmean': -0.002906686306061319}}},\n",
       " 'STS15': {'answers-forums': {'pearson': (0.004816059805726414,\n",
       "    0.9259418669703817),\n",
       "   'spearman': SpearmanrResult(correlation=0.026151520189151158, pvalue=0.6136868898421568),\n",
       "   'nsamples': 375},\n",
       "  'answers-students': {'pearson': (0.03918296576869461, 0.28385863697876323),\n",
       "   'spearman': SpearmanrResult(correlation=0.03984013410841718, pvalue=0.27585421304522323),\n",
       "   'nsamples': 750},\n",
       "  'belief': {'pearson': (0.015021320699688725, 0.7718683584847408),\n",
       "   'spearman': SpearmanrResult(correlation=0.010098170341856312, pvalue=0.8454693854921901),\n",
       "   'nsamples': 375},\n",
       "  'headlines': {'pearson': (0.017554243305938865, 0.6312431198857941),\n",
       "   'spearman': SpearmanrResult(correlation=0.03219384489201663, pvalue=0.3786305234273506),\n",
       "   'nsamples': 750},\n",
       "  'images': {'pearson': (-0.06171300104517855, 0.09124412338374546),\n",
       "   'spearman': SpearmanrResult(correlation=-0.04114135190420947, pvalue=0.26046213846165633),\n",
       "   'nsamples': 750},\n",
       "  'all': {'pearson': {'all': 0.004980804085493675,\n",
       "    'mean': 0.0029723177069740135,\n",
       "    'wmean': 0.0012357245705406233},\n",
       "   'spearman': {'all': 0.016412582538039754,\n",
       "    'mean': 0.01342846352544636,\n",
       "    'wmean': 0.012254368090432018}}},\n",
       " 'STS16': {'answer-answer': {'pearson': (0.09638987265977564,\n",
       "    0.12547448671360714),\n",
       "   'spearman': SpearmanrResult(correlation=0.0704678208720361, pvalue=0.26317243154014536),\n",
       "   'nsamples': 254},\n",
       "  'headlines': {'pearson': (0.09546151397635683, 0.13304278320286977),\n",
       "   'spearman': SpearmanrResult(correlation=0.08769641340272052, pvalue=0.16773378560824984),\n",
       "   'nsamples': 249},\n",
       "  'plagiarism': {'pearson': (-0.023254569313365135, 0.7257391914319138),\n",
       "   'spearman': SpearmanrResult(correlation=-0.036292456678818995, pvalue=0.5839775954283772),\n",
       "   'nsamples': 230},\n",
       "  'postediting': {'pearson': (0.08533342407802708, 0.18400294501056963),\n",
       "   'spearman': SpearmanrResult(correlation=0.10513255618290616, pvalue=0.10135216378278733),\n",
       "   'nsamples': 244},\n",
       "  'question-question': {'pearson': (-0.044619727218692305, 0.5211908361500954),\n",
       "   'spearman': SpearmanrResult(correlation=-0.020283823663301125, pvalue=0.7706604089711906),\n",
       "   'nsamples': 209},\n",
       "  'all': {'pearson': {'all': 0.0474392426460971,\n",
       "    'mean': 0.04186210283642042,\n",
       "    'wmean': 0.04586865613824098},\n",
       "   'spearman': {'all': 0.04372775283752752,\n",
       "    'mean': 0.04134410202310853,\n",
       "    'wmean': 0.04452023015653067}}},\n",
       " 'STSBenchmark': {'train': {'pearson': (0.0029242067551564586,\n",
       "    0.8245692798092561),\n",
       "   'spearman': SpearmanrResult(correlation=0.0036638990419097382, pvalue=0.7812079114872583),\n",
       "   'nsamples': 5749},\n",
       "  'dev': {'pearson': (-0.01772728686373328, 0.49267753704857437),\n",
       "   'spearman': SpearmanrResult(correlation=0.0018255850636838076, pvalue=0.9436797209993755),\n",
       "   'nsamples': 1500},\n",
       "  'test': {'pearson': (0.013181439308982199, 0.6247930962772686),\n",
       "   'spearman': SpearmanrResult(correlation=0.0037039774949266606, pvalue=0.8906968207880507),\n",
       "   'nsamples': 1379},\n",
       "  'all': {'pearson': {'all': 0.0012741367819083397,\n",
       "    'mean': -0.0005405469331982079,\n",
       "    'wmean': 0.0009732891918035476},\n",
       "   'spearman': {'all': 0.0040670140871223335,\n",
       "    'mean': 0.003064487200173402,\n",
       "    'wmean': 0.0033507091044238132}}},\n",
       " 'SICKRelatedness': {'train': {'pearson': (-0.04701125618938638,\n",
       "    0.0016078501873482392),\n",
       "   'spearman': SpearmanrResult(correlation=-0.04303466092832693, pvalue=0.003884543748536068),\n",
       "   'nsamples': 4500},\n",
       "  'dev': {'pearson': (0.03231563834984917, 0.47092220229455317),\n",
       "   'spearman': SpearmanrResult(correlation=-0.006204259523049101, pvalue=0.8899357054569257),\n",
       "   'nsamples': 500},\n",
       "  'test': {'pearson': (-0.013428132806185007, 0.34600898585681605),\n",
       "   'spearman': SpearmanrResult(correlation=-0.004088426444182312, pvalue=0.7741845770018561),\n",
       "   'nsamples': 4927},\n",
       "  'all': {'pearson': {'all': -0.025679792589757226,\n",
       "    'mean': -0.009374583548574072,\n",
       "    'wmean': -0.026347662336394444},\n",
       "   'spearman': {'all': -0.021552217030538626,\n",
       "    'mean': -0.01777578229851945,\n",
       "    'wmean': -0.02184968077258809}}}}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d93ed7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
