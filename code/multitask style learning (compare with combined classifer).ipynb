{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ed7de70",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#multi-task-dataloader\" data-toc-modified-id=\"multi-task-dataloader-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>multi-task dataloader</a></span></li><li><span><a href=\"#multi-task-model\" data-toc-modified-id=\"multi-task-model-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>multi-task model</a></span></li><li><span><a href=\"#bertology\" data-toc-modified-id=\"bertology-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>bertology</a></span></li><li><span><a href=\"#train\" data-toc-modified-id=\"train-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>train</a></span><ul class=\"toc-item\"><li><span><a href=\"#single-task\" data-toc-modified-id=\"single-task-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>single task</a></span><ul class=\"toc-item\"><li><span><a href=\"#ShortHumor\" data-toc-modified-id=\"ShortHumor-4.1.1\"><span class=\"toc-item-num\">4.1.1&nbsp;&nbsp;</span>ShortHumor</a></span><ul class=\"toc-item\"><li><span><a href=\"#freeze\" data-toc-modified-id=\"freeze-4.1.1.1\"><span class=\"toc-item-num\">4.1.1.1&nbsp;&nbsp;</span>freeze</a></span></li><li><span><a href=\"#unfreeze\" data-toc-modified-id=\"unfreeze-4.1.1.2\"><span class=\"toc-item-num\">4.1.1.2&nbsp;&nbsp;</span>unfreeze</a></span></li></ul></li><li><span><a href=\"#SARC\" data-toc-modified-id=\"SARC-4.1.2\"><span class=\"toc-item-num\">4.1.2&nbsp;&nbsp;</span>SARC</a></span><ul class=\"toc-item\"><li><span><a href=\"#freeze\" data-toc-modified-id=\"freeze-4.1.2.1\"><span class=\"toc-item-num\">4.1.2.1&nbsp;&nbsp;</span>freeze</a></span></li><li><span><a href=\"#unfreeze\" data-toc-modified-id=\"unfreeze-4.1.2.2\"><span class=\"toc-item-num\">4.1.2.2&nbsp;&nbsp;</span>unfreeze</a></span></li></ul></li></ul></li><li><span><a href=\"#multi-task\" data-toc-modified-id=\"multi-task-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>multi task</a></span><ul class=\"toc-item\"><li><span><a href=\"#ShortHumor-+-SARC-freeze\" data-toc-modified-id=\"ShortHumor-+-SARC-freeze-4.2.1\"><span class=\"toc-item-num\">4.2.1&nbsp;&nbsp;</span>ShortHumor + SARC freeze</a></span></li><li><span><a href=\"#ShortHumor-+-SARC-unfreeze\" data-toc-modified-id=\"ShortHumor-+-SARC-unfreeze-4.2.2\"><span class=\"toc-item-num\">4.2.2&nbsp;&nbsp;</span>ShortHumor + SARC unfreeze</a></span></li><li><span><a href=\"#PASTEL_country-+-SARC-+-ShortHumor-freeze-all\" data-toc-modified-id=\"PASTEL_country-+-SARC-+-ShortHumor-freeze-all-4.2.3\"><span class=\"toc-item-num\">4.2.3&nbsp;&nbsp;</span>PASTEL_country + SARC + ShortHumor freeze all</a></span></li><li><span><a href=\"#PASTEL_country-+-SARC-+-ShortHumor-freeze-all,-no-pooler\" data-toc-modified-id=\"PASTEL_country-+-SARC-+-ShortHumor-freeze-all,-no-pooler-4.2.4\"><span class=\"toc-item-num\">4.2.4&nbsp;&nbsp;</span>PASTEL_country + SARC + ShortHumor freeze all, no pooler</a></span></li><li><span><a href=\"#PASTEL_country-+-SARC-+-ShortHumor-unfreeze-all,-no-pooler\" data-toc-modified-id=\"PASTEL_country-+-SARC-+-ShortHumor-unfreeze-all,-no-pooler-4.2.5\"><span class=\"toc-item-num\">4.2.5&nbsp;&nbsp;</span>PASTEL_country + SARC + ShortHumor unfreeze all, no pooler</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "785f4cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torchmetrics\n",
    "import numpy as np\n",
    "import collections\n",
    "import json\n",
    "from datetime import datetime\n",
    "from tqdm.auto import tqdm, trange\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import *\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "import sys\n",
    "import os\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29182ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/huggingface/transformers/issues/5486\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a260d42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CLASSES = {\n",
    "    \"bert\": (BertConfig, BertPreTrainedModel, BertModel, BertTokenizer),\n",
    "    \"roberta\": (RobertaConfig, RobertaPreTrainedModel, RobertaModel, RobertaTokenizer),\n",
    "    \"albert\": (AlbertConfig, AlbertPreTrainedModel, AlbertModel, AlbertTokenizer),\n",
    "    \"distilbert\": (DistilBertConfig, DistilBertPreTrainedModel, DistilBertModel, DistilBertTokenizer)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3e55144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_type = 'distilbert'\n",
    "# model_name = 'distilbert-base-uncased'\n",
    "\n",
    "model_type = 'bert'\n",
    "model_name = 'bert-base-uncased'\n",
    "config_class, pretrained_model_class, model_class, tokenizer_class = MODEL_CLASSES[model_type]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "186facb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8839076",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CrowdFlower': 13,\n",
       " 'DailyDialog': 7,\n",
       " 'EmoBank_Valence': 1,\n",
       " 'EmoBank_Arousal': 1,\n",
       " 'EmoBank_Dominance': 1,\n",
       " 'HateOffensive': 3,\n",
       " 'PASTEL_age': 8,\n",
       " 'PASTEL_country': 2,\n",
       " 'PASTEL_education': 10,\n",
       " 'PASTEL_ethnic': 10,\n",
       " 'PASTEL_gender': 3,\n",
       " 'PASTEL_politics': 3,\n",
       " 'PASTEL_tod': 5,\n",
       " 'SARC': 2,\n",
       " 'SarcasmGhosh': 2,\n",
       " 'SentiTreeBank': 1,\n",
       " 'ShortHumor': 2,\n",
       " 'ShortJokeKaggle': 2,\n",
       " 'ShortRomance': 2,\n",
       " 'StanfordPoliteness': 1,\n",
       " 'TroFi': 2,\n",
       " 'VUA': 2}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dictionary: task_name: number_of_labels\n",
    "with open('../data/xslue/tasks.json', 'r') as f:\n",
    "    tasks = json.load(f)\n",
    "tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02ae7ceb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CrowdFlower': 0,\n",
       " 'DailyDialog': 1,\n",
       " 'EmoBank_Valence': 2,\n",
       " 'EmoBank_Arousal': 3,\n",
       " 'EmoBank_Dominance': 4,\n",
       " 'HateOffensive': 5,\n",
       " 'PASTEL_age': 6,\n",
       " 'PASTEL_country': 7,\n",
       " 'PASTEL_education': 8,\n",
       " 'PASTEL_ethnic': 9,\n",
       " 'PASTEL_gender': 10,\n",
       " 'PASTEL_politics': 11,\n",
       " 'PASTEL_tod': 12,\n",
       " 'SARC': 13,\n",
       " 'SarcasmGhosh': 14,\n",
       " 'SentiTreeBank': 15,\n",
       " 'ShortHumor': 16,\n",
       " 'ShortJokeKaggle': 17,\n",
       " 'ShortRomance': 18,\n",
       " 'StanfordPoliteness': 19,\n",
       " 'TroFi': 20,\n",
       " 'VUA': 21}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dictionary: task_name: task index\n",
    "tasks2idx = {k:i for i,k in enumerate(tasks)}\n",
    "tasks2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d23f160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# task and their (train) dataset size \n",
    "selected_task = ['PASTEL_country', # 33224\n",
    "#                  'SARC', # 205645\n",
    "                 'SarcasmGhosh', # 39780\n",
    "                 'ShortHumor', # 37801\n",
    "#                  'ShortJokeKaggle', # 406682\n",
    "#                  'ShortRomance', # 1902\n",
    "#                  'TroFi', # 3335\n",
    "                 'VUA', # 15157\n",
    "                ] \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318bd186",
   "metadata": {},
   "source": [
    "# multi-task dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80f1c8b",
   "metadata": {},
   "source": [
    "Test/validation dataloader consume dataset one by one, where as the train dataloader do it randomly. So the train dataloader is more complicated than test/validation dataloader. It must be able to reset a dataset once it is exhausted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00309940",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset): \n",
    "    # currently it's a Mapping-style dataset. Not sure if a Iterable-style dataset will be better\n",
    "    # limit: use to truncate dataset. This will drop rows after certain index. May influence label distribution.\n",
    "    def __init__(self, tsv_file, limit=None):\n",
    "        self.df = pd.read_csv(tsv_file, sep='\\t')\n",
    "        self.df = self.df.dropna()\n",
    "        self.df = self.df.reset_index(drop=True)\n",
    "        if self.df['label'].dtype == 'float64':\n",
    "            self.df['label'] = self.df['label'].astype('float32')\n",
    "        if limit:\n",
    "            self.df = self.df.iloc[:limit]\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        dataslice = self.df.iloc[idx]\n",
    "        sample = {'text':dataslice['text'], 'label':dataslice['label']}\n",
    "        return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6463e817",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskTrainDataLoader():\n",
    "    '''\n",
    "    Each time, a random integer selects a dataset and load a batch of data {text, label} from it. Return i_task and data\n",
    "    \n",
    "    a iterator\n",
    "    Known issue: large dataset may have not iterate once, small datasets may have been iterated many times\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, selected_task, batch_size, shuffle, num_workers, limit=None):\n",
    "        self.tasks = selected_task\n",
    "        self.split = 'train'\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.num_workers = num_workers\n",
    "        \n",
    "        self.num_tasks = len(self.tasks)\n",
    "        self.datasets = []\n",
    "        self.dataloaders = []\n",
    "        self.dataloaderiters = []\n",
    "        self.len = 0\n",
    "        for task in self.tasks:\n",
    "            self.datasets.append(MyDataset(f'../data/xslue/processed/{self.split}/{task}.tsv', limit=limit))\n",
    "            self.dataloaders.append(DataLoader(self.datasets[-1], batch_size=self.batch_size, shuffle=self.shuffle, num_workers=self.num_workers)) \n",
    "            self.dataloaderiters.append(self.dataloaders[-1]._get_iterator())\n",
    "            self.len += len(self.dataloaders[-1])\n",
    "    def __len__(self):   \n",
    "        return self.len\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.n = 0\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        i_task = np.random.randint(self.num_tasks)\n",
    "        if self.n < self.len:\n",
    "            self.n += 1\n",
    "        else:\n",
    "            raise StopIteration\n",
    "        dataloaderiter = self.dataloaderiters[i_task]    \n",
    "        try: \n",
    "            batch = next(dataloaderiter)\n",
    "        except StopIteration:\n",
    "#             self.dataloaderiters[i_task]._reset(self.dataloaders[i_task])\n",
    "#             dataloaderiter = self.dataloaderiters[i_task]\n",
    "            self.dataloaderiters[i_task] = iter(self.dataloaders[i_task])\n",
    "            dataloaderiter = self.dataloaderiters[i_task]\n",
    "            batch = next(dataloaderiter)\n",
    "        return i_task, batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61019a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskTestDataLoader():\n",
    "    '''\n",
    "    For dev and test\n",
    "    \n",
    "    a generator\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, tasks, split, batch_size, shuffle, num_workers, limit=None):\n",
    "        assert split in ['dev', 'test'], 'not implemented'\n",
    "        self.tasks = tasks\n",
    "        self.split = split\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.num_workers = num_workers\n",
    "        \n",
    "        self.num_tasks = len(tasks)\n",
    "        self.datasets = []\n",
    "        self.dataloaders = []\n",
    "        self.dataloaderiters = []\n",
    "        self.len = 0\n",
    "        for task in tasks:\n",
    "            self.datasets.append(MyDataset(f'../data/xslue/processed/{self.split}/{task}.tsv', limit=limit))\n",
    "            self.dataloaders.append(DataLoader(self.datasets[-1], batch_size=self.batch_size, shuffle=self.shuffle, num_workers=self.num_workers))\n",
    "            self.len += len(self.dataloaders[-1])\n",
    "        self.i_task = 0\n",
    "    def __len__(self):   \n",
    "        return self.len\n",
    "\n",
    "    def __iter__(self):\n",
    "        for i_task in range(self.num_tasks):\n",
    "            dataloader = self.dataloaders[i_task]\n",
    "            for batch in dataloader:\n",
    "                yield i_task, batch\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100a28b2",
   "metadata": {},
   "source": [
    "# multi-task model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c2649d",
   "metadata": {},
   "source": [
    "Given selected tasks, the model will add corresponding classification heads on the top of pretrained bert/(other bert). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "0ed50b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionHead(nn.Module):\n",
    "    def __init__(self, embedding_dim = 768, hidden_dim = 128):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.hidden = nn.Linear(embedding_dim, 1)\n",
    "        \n",
    "        self.loss_fn = nn.MSELoss()\n",
    "    def forward(self, sent_emb, label):\n",
    "        output = self.hidden(self.dropout(sent_emb)).squeeze(1)\n",
    "\n",
    "        loss = self.loss_fn(output, label)\n",
    "        return output, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "c097a3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationHead(nn.Module):\n",
    "    def __init__(self, num_labels, embedding_dim = 768, hidden_dim = 128):\n",
    "        super().__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.hidden = nn.Linear(embedding_dim, self.num_labels)\n",
    "        \n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "    def forward(self, sent_emb, label):\n",
    "        output = self.hidden(self.dropout(sent_emb))\n",
    "        \n",
    "        loss = self.loss_fn(output.view(-1, self.num_labels), label.view(-1))\n",
    "        return output, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "4eec4f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskBert(pretrained_model_class):\n",
    "    def __init__(self, config, selected_task, use_pooler=True):\n",
    "        super().__init__(config)\n",
    "        self.use_pooler = use_pooler\n",
    "        self.basemodel = model_class.from_pretrained(model_name)\n",
    "        self.style_heads = nn.ModuleList()\n",
    "        \n",
    "        for task in selected_task:\n",
    "            if tasks[task] == 1:\n",
    "                self.style_heads.append(RegressionHead())\n",
    "            else:\n",
    "                self.style_heads.append(ClassificationHead(tasks[task]))\n",
    "    def forward(self, i_task=None, label=None, **kwargs):\n",
    "        output = self.basemodel(**kwargs)\n",
    "        if self.use_pooler and ('pooler_output' in output):\n",
    "            sent_emb = output['pooler_output']\n",
    "        else:\n",
    "            sent_emb = output['last_hidden_state'][:,0,:]\n",
    "        \n",
    "        output, loss = self.style_heads[i_task](sent_emb, label) \n",
    "        return output, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "6acf874a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_loss(losses):\n",
    "    for k in losses:\n",
    "        print(f'{losses[k]:4.4f}', end=' ')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "f7439d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, tokenizer, selected_task, split, limit=None):\n",
    "    val_loss = collections.defaultdict(float)\n",
    "    val_size = collections.defaultdict(int)\n",
    "    overall_acc = torchmetrics.Accuracy() \n",
    "    task_accs = [torchmetrics.Accuracy() for i in range(len(selected_task))] \n",
    "    \n",
    "    if split == 'train':\n",
    "        mt_dataloader = MultiTaskTrainDataLoader(selected_task, 32, False, 4, limit=limit)\n",
    "    else:\n",
    "        mt_dataloader = MultiTaskTestDataLoader(selected_task, 'dev', 32, False, 4, limit=limit)\n",
    "        \n",
    "    model.eval()\n",
    "    for data in tqdm(mt_dataloader, leave=False):  \n",
    "        i_task, batch = data\n",
    "        label = batch['label'].to(device)\n",
    "        size = len(label)\n",
    "        del batch['label']\n",
    "        tokens = tokenizer(**batch, return_tensors='pt', padding=True, truncation=True, max_length=64).to(device)\n",
    "        output, loss = model(**tokens, i_task=i_task,  label=label)\n",
    "        overall_acc.update(output.to('cpu').detach(), label.to('cpu').detach())\n",
    "        task_accs[i_task].update(output.to('cpu').detach(), label.to('cpu').detach())\n",
    "        tokens = None\n",
    "        output = None\n",
    "        val_loss[i_task] += loss.detach().item()*size\n",
    "        val_size[i_task] += size\n",
    "    \n",
    "    accs = []\n",
    "    for i_task in val_loss:\n",
    "        val_loss[i_task] /= val_size[i_task]\n",
    "        accs.append(task_accs[i_task].compute())\n",
    "    model.train()\n",
    "    \n",
    "    \n",
    "    return val_loss, overall_acc.compute(), accs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "2c0ef0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_multitask(selected_task, freeze_bert, use_pooler=True, num_epoch=5, limit=None, save=True):\n",
    "    \"\"\"\n",
    "    selected_task = [\n",
    "                    'ShortHumor',\n",
    "                    'SARC',\n",
    "                    ] \n",
    "    freeze_bert = False # True or False or int, represent freeze how many layers to freeze\n",
    "    use_pooler=True\n",
    "    num_epoch = 5\n",
    "    limit = 30000 # set the maximum number of data in each dataset \n",
    "    save=True\n",
    "    \"\"\"\n",
    "    excute_time = datetime.now() \n",
    "    result_folder = '../result'\n",
    "    model_folder = f\"{result_folder}/{'+'.join(selected_task)}/{excute_time.now().strftime('%Y%m%d-%H:%M:%S')}\"\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # load bert config\n",
    "    config = config_class.from_pretrained(model_name) # bert config is actually a dataclass. can add any args you want\n",
    "    config.freeze_bert = freeze_bert\n",
    "    config.use_pooler = use_pooler\n",
    "    config.num_epoch = num_epoch\n",
    "    config.limit = limit\n",
    "    # some other args that usually keep the same\n",
    "    config.my_batchsize = 32\n",
    "    config.my_max_length = 64\n",
    "    \n",
    "    tokenizer = tokenizer_class.from_pretrained(model_name)\n",
    "    \n",
    "    train_dataloader = MultiTaskTrainDataLoader(selected_task, config.my_batchsize, False, 4, limit = limit)\n",
    "    \n",
    "    # load pretrained bert and add classification heads to it\n",
    "    mt_model = MultiTaskBert(config, selected_task, use_pooler).to(device)\n",
    "    \n",
    "    # freeze top layers\n",
    "    if freeze_bert==True:\n",
    "        for param in mt_model.basemodel.parameters():\n",
    "            param.requires_grad = False\n",
    "    elif isinstance(freeze_bert, int):\n",
    "        if freeze_bert > 0:\n",
    "            freeze_bert = freeze_bert - 12\n",
    "        for param in mt_model.basemodel.parameters():\n",
    "            param.requires_grad = False\n",
    "        for layer in mt_model.basemodel.encoder.layer: \n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = True    \n",
    "        if 'pooler' in mt_model.basemodel._modules:\n",
    "            for param in mt_model.basemodel.pooler.parameters():\n",
    "                param.requires_grad = True    \n",
    "        \n",
    "    optimizer = optim.AdamW(mt_model.parameters(), lr=5e-5)\n",
    "\n",
    "    scheduler = get_scheduler(\"linear\",\n",
    "                                optimizer=optimizer,\n",
    "                                num_warmup_steps=500,\n",
    "                                num_training_steps=num_epoch*len(train_dataloader))\n",
    "\n",
    "    # create dataframes for logging\n",
    "    columns = ['i_epoch', 'train_loss'] + [f'train_loss_{selected_task[i]}' for i in range(len(selected_task))]\n",
    "    columns += ['train_acc'] + [f'train_acc_{selected_task[i]}' for i in range(len(selected_task))]\n",
    "    columns += ['val_loss'] + [f'val_loss_{selected_task[i]}' for i in range(len(selected_task))]\n",
    "    columns += ['val_acc'] + [f'val_acc_{selected_task[i]}' for i in range(len(selected_task))]\n",
    "    df_evaluation = pd.DataFrame(columns=columns)\n",
    "    df_loss_per_step = pd.DataFrame(columns=['i_epoch', 'i_iter', 'i_task', 'task_name', 'train_loss'])\n",
    "    \n",
    "    best_accuracy = 0.0\n",
    "    progress_bar = tqdm(range(num_epoch*len(train_dataloader)))\n",
    "    for i_epoch in range(num_epoch):\n",
    "        for i_iter, data in enumerate(train_dataloader):  \n",
    "            i_task, batch = data\n",
    "            optimizer.zero_grad()\n",
    "            label = batch['label'].to(device)\n",
    "            del batch['label']\n",
    "            tokens = tokenizer(**batch, return_tensors='pt', padding=True, truncation=True, max_length=config.my_max_length).to(device)\n",
    "            output, loss = mt_model(**tokens, i_task=i_task,  label=label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            # log per step\n",
    "            step_result = {'i_epoch':i_epoch, 'i_iter':i_iter, 'i_task':i_task, 'task_name':selected_task[i_task], 'train_loss':loss.item(),}\n",
    "            df_loss_per_step = df_loss_per_step.append(step_result , ignore_index=True)\n",
    "            progress_bar.update(1)\n",
    "        \n",
    "        # run evaluation on train and validation set \n",
    "        train_loss, train_overall_acc, train_task_accs = validate(mt_model, tokenizer, selected_task, 'train', limit = limit)\n",
    "        val_loss, val_overall_acc, val_task_accs = validate(mt_model, tokenizer, selected_task, 'val', limit = limit)\n",
    "        \n",
    "        # save best model and corresponding opt and scheduler states to disk\n",
    "        if save and val_overall_acc.item() > best_accuracy: \n",
    "            torch.save(mt_model.state_dict(), f\"{model_folder}/pytorch_model.bin\")\n",
    "            torch.save(optimizer.state_dict(), f\"{model_folder}/optimizer.pt\")\n",
    "            torch.save(scheduler.state_dict(), f\"{model_folder}/scheduler.pt\")\n",
    "            \n",
    "        # collect result\n",
    "        result = {'i_epoch':i_epoch, f'train_loss':sum(train_loss.values()), 'train_acc':train_overall_acc.item(), 'val_loss':sum(val_loss.values()), 'val_acc':val_overall_acc.item()}\n",
    "        result.update({f'train_loss_{selected_task[i]}':train_loss[i] for i in train_loss})\n",
    "        result.update({f'train_acc_{selected_task[i]}':train_task_accs[i].item() for i in range(len(train_task_accs))})   \n",
    "        result.update({f'val_loss_{selected_task[i]}':val_loss[i] for i in val_loss})\n",
    "        result.update({f'val_acc_{selected_task[i]}':val_task_accs[i].item() for i in range(len(val_task_accs))})   \n",
    "        df_evaluation = df_evaluation.append(result , ignore_index=True)\n",
    "#         print('\\n'.join([f\"{k}:{v:.4}\" if isinstance(v, float) else f\"{k}:{v}\" for k,v in result.items()]))\n",
    "    \n",
    "    # save to disk\n",
    "    if save:\n",
    "        config.to_json_file(f\"{model_folder}/config.json\")\n",
    "        df_evaluation.to_csv(f\"{model_folder}/evaluation.csv\", index=False)\n",
    "        df_loss_per_step.to_csv(f\"{model_folder}/loss_per_step.csv\", index=False)\n",
    "    \n",
    "    display(df_evaluation) \n",
    "#     display(df_loss_per_step) # this is too long, not approporate to show directly\n",
    "    return df_evaluation, df_loss_per_step\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64900c64",
   "metadata": {},
   "source": [
    "# bertology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "14089e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(p):\n",
    "    \"\"\" Compute the entropy of a probability distribution \"\"\"\n",
    "    plogp = p * torch.log(p)\n",
    "    plogp[p == 0] = 0\n",
    "    return -plogp.sum(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "86f23899",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_heads_importance(\n",
    "    model, tokenizer, eval_dataloader, model_dir, compute_entropy=True, compute_importance=True, head_mask=None, \n",
    "    dont_normalize_importance_by_layer = True, dont_normalize_global_importance=True\n",
    "):\n",
    "    \"\"\" This method shows how to compute:\n",
    "        - head attention entropy\n",
    "        - head importance scores according to http://arxiv.org/abs/1905.10650\n",
    "    \"\"\"\n",
    "    # Prepare our tensors\n",
    "    n_layers, n_heads = model.basemodel.config.num_hidden_layers, model.basemodel.config.num_attention_heads\n",
    "    head_importance = torch.zeros(n_layers, n_heads).to(device)\n",
    "    attn_entropy = torch.zeros(n_layers, n_heads).to(device)\n",
    "\n",
    "    if head_mask is None:\n",
    "        head_mask = torch.ones(n_layers, n_heads).to(device)\n",
    "    head_mask.requires_grad_(requires_grad=True)\n",
    "    preds = None\n",
    "    labels = None\n",
    "    tot_tokens = 0.0\n",
    "\n",
    "    for step, batch in enumerate(tqdm(eval_dataloader, desc=\"Iteration\")):\n",
    "        i_task, batch = batch\n",
    "        label_ids = batch['label'].to(device)\n",
    "        size = len(label_ids)\n",
    "        del batch['label']\n",
    "        batch = tokenizer(**batch, return_tensors='pt', padding=True, truncation=True, max_length=64).to(device)\n",
    "        input_ids, input_mask, segment_ids = batch['input_ids'], batch['attention_mask'], batch['token_type_ids']\n",
    "        # Do a forward pass (not with torch.no_grad() since we need gradients for importance score - see below)\n",
    "        print(i_task)\n",
    "        outputs = model(i_task=i_task,\n",
    "            input_ids=input_ids, token_type_ids=segment_ids, attention_mask=input_mask, label=label_ids, head_mask=head_mask, \n",
    "            output_attentions = True, \n",
    "        )\n",
    "        loss, logits, all_attentions = (\n",
    "            outputs[1],\n",
    "            outputs[0].logits,\n",
    "            outputs[0].attentions,\n",
    "        )  # Loss and logits are the first, attention the last\n",
    "        loss.backward()  # Backpropagate to populate the gradients in the head mask\n",
    "\n",
    "        if compute_entropy:\n",
    "            for layer, attn in enumerate(all_attentions):\n",
    "                masked_entropy = entropy(attn.detach()) * input_mask.float().unsqueeze(1)\n",
    "                attn_entropy[layer] += masked_entropy.sum(-1).sum(0).detach()\n",
    "\n",
    "        if compute_importance:\n",
    "            head_importance += head_mask.grad.abs().detach()\n",
    "\n",
    "        # Also store our logits/labels if we want to compute metrics afterwards\n",
    "        if preds is None:\n",
    "            preds = logits.detach().cpu().numpy()\n",
    "            labels = label_ids.detach().cpu().numpy()\n",
    "        else:\n",
    "            preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
    "            labels = np.append(labels, label_ids.detach().cpu().numpy(), axis=0)\n",
    "\n",
    "        tot_tokens += input_mask.float().detach().sum().data\n",
    "\n",
    "    # Normalize\n",
    "    attn_entropy /= tot_tokens\n",
    "    head_importance /= tot_tokens\n",
    "    # Layerwise importance normalization\n",
    "    if not dont_normalize_importance_by_layer:\n",
    "        exponent = 2\n",
    "        norm_by_layer = torch.pow(torch.pow(head_importance, exponent).sum(-1), 1 / exponent)\n",
    "        head_importance /= norm_by_layer.unsqueeze(-1) + 1e-20\n",
    "\n",
    "    if not dont_normalize_global_importance:\n",
    "        head_importance = (head_importance - head_importance.min()) / (head_importance.max() - head_importance.min())\n",
    "\n",
    "    # Print/save matrices\n",
    "#     np.save(os.path.join(model_dir, \"attn_entropy.npy\"), attn_entropy.detach().cpu().numpy())\n",
    "#     np.save(os.path.join(model_dir, \"head_importance.npy\"), head_importance.detach().cpu().numpy())\n",
    "\n",
    "    head_ranks = torch.zeros(head_importance.numel(), dtype=torch.long, device=device)\n",
    "    head_ranks[head_importance.view(-1).sort(descending=True)[1]] = torch.arange(\n",
    "        head_importance.numel(), device=device\n",
    "    )\n",
    "    head_ranks = head_ranks.view_as(head_importance)\n",
    "\n",
    "    return attn_entropy, head_importance, preds, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "bdf305d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "bert = MultiTaskBert(BertConfig.from_pretrained('bert-base-uncased'), selected_task, use_pooler).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "798cc550",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "feeb4d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataloader = MultiTaskTestDataLoader(['ShortHumor'], 'dev', 32, False, 4, limit=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "9876bfd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad780b535ad1451fa25da5573b2339d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'logits'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1321163/4199881312.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m attn_entropy, head_importance, preds, labels = compute_heads_importance(\n\u001b[0m\u001b[1;32m      2\u001b[0m     bert, tokenizer, eval_dataloader, model_dir='a')\n",
      "\u001b[0;32m/tmp/ipykernel_1321163/4116704812.py\u001b[0m in \u001b[0;36mcompute_heads_importance\u001b[0;34m(model, tokenizer, eval_dataloader, model_dir, compute_entropy, compute_importance, head_mask, dont_normalize_importance_by_layer, dont_normalize_global_importance)\u001b[0m\n\u001b[1;32m     34\u001b[0m         loss, logits, all_attentions = (\n\u001b[1;32m     35\u001b[0m             \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m             \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         )  # Loss and logits are the first, attention the last\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'logits'"
     ]
    }
   ],
   "source": [
    "attn_entropy, head_importance, preds, labels = compute_heads_importance(\n",
    "    bert, tokenizer, eval_dataloader, model_dir='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a64ec1df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 12])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head_importance.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "42b60dc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 12])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_entropy.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d73059",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f6fdc2",
   "metadata": {},
   "source": [
    "## single task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33dc7f9",
   "metadata": {},
   "source": [
    "### ShortHumor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f17d88",
   "metadata": {},
   "source": [
    "#### freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "801bd0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_task = [\n",
    "                 'ShortHumor', \n",
    "#                  'VUA',\n",
    "                ] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8edd545e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = MultiTaskTrainDataLoader(selected_task, 32, False, 4)\n",
    "\n",
    "config = config_class.from_pretrained(model_name)\n",
    "tokenizer = tokenizer_class.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6225c252",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "327851f1facb48928c6da65fd9df5f3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1182 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1182 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/62 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i_epoch': 0, 'train_loss': 0.6093603514596295, 'train_acc': 0.6768603920936584, 'val_loss': 0.6037419830616592, 'val_acc': 0.6787695288658142}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f773cbd96f1464eaeb51eea492b96be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1182 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1182 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/62 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i_epoch': 1, 'train_loss': 0.5694271674443419, 'train_acc': 0.6997169256210327, 'val_loss': 0.5666932542697986, 'val_acc': 0.7009581327438354}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b89f9d15df2c4093a7533bed82f3dc58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1182 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1182 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/62 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i_epoch': 2, 'train_loss': 0.542740888546511, 'train_acc': 0.7273352742195129, 'val_loss': 0.5414822714171262, 'val_acc': 0.7216339111328125}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db90ff53549c4b0db4dcf65b89ea9a30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1182 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1182 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/62 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i_epoch': 3, 'train_loss': 0.5291925388148391, 'train_acc': 0.7452183961868286, 'val_loss': 0.5293197461408133, 'val_acc': 0.7367624640464783}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed6ffbe6f79045bca1def22ea7263751",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1182 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1182 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/62 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i_epoch': 4, 'train_loss': 0.5281198126989516, 'train_acc': 0.7454035878181458, 'val_loss': 0.5283215008657266, 'val_acc': 0.738779604434967}\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "num_epoch = 5\n",
    "\n",
    "mt_model = MultiTaskBert(config, selected_task).to(device)\n",
    "for param in mt_model.basemodel.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "optimizer = optim.AdamW(mt_model.parameters(), lr=5e-5)\n",
    "# optimizer = optim.SGD(mt_model.parameters(), lr=0.001)\n",
    "\n",
    "# scheduler = lr_scheduler.LinearLR(optimizer, start_factor=0.01, total_iters=500)\n",
    "scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=5e-5, total_steps=num_epoch*len(train_dataloader)) \n",
    "\n",
    "df = pd.DataFrame(columns=['i_iter', 'train_loss', 'train_acc', 'val_loss', 'val_acc'])\n",
    "\n",
    "for i_epoch in range(num_epoch):\n",
    "    for i_iter, data in enumerate(tqdm(train_dataloader)):  \n",
    "\n",
    "        i_task, batch = data\n",
    "        optimizer.zero_grad()\n",
    "        label = batch['label'].to(device)\n",
    "        del batch['label']\n",
    "        tokens = tokenizer(**batch, return_tensors='pt', padding=True, truncation=True, max_length=64).to(device)\n",
    "        output, loss = mt_model(**tokens, i_task=i_task,  label=label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "#         if i_iter in [0, 10, 25, 50, 100]:\n",
    "#             train_loss, train_overall_acc, train_task_accs = validate(mt_model, 'train')\n",
    "#             val_loss, val_overall_acc, val_task_accs = validate(mt_model, 'val')\n",
    "#             result = {'i_iter':i_iter, 'train_loss':train_loss[0], 'train_acc':train_overall_acc.item(), 'val_loss':val_loss[0], 'val_acc':val_overall_acc.item()}\n",
    "#             df = df.append(result , ignore_index=True)\n",
    "#             print(result)\n",
    "#         if i_iter>100:\n",
    "#             break\n",
    "\n",
    "    train_loss, train_overall_acc, train_task_accs = validate(mt_model, selected_task, 'train')\n",
    "    val_loss, val_overall_acc, val_task_accs = validate(mt_model, selected_task'val')\n",
    "    result = {'i_epoch':i_epoch, 'train_loss':train_loss[0], 'train_acc':train_overall_acc.item(), 'val_loss':val_loss[0], 'val_acc':val_overall_acc.item()}\n",
    "    df = df.append(result , ignore_index=True)\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8dfb1ef4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>i_iter</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>i_epoch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.609360</td>\n",
       "      <td>0.676860</td>\n",
       "      <td>0.603742</td>\n",
       "      <td>0.678770</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.569427</td>\n",
       "      <td>0.699717</td>\n",
       "      <td>0.566693</td>\n",
       "      <td>0.700958</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.542741</td>\n",
       "      <td>0.727335</td>\n",
       "      <td>0.541482</td>\n",
       "      <td>0.721634</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.529193</td>\n",
       "      <td>0.745218</td>\n",
       "      <td>0.529320</td>\n",
       "      <td>0.736762</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.528120</td>\n",
       "      <td>0.745404</td>\n",
       "      <td>0.528322</td>\n",
       "      <td>0.738780</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   i_iter  train_loss  train_acc  val_loss   val_acc  i_epoch\n",
       "0     NaN    0.609360   0.676860  0.603742  0.678770      0.0\n",
       "1     NaN    0.569427   0.699717  0.566693  0.700958      1.0\n",
       "2     NaN    0.542741   0.727335  0.541482  0.721634      2.0\n",
       "3     NaN    0.529193   0.745218  0.529320  0.736762      3.0\n",
       "4     NaN    0.528120   0.745404  0.528322  0.738780      4.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with onecycle lr_scheduler\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8717197",
   "metadata": {},
   "source": [
    "#### unfreeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "294f962b",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_task = [\n",
    "                 'ShortHumor', \n",
    "#                  'VUA',\n",
    "                ] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "efb60987",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = MultiTaskTrainDataLoader(selected_task, 32, False, 4)\n",
    "\n",
    "config = config_class.from_pretrained(model_name)\n",
    "tokenizer = tokenizer_class.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "446157c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1182 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1182 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/62 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i_epoch': 0, 'train_loss': 0.07624205425341254, 'train_acc': 0.9742070436477661, 'val_loss': 0.1591183666442298, 'val_acc': 0.940494179725647}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1182 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1182 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/62 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f19a2515550>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/home/jz17d/anaconda3/envs/torch/lib/python3.9/multiprocessing/process.py\", line 160, in is_alive\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f19a2515550>Exception ignored in: \n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f19a2515550><function _MultiProcessingDataLoaderIter.__del__ at 0x7f19a2515550>Traceback (most recent call last):\n",
      "\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()  File \"/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "\n",
      "          File \"/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "self._shutdown_workers()\n",
      "  File \"/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "if w.is_alive():\n",
      "        if w.is_alive():if w.is_alive():\n",
      "    \n",
      "assert self._parent_pid == os.getpid(), 'can only test a child process'  File \"/home/jz17d/anaconda3/envs/torch/lib/python3.9/multiprocessing/process.py\", line 160, in is_alive\n",
      "  File \"/home/jz17d/anaconda3/envs/torch/lib/python3.9/multiprocessing/process.py\", line 160, in is_alive\n",
      "\n",
      "  File \"/home/jz17d/anaconda3/envs/torch/lib/python3.9/multiprocessing/process.py\", line 160, in is_alive\n",
      "AssertionError            assert self._parent_pid == os.getpid(), 'can only test a child process'assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "\n",
      ": AssertionErrorcan only test a child process: \n",
      "AssertionError: can only test a child processcan only test a child process\n",
      "assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "\n",
      "AssertionError: can only test a child process\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i_epoch': 1, 'train_loss': 0.030277462488580556, 'train_acc': 0.9911113381385803, 'val_loss': 0.14881271859926823, 'val_acc': 0.9520927667617798}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1182 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1182 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/62 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i_epoch': 2, 'train_loss': 0.011882821528125562, 'train_acc': 0.9964551329612732, 'val_loss': 0.15233089646162015, 'val_acc': 0.9546142220497131}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1182 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1182 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/62 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i_epoch': 3, 'train_loss': 0.0017386663505057035, 'train_acc': 0.9996032118797302, 'val_loss': 0.14110398142522484, 'val_acc': 0.9667171239852905}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1182 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1182 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/62 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i_epoch': 4, 'train_loss': 0.000905192531714256, 'train_acc': 0.9998412728309631, 'val_loss': 0.15679824659722963, 'val_acc': 0.9636913537979126}\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "num_epoch = 5\n",
    "\n",
    "mt_model = MultiTaskBert(config, selected_task).to(device)\n",
    "# for param in mt_model.basemodel.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "optimizer = optim.AdamW(mt_model.parameters(), lr=5e-5)\n",
    "# optimizer = optim.SGD(mt_model.parameters(), lr=0.001)\n",
    "\n",
    "# scheduler = lr_scheduler.LinearLR(optimizer, start_factor=0.01, total_iters=500)\n",
    "scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=5e-5, total_steps=num_epoch*len(train_dataloader)) \n",
    "\n",
    "df = pd.DataFrame(columns=['i_iter', 'train_loss', 'train_acc', 'val_loss', 'val_acc'])\n",
    "\n",
    "for i_epoch in range(num_epoch):\n",
    "    for i_iter, data in enumerate(tqdm(train_dataloader)):  \n",
    "\n",
    "        i_task, batch = data\n",
    "        optimizer.zero_grad()\n",
    "        label = batch['label'].to(device)\n",
    "        del batch['label']\n",
    "        tokens = tokenizer(**batch, return_tensors='pt', padding=True, truncation=True, max_length=64).to(device)\n",
    "        output, loss = mt_model(**tokens, i_task=i_task,  label=label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "#         if i_iter in [0, 10, 25, 50, 100]:\n",
    "#             train_loss, train_overall_acc, train_task_accs = validate(mt_model, 'train')\n",
    "#             val_loss, val_overall_acc, val_task_accs = validate(mt_model, 'val')\n",
    "#             result = {'i_iter':i_iter, 'train_loss':train_loss[0], 'train_acc':train_overall_acc.item(), 'val_loss':val_loss[0], 'val_acc':val_overall_acc.item()}\n",
    "#             df = df.append(result , ignore_index=True)\n",
    "#             print(result)\n",
    "#         if i_iter>100:\n",
    "#             break\n",
    "\n",
    "    train_loss, train_overall_acc, train_task_accs = validate(mt_model, 'train')\n",
    "    val_loss, val_overall_acc, val_task_accs = validate(mt_model, 'val')\n",
    "    result = {'i_epoch':i_epoch, 'train_loss':train_loss[0], 'train_acc':train_overall_acc.item(), 'val_loss':val_loss[0], 'val_acc':val_overall_acc.item()}\n",
    "    df = df.append(result , ignore_index=True)\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "02a03a82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>i_iter</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>i_epoch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.076242</td>\n",
       "      <td>0.974207</td>\n",
       "      <td>0.159118</td>\n",
       "      <td>0.940494</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.030277</td>\n",
       "      <td>0.991111</td>\n",
       "      <td>0.148813</td>\n",
       "      <td>0.952093</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.011883</td>\n",
       "      <td>0.996455</td>\n",
       "      <td>0.152331</td>\n",
       "      <td>0.954614</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001739</td>\n",
       "      <td>0.999603</td>\n",
       "      <td>0.141104</td>\n",
       "      <td>0.966717</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000905</td>\n",
       "      <td>0.999841</td>\n",
       "      <td>0.156798</td>\n",
       "      <td>0.963691</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   i_iter  train_loss  train_acc  val_loss   val_acc  i_epoch\n",
       "0     NaN    0.076242   0.974207  0.159118  0.940494      0.0\n",
       "1     NaN    0.030277   0.991111  0.148813  0.952093      1.0\n",
       "2     NaN    0.011883   0.996455  0.152331  0.954614      2.0\n",
       "3     NaN    0.001739   0.999603  0.141104  0.966717      3.0\n",
       "4     NaN    0.000905   0.999841  0.156798  0.963691      4.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df # ShortHumor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea67d1a4",
   "metadata": {},
   "source": [
    "### SARC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389a2285",
   "metadata": {},
   "source": [
    "#### freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6e43c6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_task = [\n",
    "#                 'SarcasmGhosh',\n",
    "                'SARC',\n",
    "                ] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "188c7325",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = MultiTaskTrainDataLoader(selected_task, 32, False, 4)\n",
    "\n",
    "config = config_class.from_pretrained(model_name)\n",
    "tokenizer = tokenizer_class.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "33a2a852",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adb4457bba044b0db9d126e414900eeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6427 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6427 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1607 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i_epoch': 0, 'train_loss': 0.6752687707076435, 'train_acc': 0.5900925993919373, 'val_loss': 0.6748735675782087, 'val_acc': 0.5901769995689392}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "789b2181eaa646918958c0875954be9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6427 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6427 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1607 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i_epoch': 1, 'train_loss': 0.6693607444583342, 'train_acc': 0.5988796353340149, 'val_loss': 0.6689359912007867, 'val_acc': 0.6004862785339355}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8d2e11a2e0e4787bd30d94afeb98f21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6427 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6427 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1607 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i_epoch': 2, 'train_loss': 0.6668882590044554, 'train_acc': 0.6024197340011597, 'val_loss': 0.6664740512445728, 'val_acc': 0.6041431427001953}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d157dba33a89401c9616bfc91aa8bf93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6427 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6427 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1607 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i_epoch': 3, 'train_loss': 0.6650835614176025, 'train_acc': 0.6039320230484009, 'val_loss': 0.6646254126094839, 'val_acc': 0.6058354377746582}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04415966ecad40609bbc9d6acc6df7a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6427 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6427 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1607 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ff312b82d30><function _MultiProcessingDataLoaderIter.__del__ at 0x7ff312b82d30>\n",
      "\n",
      "Traceback (most recent call last):\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ff312b82d30>  File \"/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "          File \"/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "self._shutdown_workers()self._shutdown_workers()    \n",
      "\n",
      "self._shutdown_workers()  File \"/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "  File \"/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "\n",
      "  File \"/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "        if w.is_alive():\n",
      "if w.is_alive():  File \"/home/jz17d/anaconda3/envs/torch/lib/python3.9/multiprocessing/process.py\", line 160, in is_alive\n",
      "    if w.is_alive():    \n",
      "assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "\n",
      "AssertionError  File \"/home/jz17d/anaconda3/envs/torch/lib/python3.9/multiprocessing/process.py\", line 160, in is_alive\n",
      "      File \"/home/jz17d/anaconda3/envs/torch/lib/python3.9/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process': \n",
      "AssertionError: can only test a child process\n",
      "can only test a child processassert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "\n",
      "AssertionError: can only test a child process\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i_epoch': 4, 'train_loss': 0.6644202125413103, 'train_acc': 0.6025412678718567, 'val_loss': 0.6639701425878484, 'val_acc': 0.6020035147666931}\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "num_epoch = 5\n",
    "\n",
    "mt_model = MultiTaskBert(config, selected_task).to(device)\n",
    "for param in mt_model.basemodel.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "optimizer = optim.AdamW(mt_model.parameters(), lr=5e-5)\n",
    "# optimizer = optim.SGD(mt_model.parameters(), lr=0.001)\n",
    "\n",
    "num_warmup_steps=500\n",
    "num_training_steps=num_epoch*len(train_dataloader)\n",
    "def lr_lambda(current_step: int):\n",
    "    if current_step < num_warmup_steps:\n",
    "        return float(current_step) / float(max(1, num_warmup_steps))\n",
    "    return max(\n",
    "        0.0, float(num_training_steps - current_step) / float(max(1, num_training_steps - num_warmup_steps))\n",
    "    )\n",
    "scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda, -1)\n",
    "# scheduler = lr_scheduler.LinearLR(optimizer, start_factor=0.01, total_iters=500)\n",
    "# scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=5e-5, total_steps=num_epoch*len(train_dataloader)) \n",
    "\n",
    "df = pd.DataFrame(columns=['i_iter', 'train_loss', 'train_acc', 'val_loss', 'val_acc'])\n",
    "\n",
    "for i_epoch in range(num_epoch):\n",
    "    for i_iter, data in enumerate(tqdm(train_dataloader)):  \n",
    "\n",
    "        i_task, batch = data\n",
    "        optimizer.zero_grad()\n",
    "        label = batch['label'].to(device)\n",
    "        del batch['label']\n",
    "        tokens = tokenizer(**batch, return_tensors='pt', padding=True, truncation=True, max_length=64).to(device)\n",
    "        output, loss = mt_model(**tokens, i_task=i_task,  label=label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "#         if i_iter in [0, 10, 25, 50, 100]:\n",
    "#             train_loss, train_overall_acc, train_task_accs = validate(mt_model, 'train')\n",
    "#             val_loss, val_overall_acc, val_task_accs = validate(mt_model, 'val')\n",
    "#             result = {'i_iter':i_iter, 'train_loss':train_loss[0], 'train_acc':train_overall_acc.item(), 'val_loss':val_loss[0], 'val_acc':val_overall_acc.item()}\n",
    "#             df = df.append(result , ignore_index=True)\n",
    "#             print(result)\n",
    "#         if i_iter>100:\n",
    "#             break\n",
    "\n",
    "    train_loss, train_overall_acc, train_task_accs = validate(mt_model, selected_task, split='train')\n",
    "    val_loss, val_overall_acc, val_task_accs = validate(mt_model, selected_task, split='val')\n",
    "    result = {'i_epoch':i_epoch, 'train_loss':train_loss[0], 'train_acc':train_overall_acc.item(), 'val_loss':val_loss[0], 'val_acc':val_overall_acc.item()}\n",
    "    df = df.append(result , ignore_index=True)\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "46b0aa52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'base_lrs': [5e-05],\n",
       " 'last_epoch': 9380,\n",
       " '_step_count': 9381,\n",
       " 'verbose': False,\n",
       " '_get_lr_called_within_step': False,\n",
       " '_last_lr': [0.0],\n",
       " 'lr_lambdas': [None]}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scheduler.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "10a5e2f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>i_iter</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>i_epoch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.675269</td>\n",
       "      <td>0.590093</td>\n",
       "      <td>0.674874</td>\n",
       "      <td>0.590177</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.669361</td>\n",
       "      <td>0.598880</td>\n",
       "      <td>0.668936</td>\n",
       "      <td>0.600486</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.666888</td>\n",
       "      <td>0.602420</td>\n",
       "      <td>0.666474</td>\n",
       "      <td>0.604143</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.665084</td>\n",
       "      <td>0.603932</td>\n",
       "      <td>0.664625</td>\n",
       "      <td>0.605835</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.664420</td>\n",
       "      <td>0.602541</td>\n",
       "      <td>0.663970</td>\n",
       "      <td>0.602004</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   i_iter  train_loss  train_acc  val_loss   val_acc  i_epoch\n",
       "0     NaN    0.675269   0.590093  0.674874  0.590177      0.0\n",
       "1     NaN    0.669361   0.598880  0.668936  0.600486      1.0\n",
       "2     NaN    0.666888   0.602420  0.666474  0.604143      2.0\n",
       "3     NaN    0.665084   0.603932  0.664625  0.605835      3.0\n",
       "4     NaN    0.664420   0.602541  0.663970  0.602004      4.0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9130452d",
   "metadata": {},
   "source": [
    "#### unfreeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7f015390",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_task = [\n",
    "#                 'SarcasmGhosh',\n",
    "                'SARC',\n",
    "    \n",
    "                ] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "822baca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = MultiTaskTrainDataLoader(selected_task, 32, False, 4)\n",
    "\n",
    "config = config_class.from_pretrained(model_name)\n",
    "tokenizer = tokenizer_class.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4f2d854c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a52942543c6e493b9aeaa143250c35d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6427 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6427 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1607 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i_epoch': 0, 'train_loss': 0.5204895184541661, 'train_acc': 0.7440722584724426, 'val_loss': 0.5617923329948242, 'val_acc': 0.7100952863693237}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2004021697c4c91a604ecdb9384318b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6427 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6427 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1607 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i_epoch': 1, 'train_loss': 0.4135907814818228, 'train_acc': 0.8152292370796204, 'val_loss': 0.5866683719014871, 'val_acc': 0.7142579555511475}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d8aec73d1a043f7bce25cf936bbab24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6427 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6427 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1607 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i_epoch': 2, 'train_loss': 0.26873430512040186, 'train_acc': 0.8927320837974548, 'val_loss': 0.713898151434713, 'val_acc': 0.7076444029808044}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a388af445ac445d38483d4ed7207084d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6427 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6427 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1607 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i_epoch': 3, 'train_loss': 0.1376631868924303, 'train_acc': 0.9499280452728271, 'val_loss': 0.8818678910805079, 'val_acc': 0.7074499130249023}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "368d2a97f3434c67b4dca3fa421af15a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6427 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6427 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1607 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f43cc4f3d30>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "Exception ignored in: Exception ignored in:     self._shutdown_workers()\n",
      "<function _MultiProcessingDataLoaderIter.__del__ at 0x7f43cc4f3d30>  File \"/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "<function _MultiProcessingDataLoaderIter.__del__ at 0x7f43cc4f3d30>\n",
      "\n",
      "Traceback (most recent call last):\n",
      "    if w.is_alive():\n",
      "  File \"/home/jz17d/anaconda3/envs/torch/lib/python3.9/multiprocessing/process.py\", line 160, in is_alive\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "          File \"/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'self._shutdown_workers()\n",
      "\n",
      "  File \"/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "AssertionError    : if w.is_alive():can only test a child process\n",
      "\n",
      "  File \"/home/jz17d/anaconda3/envs/torch/lib/python3.9/multiprocessing/process.py\", line 160, in is_alive\n",
      "self._shutdown_workers()    \n",
      "assert self._parent_pid == os.getpid(), 'can only test a child process'  File \"/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "\n",
      "    : AssertionErrorcan only test a child processif w.is_alive():\n",
      "\n",
      "  File \"/home/jz17d/anaconda3/envs/torch/lib/python3.9/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i_epoch': 4, 'train_loss': 0.08787503195361171, 'train_acc': 0.9715479016304016, 'val_loss': 0.9523796078389406, 'val_acc': 0.7084224820137024}\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "num_epoch = 5\n",
    "\n",
    "mt_model = MultiTaskBert(config, selected_task).to(device)\n",
    "# for param in mt_model.basemodel.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "optimizer = optim.AdamW(mt_model.parameters(), lr=5e-5)\n",
    "# optimizer = optim.SGD(mt_model.parameters(), lr=0.001)\n",
    "\n",
    "# scheduler = lr_scheduler.LinearLR(optimizer, start_factor=0.01, total_iters=500)\n",
    "scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=5e-5, total_steps=num_epoch*len(train_dataloader)) \n",
    "\n",
    "df = pd.DataFrame(columns=['i_iter', 'train_loss', 'train_acc', 'val_loss', 'val_acc'])\n",
    "\n",
    "for i_epoch in range(num_epoch):\n",
    "    for i_iter, data in enumerate(tqdm(train_dataloader)):  \n",
    "\n",
    "        i_task, batch = data\n",
    "        optimizer.zero_grad()\n",
    "        label = batch['label'].to(device)\n",
    "        del batch['label']\n",
    "        tokens = tokenizer(**batch, return_tensors='pt', padding=True, truncation=True, max_length=64).to(device)\n",
    "        output, loss = mt_model(**tokens, i_task=i_task,  label=label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "#         if i_iter in [0, 10, 25, 50, 100]:\n",
    "#             train_loss, train_overall_acc, train_task_accs = validate(mt_model, 'train')\n",
    "#             val_loss, val_overall_acc, val_task_accs = validate(mt_model, 'val')\n",
    "#             result = {'i_iter':i_iter, 'train_loss':train_loss[0], 'train_acc':train_overall_acc.item(), 'val_loss':val_loss[0], 'val_acc':val_overall_acc.item()}\n",
    "#             df = df.append(result , ignore_index=True)\n",
    "#             print(result)\n",
    "#         if i_iter>100:\n",
    "#             break\n",
    "\n",
    "    train_loss, train_overall_acc, train_task_accs = validate(mt_model, selected_task,'train')\n",
    "    val_loss, val_overall_acc, val_task_accs = validate(mt_model, selected_task, 'val')\n",
    "    result = {'i_epoch':i_epoch, 'train_loss':train_loss[0], 'train_acc':train_overall_acc.item(), 'val_loss':val_loss[0], 'val_acc':val_overall_acc.item()}\n",
    "    df = df.append(result , ignore_index=True)\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d1755c53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>i_iter</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>i_epoch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.520490</td>\n",
       "      <td>0.744072</td>\n",
       "      <td>0.561792</td>\n",
       "      <td>0.710095</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.413591</td>\n",
       "      <td>0.815229</td>\n",
       "      <td>0.586668</td>\n",
       "      <td>0.714258</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.268734</td>\n",
       "      <td>0.892732</td>\n",
       "      <td>0.713898</td>\n",
       "      <td>0.707644</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.137663</td>\n",
       "      <td>0.949928</td>\n",
       "      <td>0.881868</td>\n",
       "      <td>0.707450</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.087875</td>\n",
       "      <td>0.971548</td>\n",
       "      <td>0.952380</td>\n",
       "      <td>0.708422</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   i_iter  train_loss  train_acc  val_loss   val_acc  i_epoch\n",
       "0     NaN    0.520490   0.744072  0.561792  0.710095      0.0\n",
       "1     NaN    0.413591   0.815229  0.586668  0.714258      1.0\n",
       "2     NaN    0.268734   0.892732  0.713898  0.707644      2.0\n",
       "3     NaN    0.137663   0.949928  0.881868  0.707450      3.0\n",
       "4     NaN    0.087875   0.971548  0.952380  0.708422      4.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f61ac0",
   "metadata": {},
   "source": [
    "## multi task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c463e674",
   "metadata": {},
   "source": [
    "### ShortHumor + SARC freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "af773f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ad5b7ad6510420da5cf975b0ed9f799",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1876 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1876 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i_epoch': 0, 'train_loss': 1.2588409416063957, 'train_acc': 0.6209677457809448, 'val_loss': 1.2555277911088591, 'val_acc': 0.5460400581359863, 'train_loss_SARC': 0.689106840099025, 'train_loss_ShortHumor': 0.5697341015073706, 'train_acc_ShortHumor': 0.5322515964508057, 'train_acc_SARC': 0.7093533873558044, 'val_loss_ShortHumor': 0.5662161913774137, 'val_loss_SARC': 0.6893115997314453, 'val_acc_ShortHumor': 0.7044881582260132, 'val_acc_SARC': 0.5355666875839233}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b0d1ff796054365a6a3e697007b94d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1876 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1876 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i_epoch': 1, 'train_loss': 1.233905949858753, 'train_acc': 0.6412623524665833, 'val_loss': 1.233718678321877, 'val_acc': 0.5660507082939148, 'train_loss_ShortHumor': 0.5492658669096266, 'train_loss_SARC': 0.6846400829491264, 'train_acc_ShortHumor': 0.7260255813598633, 'train_acc_SARC': 0.5563634037971497, 'val_loss_ShortHumor': 0.5489392152532326, 'val_loss_SARC': 0.6847794630686442, 'val_acc_ShortHumor': 0.7221381664276123, 'val_acc_SARC': 0.555733323097229}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7864b9f9db744a1184701bd4f2d54532",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1876 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1876 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i_epoch': 2, 'train_loss': 1.2160551427783572, 'train_acc': 0.6440615653991699, 'val_loss': 1.2155566978646477, 'val_acc': 0.5639558434486389, 'train_loss_SARC': 0.6834724632036471, 'train_loss_ShortHumor': 0.5325826795747102, 'train_acc_ShortHumor': 0.5536498427391052, 'train_acc_SARC': 0.7403978705406189, 'val_loss_ShortHumor': 0.5320815655900202, 'val_loss_SARC': 0.6834751322746276, 'val_acc_ShortHumor': 0.7286939024925232, 'val_acc_SARC': 0.5530666708946228}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eae9f47d00ef4643823802c355c34912",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1876 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1876 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i_epoch': 3, 'train_loss': 1.2075021417780603, 'train_acc': 0.6585410833358765, 'val_loss': 1.2078232778113958, 'val_acc': 0.5783385038375854, 'train_loss_SARC': 0.6810706823840658, 'train_loss_ShortHumor': 0.5264314593939946, 'train_acc_ShortHumor': 0.5702371597290039, 'train_acc_SARC': 0.7459549307823181, 'val_loss_ShortHumor': 0.5266925804974513, 'val_loss_SARC': 0.6811306973139445, 'val_acc_ShortHumor': 0.7367624640464783, 'val_acc_SARC': 0.5678666830062866}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e46e13d275774e6f8512174996cf0acd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1876 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1876 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i_epoch': 4, 'train_loss': 1.205312835984088, 'train_acc': 0.6587243676185608, 'val_loss': 1.2054009467225453, 'val_acc': 0.5861238837242126, 'train_loss_SARC': 0.6804667345453256, 'train_loss_ShortHumor': 0.5248461014387623, 'train_acc_ShortHumor': 0.5764569640159607, 'train_acc_SARC': 0.7467577457427979, 'val_loss_ShortHumor': 0.5249551163138131, 'val_loss_SARC': 0.6804458304087321, 'val_acc_ShortHumor': 0.738779604434967, 'val_acc_SARC': 0.576033353805542}\n"
     ]
    }
   ],
   "source": [
    "##################### settings #####################\n",
    "selected_task = [\n",
    "                'ShortHumor',\n",
    "                'SARC',\n",
    "                ] \n",
    "num_epoch = 5\n",
    "freeze_bert = True\n",
    "limit = 30000 # set the maximum number of data in each dataset \n",
    "####################################################\n",
    "\n",
    "train_dataloader = MultiTaskTrainDataLoader(selected_task, 32, False, 4, limit = limit)\n",
    "config = config_class.from_pretrained(model_name)\n",
    "tokenizer = tokenizer_class.from_pretrained(model_name)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "mt_model = MultiTaskBert(config, selected_task).to(device)\n",
    "if freeze_bert:\n",
    "    for param in mt_model.basemodel.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "optimizer = optim.AdamW(mt_model.parameters(), lr=5e-5)\n",
    "\n",
    "scheduler = get_scheduler(\"linear\",\n",
    "                            optimizer=optimizer,\n",
    "                            num_warmup_steps=500,\n",
    "                            num_training_steps=num_epoch*len(train_dataloader))\n",
    "\n",
    "# create df to save metrics\n",
    "columns = ['i_epoch', 'train_loss'] + [f'train_loss_{selected_task[i]}' for i in range(len(selected_task))]\n",
    "columns += ['train_acc'] + [f'train_acc_{selected_task[i]}' for i in range(len(selected_task))]\n",
    "columns += ['val_loss'] + [f'val_loss_{selected_task[i]}' for i in range(len(selected_task))]\n",
    "columns += ['val_acc'] + [f'val_acc_{selected_task[i]}' for i in range(len(selected_task))]\n",
    "df = pd.DataFrame(columns=columns)\n",
    "\n",
    "for i_epoch in range(num_epoch):\n",
    "    for i_iter, data in enumerate(tqdm(train_dataloader)):  \n",
    "        i_task, batch = data\n",
    "        optimizer.zero_grad()\n",
    "        label = batch['label'].to(device)\n",
    "        del batch['label']\n",
    "        tokens = tokenizer(**batch, return_tensors='pt', padding=True, truncation=True, max_length=64).to(device)\n",
    "        output, loss = mt_model(**tokens, i_task=i_task,  label=label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "#         if i_iter in [0, 10, 25, 50, 100]:\n",
    "#             train_loss, train_overall_acc, train_task_accs = validate(mt_model, 'train')\n",
    "#             val_loss, val_overall_acc, val_task_accs = validate(mt_model, 'val')\n",
    "#             result = {'i_iter':i_iter, 'train_loss':train_loss[0], 'train_acc':train_overall_acc.item(), 'val_loss':val_loss[0], 'val_acc':val_overall_acc.item()}\n",
    "#             df = df.append(result , ignore_index=True)\n",
    "#             print(result)\n",
    "#         if i_iter>100:\n",
    "#             break\n",
    "    \n",
    "    # run evaluation on train and validation set \n",
    "    train_loss, train_overall_acc, train_task_accs = validate(mt_model, selected_task,'train', limit = limit)\n",
    "    val_loss, val_overall_acc, val_task_accs = validate(mt_model, selected_task, 'val', limit = limit)\n",
    "    \n",
    "    # collect result\n",
    "    result = {'i_epoch':i_epoch, f'train_loss':sum(train_loss.values()), 'train_acc':train_overall_acc.item(), 'val_loss':sum(val_loss.values()), 'val_acc':val_overall_acc.item()}\n",
    "    result.update({f'train_loss_{selected_task[i]}':train_loss[i] for i in train_loss})\n",
    "    result.update({f'train_acc_{selected_task[i]}':train_task_accs[i].item() for i in range(len(train_task_accs))})   \n",
    "    result.update({f'val_loss_{selected_task[i]}':val_loss[i] for i in val_loss})\n",
    "    result.update({f'val_acc_{selected_task[i]}':val_task_accs[i].item() for i in range(len(val_task_accs))})   \n",
    "    df = df.append(result , ignore_index=True)\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cb18722b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>i_epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_loss_ShortHumor</th>\n",
       "      <th>train_loss_SARC</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>train_acc_ShortHumor</th>\n",
       "      <th>train_acc_SARC</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_lossShortHumor</th>\n",
       "      <th>val_lossSARC</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>val_accShortHumor</th>\n",
       "      <th>val_accSARC</th>\n",
       "      <th>val_acc_SARC</th>\n",
       "      <th>val_acc_ShortHumor</th>\n",
       "      <th>val_loss_SARC</th>\n",
       "      <th>val_loss_ShortHumor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.258841</td>\n",
       "      <td>0.569734</td>\n",
       "      <td>0.689107</td>\n",
       "      <td>0.620968</td>\n",
       "      <td>0.532252</td>\n",
       "      <td>0.709353</td>\n",
       "      <td>1.255528</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.546040</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.535567</td>\n",
       "      <td>0.704488</td>\n",
       "      <td>0.689312</td>\n",
       "      <td>0.566216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.233906</td>\n",
       "      <td>0.549266</td>\n",
       "      <td>0.684640</td>\n",
       "      <td>0.641262</td>\n",
       "      <td>0.726026</td>\n",
       "      <td>0.556363</td>\n",
       "      <td>1.233719</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.566051</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.555733</td>\n",
       "      <td>0.722138</td>\n",
       "      <td>0.684779</td>\n",
       "      <td>0.548939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.216055</td>\n",
       "      <td>0.532583</td>\n",
       "      <td>0.683472</td>\n",
       "      <td>0.644062</td>\n",
       "      <td>0.553650</td>\n",
       "      <td>0.740398</td>\n",
       "      <td>1.215557</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.563956</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.553067</td>\n",
       "      <td>0.728694</td>\n",
       "      <td>0.683475</td>\n",
       "      <td>0.532082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.207502</td>\n",
       "      <td>0.526431</td>\n",
       "      <td>0.681071</td>\n",
       "      <td>0.658541</td>\n",
       "      <td>0.570237</td>\n",
       "      <td>0.745955</td>\n",
       "      <td>1.207823</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.578339</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.567867</td>\n",
       "      <td>0.736762</td>\n",
       "      <td>0.681131</td>\n",
       "      <td>0.526693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1.205313</td>\n",
       "      <td>0.524846</td>\n",
       "      <td>0.680467</td>\n",
       "      <td>0.658724</td>\n",
       "      <td>0.576457</td>\n",
       "      <td>0.746758</td>\n",
       "      <td>1.205401</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.586124</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.576033</td>\n",
       "      <td>0.738780</td>\n",
       "      <td>0.680446</td>\n",
       "      <td>0.524955</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   i_epoch  train_loss  train_loss_ShortHumor  train_loss_SARC  train_acc  \\\n",
       "0      0.0    1.258841               0.569734         0.689107   0.620968   \n",
       "1      1.0    1.233906               0.549266         0.684640   0.641262   \n",
       "2      2.0    1.216055               0.532583         0.683472   0.644062   \n",
       "3      3.0    1.207502               0.526431         0.681071   0.658541   \n",
       "4      4.0    1.205313               0.524846         0.680467   0.658724   \n",
       "\n",
       "   train_acc_ShortHumor  train_acc_SARC  val_loss  val_lossShortHumor  \\\n",
       "0              0.532252        0.709353  1.255528                 NaN   \n",
       "1              0.726026        0.556363  1.233719                 NaN   \n",
       "2              0.553650        0.740398  1.215557                 NaN   \n",
       "3              0.570237        0.745955  1.207823                 NaN   \n",
       "4              0.576457        0.746758  1.205401                 NaN   \n",
       "\n",
       "   val_lossSARC   val_acc  val_accShortHumor  val_accSARC  val_acc_SARC  \\\n",
       "0           NaN  0.546040                NaN          NaN      0.535567   \n",
       "1           NaN  0.566051                NaN          NaN      0.555733   \n",
       "2           NaN  0.563956                NaN          NaN      0.553067   \n",
       "3           NaN  0.578339                NaN          NaN      0.567867   \n",
       "4           NaN  0.586124                NaN          NaN      0.576033   \n",
       "\n",
       "   val_acc_ShortHumor  val_loss_SARC  val_loss_ShortHumor  \n",
       "0            0.704488       0.689312             0.566216  \n",
       "1            0.722138       0.684779             0.548939  \n",
       "2            0.728694       0.683475             0.532082  \n",
       "3            0.736762       0.681131             0.526693  \n",
       "4            0.738780       0.680446             0.524955  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a709739",
   "metadata": {},
   "source": [
    "### ShortHumor + SARC unfreeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2a0c8d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0afde84fddf3401491173433e18823b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1876 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1876 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i_epoch': 0, 'train_loss': 0.5597279640742069, 'train_acc': 0.8793488144874573, 'val_loss': 0.7281250772883059, 'val_acc': 0.704968273639679, 'train_loss_SARC': 0.4994337105401564, 'train_loss_ShortHumor': 0.0602942535340505, 'train_acc_ShortHumor': 0.7725638151168823, 'train_acc_SARC': 0.98238605260849, 'val_loss_ShortHumor': 0.14456106155013654, 'val_loss_SARC': 0.5835640157381694, 'val_acc_ShortHumor': 0.9379727840423584, 'val_acc_SARC': 0.6895666718482971}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7f40e150da84461994a737ddf3a51d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1876 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1876 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i_epoch': 1, 'train_loss': 0.3492017807090173, 'train_acc': 0.9272527098655701, 'val_loss': 0.8544494809125447, 'val_acc': 0.6973704695701599, 'train_loss_ShortHumor': 0.024158018609146632, 'train_loss_SARC': 0.3250437620998707, 'train_acc_ShortHumor': 0.9923270344734192, 'train_acc_SARC': 0.8677259087562561, 'val_loss_ShortHumor': 0.18864742008592525, 'val_loss_SARC': 0.6658020608266194, 'val_acc_ShortHumor': 0.9485628008842468, 'val_acc_SARC': 0.6807666420936584}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81d3394808b543b7ac70f716083b7749",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1876 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1876 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i_epoch': 2, 'train_loss': 0.14585648414454147, 'train_acc': 0.97415691614151, 'val_loss': 1.120236882763575, 'val_acc': 0.6930869817733765, 'train_loss_SARC': 0.14131026145500633, 'train_loss_ShortHumor': 0.004546222689535153, 'train_acc_ShortHumor': 0.9495192170143127, 'train_acc_SARC': 0.9987027645111084, 'val_loss_ShortHumor': 0.1890764776363676, 'val_loss_SARC': 0.9311604051272074, 'val_acc_ShortHumor': 0.950075626373291, 'val_acc_SARC': 0.6761000156402588}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c8e3369fe6d4fe6bd21c24dfe384457",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1876 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1876 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i_epoch': 3, 'train_loss': 0.06999924572020612, 'train_acc': 0.9894194602966309, 'val_loss': 1.2681802114517926, 'val_acc': 0.6880217790603638, 'train_loss_SARC': 0.06785879645183454, 'train_loss_ShortHumor': 0.0021404492683715828, 'train_acc_ShortHumor': 0.9789506196975708, 'train_acc_SARC': 0.9994137287139893, 'val_loss_ShortHumor': 0.20113971268648279, 'val_loss_SARC': 1.0670404987653097, 'val_acc_ShortHumor': 0.9581442475318909, 'val_acc_SARC': 0.6701666712760925}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15a08b000c494fb395c25ef1d9bc0f10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1876 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1876 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i_epoch': 4, 'train_loss': 0.026761318635796446, 'train_acc': 0.9964342713356018, 'val_loss': 1.4792386742195043, 'val_acc': 0.6959322094917297, 'train_loss_ShortHumor': 0.0010979920536727916, 'train_loss_SARC': 0.025663326582123655, 'train_acc_ShortHumor': 0.9997988343238831, 'train_acc_SARC': 0.9931107759475708, 'val_loss_ShortHumor': 0.18398326656823877, 'val_loss_SARC': 1.2952554076512655, 'val_acc_ShortHumor': 0.9601613879203796, 'val_acc_SARC': 0.6784666776657104}\n"
     ]
    }
   ],
   "source": [
    "##################### settings #####################\n",
    "selected_task = [\n",
    "                'ShortHumor',\n",
    "                'SARC',\n",
    "                ] \n",
    "num_epoch = 5\n",
    "freeze_bert = False\n",
    "limit = 30000 # set the maximum number of data in each dataset \n",
    "####################################################\n",
    "\n",
    "train_dataloader = MultiTaskTrainDataLoader(selected_task, 32, False, 4, limit = limit)\n",
    "config = config_class.from_pretrained(model_name)\n",
    "tokenizer = tokenizer_class.from_pretrained(model_name)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "mt_model = MultiTaskBert(config, selected_task).to(device)\n",
    "if freeze_bert:\n",
    "    for param in mt_model.basemodel.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "optimizer = optim.AdamW(mt_model.parameters(), lr=5e-5)\n",
    "\n",
    "scheduler = get_scheduler(\"linear\",\n",
    "                            optimizer=optimizer,\n",
    "                            num_warmup_steps=500,\n",
    "                            num_training_steps=num_epoch*len(train_dataloader))\n",
    "\n",
    "# create df to save metrics\n",
    "columns = ['i_epoch', 'train_loss'] + [f'train_loss_{selected_task[i]}' for i in range(len(selected_task))]\n",
    "columns += ['train_acc'] + [f'train_acc_{selected_task[i]}' for i in range(len(selected_task))]\n",
    "columns += ['val_loss'] + [f'val_loss_{selected_task[i]}' for i in range(len(selected_task))]\n",
    "columns += ['val_acc'] + [f'val_acc_{selected_task[i]}' for i in range(len(selected_task))]\n",
    "df = pd.DataFrame(columns=columns)\n",
    "\n",
    "progress_bar = tqdm(range(num_epoch*len(train_dataloader)))\n",
    "for i_epoch in range(num_epoch):\n",
    "    for i_iter, data in enumerate(train_dataloader):  \n",
    "        i_task, batch = data\n",
    "        optimizer.zero_grad()\n",
    "        label = batch['label'].to(device)\n",
    "        del batch['label']\n",
    "        tokens = tokenizer(**batch, return_tensors='pt', padding=True, truncation=True, max_length=64).to(device)\n",
    "        output, loss = mt_model(**tokens, i_task=i_task,  label=label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        progress_bar.update(1)\n",
    "#         if i_iter in [0, 10, 25, 50, 100]:\n",
    "#             train_loss, train_overall_acc, train_task_accs = validate(mt_model, 'train')\n",
    "#             val_loss, val_overall_acc, val_task_accs = validate(mt_model, 'val')\n",
    "#             result = {'i_iter':i_iter, 'train_loss':train_loss[0], 'train_acc':train_overall_acc.item(), 'val_loss':val_loss[0], 'val_acc':val_overall_acc.item()}\n",
    "#             df = df.append(result , ignore_index=True)\n",
    "#             print(result)\n",
    "#         if i_iter>100:\n",
    "#             break\n",
    "    \n",
    "    # run evaluation on train and validation set \n",
    "    train_loss, train_overall_acc, train_task_accs = validate(mt_model, selected_task,'train', limit = limit)\n",
    "    val_loss, val_overall_acc, val_task_accs = validate(mt_model, selected_task, 'val', limit = limit)\n",
    "    \n",
    "    # collect result\n",
    "    result = {'i_epoch':i_epoch, f'train_loss':sum(train_loss.values()), 'train_acc':train_overall_acc.item(), 'val_loss':sum(val_loss.values()), 'val_acc':val_overall_acc.item()}\n",
    "    result.update({f'train_loss_{selected_task[i]}':train_loss[i] for i in train_loss})\n",
    "    result.update({f'train_acc_{selected_task[i]}':train_task_accs[i].item() for i in range(len(train_task_accs))})   \n",
    "    result.update({f'val_loss_{selected_task[i]}':val_loss[i] for i in val_loss})\n",
    "    result.update({f'val_acc_{selected_task[i]}':val_task_accs[i].item() for i in range(len(val_task_accs))})   \n",
    "    df = df.append(result , ignore_index=True)\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eda7dcaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>i_epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_loss_ShortHumor</th>\n",
       "      <th>train_loss_SARC</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>train_acc_ShortHumor</th>\n",
       "      <th>train_acc_SARC</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_lossShortHumor</th>\n",
       "      <th>val_lossSARC</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>val_accShortHumor</th>\n",
       "      <th>val_accSARC</th>\n",
       "      <th>val_acc_SARC</th>\n",
       "      <th>val_acc_ShortHumor</th>\n",
       "      <th>val_loss_SARC</th>\n",
       "      <th>val_loss_ShortHumor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.559728</td>\n",
       "      <td>0.060294</td>\n",
       "      <td>0.499434</td>\n",
       "      <td>0.879349</td>\n",
       "      <td>0.772564</td>\n",
       "      <td>0.982386</td>\n",
       "      <td>0.728125</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.704968</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.689567</td>\n",
       "      <td>0.937973</td>\n",
       "      <td>0.583564</td>\n",
       "      <td>0.144561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.349202</td>\n",
       "      <td>0.024158</td>\n",
       "      <td>0.325044</td>\n",
       "      <td>0.927253</td>\n",
       "      <td>0.992327</td>\n",
       "      <td>0.867726</td>\n",
       "      <td>0.854449</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.697370</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.680767</td>\n",
       "      <td>0.948563</td>\n",
       "      <td>0.665802</td>\n",
       "      <td>0.188647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.145856</td>\n",
       "      <td>0.004546</td>\n",
       "      <td>0.141310</td>\n",
       "      <td>0.974157</td>\n",
       "      <td>0.949519</td>\n",
       "      <td>0.998703</td>\n",
       "      <td>1.120237</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.693087</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.676100</td>\n",
       "      <td>0.950076</td>\n",
       "      <td>0.931160</td>\n",
       "      <td>0.189076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.069999</td>\n",
       "      <td>0.002140</td>\n",
       "      <td>0.067859</td>\n",
       "      <td>0.989419</td>\n",
       "      <td>0.978951</td>\n",
       "      <td>0.999414</td>\n",
       "      <td>1.268180</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.688022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.670167</td>\n",
       "      <td>0.958144</td>\n",
       "      <td>1.067040</td>\n",
       "      <td>0.201140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.026761</td>\n",
       "      <td>0.001098</td>\n",
       "      <td>0.025663</td>\n",
       "      <td>0.996434</td>\n",
       "      <td>0.999799</td>\n",
       "      <td>0.993111</td>\n",
       "      <td>1.479239</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.695932</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.678467</td>\n",
       "      <td>0.960161</td>\n",
       "      <td>1.295255</td>\n",
       "      <td>0.183983</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   i_epoch  train_loss  train_loss_ShortHumor  train_loss_SARC  train_acc  \\\n",
       "0      0.0    0.559728               0.060294         0.499434   0.879349   \n",
       "1      1.0    0.349202               0.024158         0.325044   0.927253   \n",
       "2      2.0    0.145856               0.004546         0.141310   0.974157   \n",
       "3      3.0    0.069999               0.002140         0.067859   0.989419   \n",
       "4      4.0    0.026761               0.001098         0.025663   0.996434   \n",
       "\n",
       "   train_acc_ShortHumor  train_acc_SARC  val_loss  val_lossShortHumor  \\\n",
       "0              0.772564        0.982386  0.728125                 NaN   \n",
       "1              0.992327        0.867726  0.854449                 NaN   \n",
       "2              0.949519        0.998703  1.120237                 NaN   \n",
       "3              0.978951        0.999414  1.268180                 NaN   \n",
       "4              0.999799        0.993111  1.479239                 NaN   \n",
       "\n",
       "   val_lossSARC   val_acc  val_accShortHumor  val_accSARC  val_acc_SARC  \\\n",
       "0           NaN  0.704968                NaN          NaN      0.689567   \n",
       "1           NaN  0.697370                NaN          NaN      0.680767   \n",
       "2           NaN  0.693087                NaN          NaN      0.676100   \n",
       "3           NaN  0.688022                NaN          NaN      0.670167   \n",
       "4           NaN  0.695932                NaN          NaN      0.678467   \n",
       "\n",
       "   val_acc_ShortHumor  val_loss_SARC  val_loss_ShortHumor  \n",
       "0            0.937973       0.583564             0.144561  \n",
       "1            0.948563       0.665802             0.188647  \n",
       "2            0.950076       0.931160             0.189076  \n",
       "3            0.958144       1.067040             0.201140  \n",
       "4            0.960161       1.295255             0.183983  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57eb31f9",
   "metadata": {},
   "source": [
    "### PASTEL_country + SARC + ShortHumor freeze all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f98d2d11",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e606a4d514a4471b0f4cab2452ec2b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2814 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2814 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1130 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i_epoch': 0, 'train_loss': 1.3478879763034892, 'train_acc': 0.7459903359413147, 'val_loss': 1.351353927016269, 'val_acc': 0.5846247673034668, 'train_loss_SARC': 0.6901458172895996, 'train_loss_PASTEL_country': 0.10328395434886294, 'train_loss_ShortHumor': 0.5544582046650268, 'train_acc_PASTEL_country': 0.5218007564544678, 'train_acc_SARC': 0.9789002537727356, 'train_acc_ShortHumor': 0.7239354252815247, 'val_loss_PASTEL_country': 0.11057325167809728, 'val_loss_SARC': 0.690218567721049, 'val_loss_ShortHumor': 0.5505621076171229, 'val_acc_PASTEL_country': 0.9764025807380676, 'val_acc_SARC': 0.5213000178337097, 'val_acc_ShortHumor': 0.7221381664276123}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a7717cc002c4f7898b3019d394ec75d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2814 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2814 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1130 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i_epoch': 1, 'train_loss': 1.315136425109223, 'train_acc': 0.7626510858535767, 'val_loss': 1.3229866455977528, 'val_acc': 0.6278226971626282, 'train_loss_SARC': 0.6801249834743752, 'train_loss_ShortHumor': 0.5330846307033215, 'train_loss_PASTEL_country': 0.10192681093152628, 'train_acc_PASTEL_country': 0.5764173865318298, 'train_acc_SARC': 0.740421712398529, 'train_acc_ShortHumor': 0.9790101647377014, 'val_loss_PASTEL_country': 0.11147631177364302, 'val_loss_SARC': 0.6801381779670715, 'val_loss_ShortHumor': 0.5313721558570381, 'val_acc_PASTEL_country': 0.9764025807380676, 'val_acc_SARC': 0.5729333162307739, 'val_acc_ShortHumor': 0.7281895875930786}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dbbed32d0d64f598544c40eaa1e0b85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2814 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2814 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1130 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i_epoch': 2, 'train_loss': 1.3030687483747263, 'train_acc': 0.7674493789672852, 'val_loss': 1.3115802746861982, 'val_acc': 0.629123330116272, 'train_loss_SARC': 0.6796582365760059, 'train_loss_PASTEL_country': 0.10222911398486915, 'train_loss_ShortHumor': 0.5211813978138512, 'train_acc_PASTEL_country': 0.5743288993835449, 'train_acc_SARC': 0.978828489780426, 'train_acc_ShortHumor': 0.7496123909950256, 'val_loss_PASTEL_country': 0.11145547760505649, 'val_loss_SARC': 0.6797234460512797, 'val_loss_ShortHumor': 0.5204013510298621, 'val_acc_PASTEL_country': 0.9764025807380676, 'val_acc_SARC': 0.5738666653633118, 'val_acc_ShortHumor': 0.7377710342407227}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7922c2a24897462798822c044198beed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2814 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2814 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1130 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i_epoch': 3, 'train_loss': 1.2915832654363686, 'train_acc': 0.7729625701904297, 'val_loss': 1.3019626897497782, 'val_acc': 0.6373146176338196, 'train_loss_ShortHumor': 0.5124990444973008, 'train_loss_PASTEL_country': 0.10152204740968009, 'train_loss_SARC': 0.6775621735293876, 'train_acc_PASTEL_country': 0.7571437954902649, 'train_acc_SARC': 0.9790216684341431, 'train_acc_ShortHumor': 0.5837439894676208, 'val_loss_PASTEL_country': 0.11151475002713342, 'val_loss_SARC': 0.6775517244021098, 'val_loss_ShortHumor': 0.5128962153205352, 'val_acc_PASTEL_country': 0.9764025807380676, 'val_acc_SARC': 0.582966685295105, 'val_acc_ShortHumor': 0.7493696212768555}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8596ed5db3de4a06bd5ea69aa3dbbf1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2814 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2814 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1130 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i_epoch': 4, 'train_loss': 1.2907954024404542, 'train_acc': 0.774362325668335, 'val_loss': 1.3010049818080396, 'val_acc': 0.6396945118904114, 'train_loss_ShortHumor': 0.5133960778228087, 'train_loss_PASTEL_country': 0.10103028968157811, 'train_loss_SARC': 0.6763690349360674, 'train_acc_PASTEL_country': 0.7556447386741638, 'train_acc_SARC': 0.9791115522384644, 'train_acc_ShortHumor': 0.5883961319923401, 'val_loss_PASTEL_country': 0.11138038554800912, 'val_loss_SARC': 0.6762410214424134, 'val_loss_ShortHumor': 0.5133835748176171, 'val_acc_PASTEL_country': 0.9764025807380676, 'val_acc_SARC': 0.5862666964530945, 'val_acc_ShortHumor': 0.7428139448165894}\n"
     ]
    }
   ],
   "source": [
    "selected_task = ['PASTEL_country', # 33224\n",
    "                 'SARC', # 205645\n",
    "#                  'SarcasmGhosh', # 39780\n",
    "                 'ShortHumor', # 37801\n",
    "#                  'ShortJokeKaggle', # 406682\n",
    "#                  'ShortRomance', # 1902\n",
    "#                  'TroFi', # 3335\n",
    "#                  'VUA', # 15157\n",
    "                ] \n",
    "freeze_bert = True\n",
    "use_pooler=True\n",
    "num_epoch=5\n",
    "limit=30000\n",
    "\n",
    "df = train_multitask(selected_task, freeze_bert, use_pooler=use_pooler, num_epoch=num_epoch, limit=limit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aae49bd",
   "metadata": {},
   "source": [
    "### PASTEL_country + SARC + ShortHumor freeze all, no pooler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "97e7c19d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8b987880b32430aad39983320d4aa2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2814 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2814 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1130 [01:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i_epoch': 0, 'train_loss': 1.2177376822810455, 'train_acc': 0.8013020157814026, 'val_loss': 1.2296284687517374, 'val_acc': 0.6579034924507141, 'train_loss_PASTEL_country': 0.10260867291143409, 'train_loss_ShortHumor': 0.45339751350196145, 'train_loss_SARC': 0.66173149586765, 'train_acc_PASTEL_country': 0.9791006445884705, 'train_acc_SARC': 0.8129602670669556, 'train_acc_ShortHumor': 0.6054857969284058, 'val_loss_PASTEL_country': 0.11002312396138784, 'val_loss_SARC': 0.6615031373341879, 'val_loss_ShortHumor': 0.4581022074561617, 'val_acc_PASTEL_country': 0.9764025807380676, 'val_acc_SARC': 0.604033350944519, 'val_acc_ShortHumor': 0.8058497309684753}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d8ebd3571af4e3da7b48d71e0def87a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2814 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dc31cd27e7244a98339d86733931673",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2814 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1321163/975902564.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_multitask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mselected_task\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreeze_bert\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_pooler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_pooler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_1321163/349049848.py\u001b[0m in \u001b[0;36mtrain_multitask\u001b[0;34m(selected_task, freeze_bert, use_pooler, num_epoch, limit, save)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;31m# run evaluation on train and validation set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_overall_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_task_accs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmt_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mselected_task\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_overall_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_task_accs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmt_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mselected_task\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'val'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1321163/291156682.py\u001b[0m in \u001b[0;36mvalidate\u001b[0;34m(model, tokenizer, selected_task, split, limit)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi_task\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi_task\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0moverall_acc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mtask_accs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi_task\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/torchmetrics/metric.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    265\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_called\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_enable_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/torchmetrics/classification/accuracy.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, preds, target)\u001b[0m\n\u001b[1;32m    226\u001b[0m         \"\"\"\n\u001b[1;32m    227\u001b[0m         \u001b[0;34m\"\"\" returns the mode of the data (binary, multi label, multi class, multi-dim multi class) \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m         \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtop_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmulticlass\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/torchmetrics/functional/classification/accuracy.py\u001b[0m in \u001b[0;36m_mode\u001b[0;34m(preds, target, threshold, top_k, num_classes, multiclass)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \"\"\"\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     mode = _check_classification_inputs(\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtop_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulticlass\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmulticlass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     )\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/torchmetrics/utilities/checks.py\u001b[0m in \u001b[0;36m_check_classification_inputs\u001b[0;34m(preds, target, threshold, num_classes, multiclass, top_k)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;31m# Basic validation (that does not need case/type information)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     \u001b[0m_basic_input_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulticlass\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m     \u001b[0;31m# Check that shape/types fall into one of the cases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/torchmetrics/utilities/checks.py\u001b[0m in \u001b[0;36m_basic_input_validation\u001b[0;34m(preds, target, threshold, multiclass)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The `target` has to be an integer tensor.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The `target` has to be a non-negative tensor.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "selected_task = ['PASTEL_country', # 33224\n",
    "                 'SARC', # 205645\n",
    "#                  'SarcasmGhosh', # 39780\n",
    "                 'ShortHumor', # 37801\n",
    "#                  'ShortJokeKaggle', # 406682\n",
    "#                  'ShortRomance', # 1902\n",
    "#                  'TroFi', # 3335\n",
    "#                  'VUA', # 15157\n",
    "                ] \n",
    "freeze_bert = True\n",
    "use_pooler=False\n",
    "num_epoch=5\n",
    "limit=30000\n",
    "\n",
    "df = train_multitask(selected_task, freeze_bert, use_pooler=use_pooler, num_epoch=num_epoch, limit=limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b20cd9b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i_epoch:0\n",
      "train_loss:1.218\n",
      "train_acc:0.8013\n",
      "val_loss:1.23\n",
      "val_acc:0.6579\n",
      "train_loss_PASTEL_country:0.1026\n",
      "train_loss_ShortHumor:0.4534\n",
      "train_loss_SARC:0.6617\n",
      "train_acc_PASTEL_country:0.9791\n",
      "train_acc_SARC:0.813\n",
      "train_acc_ShortHumor:0.6055\n",
      "val_loss_PASTEL_country:0.11\n",
      "val_loss_SARC:0.6615\n",
      "val_loss_ShortHumor:0.4581\n",
      "val_acc_PASTEL_country:0.9764\n",
      "val_acc_SARC:0.604\n",
      "val_acc_ShortHumor:0.8058\n"
     ]
    }
   ],
   "source": [
    "d = {'i_epoch': 0, 'train_loss': 1.2177376822810455, 'train_acc': 0.8013020157814026, 'val_loss': 1.2296284687517374, 'val_acc': 0.6579034924507141, 'train_loss_PASTEL_country': 0.10260867291143409, 'train_loss_ShortHumor': 0.45339751350196145, 'train_loss_SARC': 0.66173149586765, 'train_acc_PASTEL_country': 0.9791006445884705, 'train_acc_SARC': 0.8129602670669556, 'train_acc_ShortHumor': 0.6054857969284058, 'val_loss_PASTEL_country': 0.11002312396138784, 'val_loss_SARC': 0.6615031373341879, 'val_loss_ShortHumor': 0.4581022074561617, 'val_acc_PASTEL_country': 0.9764025807380676, 'val_acc_SARC': 0.604033350944519, 'val_acc_ShortHumor': 0.8058497309684753}\n",
    "print('\\n'.join([f\"{k}:{v:.4}\" if isinstance(v, float) else f\"{k}:{v}\" for k,v in d.items()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3c8c70",
   "metadata": {},
   "source": [
    "### PASTEL_country + SARC + ShortHumor unfreeze all, no pooler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f3ec24",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_task = ['PASTEL_country', # 33224\n",
    "                 'SARC', # 205645\n",
    "#                  'SarcasmGhosh', # 39780\n",
    "                 'ShortHumor', # 37801\n",
    "#                  'ShortJokeKaggle', # 406682\n",
    "#                  'ShortRomance', # 1902\n",
    "#                  'TroFi', # 3335\n",
    "#                  'VUA', # 15157\n",
    "                ] \n",
    "freeze_bert = False\n",
    "use_pooler=False\n",
    "num_epoch=5\n",
    "limit=30000\n",
    "\n",
    "df = train_multitask(selected_task, freeze_bert, use_pooler=use_pooler, num_epoch=num_epoch, limit=limit)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "372.4px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
