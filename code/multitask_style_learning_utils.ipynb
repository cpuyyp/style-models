{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bde1c77e",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#functions\" data-toc-modified-id=\"functions-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>functions</a></span><ul class=\"toc-item\"><li><span><a href=\"#multi-task-dataloader\" data-toc-modified-id=\"multi-task-dataloader-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>multi-task dataloader</a></span></li><li><span><a href=\"#multi-task-model\" data-toc-modified-id=\"multi-task-model-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>multi-task model</a></span></li><li><span><a href=\"#train-and-validate\" data-toc-modified-id=\"train-and-validate-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>train and validate</a></span></li><li><span><a href=\"#bertology\" data-toc-modified-id=\"bertology-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>bertology</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5484ef75",
   "metadata": {},
   "source": [
    "# functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fccb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext pycodestyle_magic\n",
    "# %flake8_on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785f4cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import collections\n",
    "import json\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import List\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torchmetrics\n",
    "from transformers import *\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29182ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/huggingface/transformers/issues/5486\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186facb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8839076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary: task_name: number_of_labels\n",
    "with open('../data/xslue/tasks.json', 'r') as f:\n",
    "    tasks2labels = json.load(f)\n",
    "tasks2labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ae7ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary: task_name: task index\n",
    "tasks2idx = {k:i for i,k in enumerate(tasks2labels)}\n",
    "tasks2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d23f160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some binary tasks and their (train) dataset size \n",
    "selected_task = ['PASTEL_country', # 33224\n",
    "#                  'SARC', # 205645\n",
    "                 'SarcasmGhosh', # 39780\n",
    "                 'ShortHumor', # 37801\n",
    "#                  'ShortJokeKaggle', # 406682\n",
    "#                  'ShortRomance', # 1902\n",
    "#                  'TroFi', # 3335\n",
    "                 'VUA', # 15157\n",
    "                ] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02f71e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainingArgs:\n",
    "    # training args\n",
    "    selected_tasks: List\n",
    "    base_model_name: str \n",
    "    freeze_bert: bool\n",
    "    use_pooler: bool\n",
    "    num_epoch: int\n",
    "    lr: float = 5e-5\n",
    "    num_warmup_steps = 500\n",
    "    model_folder: str = None # this will be inferred based on tasks\n",
    "        \n",
    "    # data loader args\n",
    "    batch_size: int = 32\n",
    "    max_length: int = 64\n",
    "    shuffle: bool = False\n",
    "    num_workers: int = 4\n",
    "    data_limit: int = None # if not None, truncate dataset to keep only top {data_limit} rows\n",
    "    \n",
    "    # post training args\n",
    "    save_best: bool = True\n",
    "    load_best_at_end: bool = True\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        excute_time = datetime.now() \n",
    "        result_folder = '../result'\n",
    "        model_folder = f\"{result_folder}/{'+'.join(self.selected_tasks)}/{excute_time.now().strftime('%Y%m%d-%H:%M:%S')}\"\n",
    "        Path(model_folder).mkdir(parents=True, exist_ok=True)\n",
    "        self.model_folder = model_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318bd186",
   "metadata": {},
   "source": [
    "## multi-task dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e8f997",
   "metadata": {},
   "source": [
    "Test/validation dataloader consume dataset one by one, where as the train dataloader do it randomly. So the train dataloader is more complicated than test/validation dataloader. It must be able to reset a dataset once it is exhausted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00309940",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset): \n",
    "    # currently it's a Mapping-style dataset. Not sure if a Iterable-style dataset will be better\n",
    "    # limit: use to truncate dataset. This will drop rows after certain index. May influence label distribution.\n",
    "    def __init__(self, tsv_file, data_limit=None):\n",
    "        self.df = pd.read_csv(tsv_file, sep='\\t')\n",
    "        self.df = self.df.dropna()\n",
    "        self.df = self.df.reset_index(drop=True)\n",
    "        if self.df['label'].dtype == 'float64':\n",
    "            self.df['label'] = self.df['label'].astype('float32')\n",
    "        if data_limit:\n",
    "            self.df = self.df.iloc[:data_limit]\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        dataslice = self.df.iloc[idx]\n",
    "        sample = {'text':dataslice['text'], 'label':dataslice['label']}\n",
    "        return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6463e817",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskTrainDataLoader():\n",
    "    '''\n",
    "    Each time, a random integer selects a dataset and load a batch of data {text, label} from it. Return i_task and data\n",
    "    \n",
    "    a iterator\n",
    "    Known issue: large dataset may have not iterate once, small datasets may have been iterated many times\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, training_args):\n",
    "        self.tasks = training_args.selected_tasks\n",
    "        self.split = 'train'\n",
    "        self.batch_size = training_args.batch_size\n",
    "        self.shuffle = training_args.shuffle\n",
    "        self.num_workers = training_args.num_workers\n",
    "        self.data_limit = training_args.data_limit\n",
    "        \n",
    "        self.num_tasks = len(self.tasks)\n",
    "        self.datasets = []\n",
    "        self.dataloaders = []\n",
    "        self.dataloaderiters = []\n",
    "        self.len = 0\n",
    "        for task in self.tasks:\n",
    "            self.datasets.append(MyDataset(f'../data/xslue/processed/{self.split}/{task}.tsv', data_limit=self.data_limit))\n",
    "            self.dataloaders.append(DataLoader(self.datasets[-1], batch_size=self.batch_size, shuffle=self.shuffle, num_workers=self.num_workers)) \n",
    "            self.dataloaderiters.append(self.dataloaders[-1]._get_iterator())\n",
    "            self.len += len(self.dataloaders[-1])\n",
    "    def __len__(self):   \n",
    "        return self.len\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.n = 0\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        i_task = np.random.randint(self.num_tasks)\n",
    "        if self.n < self.len:\n",
    "            self.n += 1\n",
    "        else:\n",
    "            raise StopIteration\n",
    "        dataloaderiter = self.dataloaderiters[i_task]    \n",
    "        try: \n",
    "            batch = next(dataloaderiter)\n",
    "        except StopIteration:\n",
    "#             self.dataloaderiters[i_task]._reset(self.dataloaders[i_task])\n",
    "#             dataloaderiter = self.dataloaderiters[i_task]\n",
    "            self.dataloaderiters[i_task] = iter(self.dataloaders[i_task])\n",
    "            dataloaderiter = self.dataloaderiters[i_task]\n",
    "            batch = next(dataloaderiter)\n",
    "        return i_task, batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61019a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskTestDataLoader():\n",
    "    '''\n",
    "    Used for evaluation\n",
    "    \n",
    "    a generator\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, training_args, split):\n",
    "        assert split in ['train', 'dev', 'test'], 'not implemented'\n",
    "        self.tasks = training_args.selected_tasks\n",
    "        self.split = split\n",
    "        self.batch_size = training_args.batch_size\n",
    "        self.shuffle = training_args.shuffle\n",
    "        self.num_workers = training_args.num_workers\n",
    "        self.data_limit = training_args.data_limit\n",
    "        \n",
    "        self.num_tasks = len(self.tasks)\n",
    "        self.datasets = []\n",
    "        self.dataloaders = []\n",
    "        self.dataloaderiters = []\n",
    "        self.len = 0\n",
    "        for task in self.tasks:\n",
    "            self.datasets.append(MyDataset(f'../data/xslue/processed/{self.split}/{task}.tsv', data_limit=self.data_limit))\n",
    "            self.dataloaders.append(DataLoader(self.datasets[-1], batch_size=self.batch_size, shuffle=self.shuffle, num_workers=self.num_workers))\n",
    "            self.len += len(self.dataloaders[-1])\n",
    "        self.i_task = 0\n",
    "    def __len__(self):   \n",
    "        return self.len\n",
    "\n",
    "    def __iter__(self):\n",
    "        for i_task in range(self.num_tasks):\n",
    "            dataloader = self.dataloaders[i_task]\n",
    "            for batch in dataloader:\n",
    "                yield i_task, batch\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100a28b2",
   "metadata": {},
   "source": [
    "## multi-task model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957b24e7",
   "metadata": {},
   "source": [
    "Given selected tasks, the model will add corresponding classification heads on the top of pretrained bert/(other bert). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed50b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionHead(nn.Module):\n",
    "    def __init__(self, embedding_dim = 768, hidden_dim = 128):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.hidden = nn.Linear(embedding_dim, 1)\n",
    "        \n",
    "        self.loss_fn = nn.MSELoss()\n",
    "    def forward(self, sent_emb, label):\n",
    "        output = self.hidden(self.dropout(sent_emb)).squeeze(1)\n",
    "\n",
    "        loss = self.loss_fn(output, label)\n",
    "        return output, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c097a3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationHead(nn.Module):\n",
    "    def __init__(self, num_labels, embedding_dim = 768, hidden_dim = 128):\n",
    "        super().__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.hidden = nn.Linear(embedding_dim, self.num_labels)\n",
    "        \n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "    def forward(self, sent_emb, label):\n",
    "        output = self.hidden(self.dropout(sent_emb))\n",
    "        \n",
    "        loss = self.loss_fn(output.view(-1, self.num_labels), label.view(-1))\n",
    "        return output, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eec4f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskBert(PreTrainedModel):\n",
    "    def __init__(self, config, training_args):\n",
    "        super().__init__(config)\n",
    "#         self.training_args = training_args\n",
    "        self.use_pooler = training_args.use_pooler\n",
    "        self.basemodel = AutoModel.from_pretrained(training_args.base_model_name)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(training_args.base_model_name)\n",
    "        self.style_heads = nn.ModuleList()\n",
    "        \n",
    "        for task in training_args.selected_tasks:\n",
    "            if tasks2labels[task] == 1:\n",
    "                self.style_heads.append(RegressionHead())\n",
    "            else:\n",
    "                self.style_heads.append(ClassificationHead(tasks2labels[task]))\n",
    "    def forward(self, i_task=None, label=None, **kwargs):\n",
    "        output = self.basemodel(**kwargs)\n",
    "        if self.use_pooler and ('pooler_output' in output):\n",
    "            sent_emb = output['pooler_output']\n",
    "        else:\n",
    "            sent_emb = output['last_hidden_state'][:,0,:]\n",
    "        \n",
    "        logits, loss = self.style_heads[i_task](sent_emb, label) \n",
    "        return SequenceClassifierOutput(loss=loss, logits=logits, hidden_states=output.hidden_states, attentions=output.attentions)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c28749",
   "metadata": {},
   "source": [
    "## train and validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7439d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, training_args, split):\n",
    "    val_loss = collections.defaultdict(float)\n",
    "    val_size = collections.defaultdict(int)\n",
    "    overall_acc = torchmetrics.Accuracy() \n",
    "    task_accs = [torchmetrics.Accuracy() for i in range(len(selected_task))] \n",
    "    \n",
    "    mt_dataloader = MultiTaskTestDataLoader(training_args, split=split)\n",
    "        \n",
    "    model.eval()\n",
    "    for data in tqdm(mt_dataloader, leave=False):  \n",
    "        i_task, batch = data\n",
    "        label = batch['label'].to(device)\n",
    "        size = len(label)\n",
    "        del batch['label']\n",
    "        tokens = model.tokenizer(**batch, return_tensors='pt', padding=True, truncation=True, max_length=64).to(device)\n",
    "        output = model(**tokens, i_task=i_task,  label=label)\n",
    "        loss = output.loss\n",
    "        logits = output.logits\n",
    "        overall_acc.update(logits.to('cpu').detach(), label.to('cpu').detach())\n",
    "        task_accs[i_task].update(logits.to('cpu').detach(), label.to('cpu').detach())\n",
    "        tokens = None\n",
    "        output = None\n",
    "        val_loss[i_task] += loss.detach().item()*size\n",
    "        val_size[i_task] += size\n",
    "    \n",
    "    accs = []\n",
    "    for i_task in val_loss:\n",
    "        val_loss[i_task] /= val_size[i_task]\n",
    "        accs.append(task_accs[i_task].compute())\n",
    "    model.train()\n",
    "    \n",
    "    return val_loss, overall_acc.compute(), accs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42dac54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(training_args):\n",
    "    config = AutoConfig.from_pretrained(training_args.base_model_name) \n",
    "    model = MultiTaskBert(config, training_args).to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d116c58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_model(model, freeze_bert):\n",
    "    '''\n",
    "    if freeze_bert == True, freeze all layer. \n",
    "    if freeze_bert is a positive integer, freeze the bottom {freeze_bert} attention layers\n",
    "    negative integer should also work\n",
    "    '''\n",
    "    if freeze_bert==True:\n",
    "        for param in model.basemodel.parameters():\n",
    "            param.requires_grad = False\n",
    "    elif isinstance(freeze_bert, int):\n",
    "        for layer in model.basemodel.encoder.layer[:freeze_bert]: \n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = False  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3a1b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_best_model(model, model_folder):\n",
    "    model.load_state_dict(torch.load(f\"{model_folder}/pytorch_model.bin\"))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595b3799",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, training_args):\n",
    "    \n",
    "    # these two will be used frequently\n",
    "    model_folder = training_args.model_folder\n",
    "    selected_tasks = training_args.selected_tasks\n",
    "    \n",
    "    train_dataloader = MultiTaskTrainDataLoader(training_args)\n",
    "    num_training_steps = training_args.num_epoch*len(train_dataloader)\n",
    "    \n",
    "    optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
    "    scheduler = get_scheduler(\"linear\",\n",
    "                                optimizer=optimizer,\n",
    "                                num_warmup_steps=training_args.num_warmup_steps,\n",
    "                                num_training_steps=num_training_steps)\n",
    "\n",
    "    # create dataframes for logging\n",
    "    columns = ['i_epoch', 'train_loss'] + [f'train_loss_{selected_tasks[i]}' for i in range(len(selected_tasks))]\n",
    "    columns += ['train_acc'] + [f'train_acc_{selected_tasks[i]}' for i in range(len(selected_tasks))]\n",
    "    columns += ['val_loss'] + [f'val_loss_{selected_tasks[i]}' for i in range(len(selected_tasks))]\n",
    "    columns += ['val_acc'] + [f'val_acc_{selected_tasks[i]}' for i in range(len(selected_tasks))]\n",
    "    df_evaluation = pd.DataFrame(columns=columns)\n",
    "    df_loss_per_step = pd.DataFrame(columns=['i_epoch', 'i_iter', 'i_task', 'task_name', 'train_loss'])\n",
    "    \n",
    "    best_accuracy = 0.0\n",
    "    progress_bar = tqdm(range(num_training_steps))\n",
    "    for i_epoch in range(training_args.num_epoch):\n",
    "        for i_iter, data in enumerate(train_dataloader):  \n",
    "            i_task, batch = data\n",
    "            optimizer.zero_grad()\n",
    "            label = batch['label'].to(device)\n",
    "            del batch['label']\n",
    "            tokens = model.tokenizer(**batch, return_tensors='pt', padding=True, truncation=True, max_length=training_args.max_length).to(device)\n",
    "            output = model(**tokens, i_task=i_task,  label=label)\n",
    "            loss = output.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            # log per step\n",
    "            step_result = {'i_epoch':i_epoch, 'i_iter':i_iter, 'i_task':i_task, 'task_name':selected_tasks[i_task], 'train_loss':loss.item(),}\n",
    "            df_loss_per_step = df_loss_per_step.append(step_result , ignore_index=True)\n",
    "            progress_bar.update(1)\n",
    "        \n",
    "        # run evaluation on train and validation set \n",
    "        train_loss, train_overall_acc, train_task_accs = validate(model, training_args, split='train')\n",
    "        val_loss, val_overall_acc, val_task_accs = validate(model, training_args, split='dev')\n",
    "        \n",
    "        # save best model and corresponding opt and scheduler states to disk\n",
    "        if training_args.save_best and val_overall_acc.item() > best_accuracy: \n",
    "            torch.save(model.state_dict(), f\"{model_folder}/pytorch_model.bin\")\n",
    "            torch.save(optimizer.state_dict(), f\"{model_folder}/optimizer.pt\")\n",
    "            torch.save(scheduler.state_dict(), f\"{model_folder}/scheduler.pt\")\n",
    "            \n",
    "        # collect result\n",
    "        epoch_result = {'i_epoch':i_epoch, f'train_loss':sum(train_loss.values()), 'train_acc':train_overall_acc.item(), 'val_loss':sum(val_loss.values()), 'val_acc':val_overall_acc.item()}\n",
    "        epoch_result.update({f'train_loss_{selected_tasks[i]}':train_loss[i] for i in train_loss})\n",
    "        epoch_result.update({f'train_acc_{selected_tasks[i]}':train_task_accs[i].item() for i in range(len(train_task_accs))})   \n",
    "        epoch_result.update({f'val_loss_{selected_tasks[i]}':val_loss[i] for i in val_loss})\n",
    "        epoch_result.update({f'val_acc_{selected_tasks[i]}':val_task_accs[i].item() for i in range(len(val_task_accs))})   \n",
    "        df_evaluation = df_evaluation.append(epoch_result , ignore_index=True)\n",
    "#         print('\\n'.join([f\"{k}:{v:.4}\" if isinstance(v, float) else f\"{k}:{v}\" for k,v in result.items()]))\n",
    "    \n",
    "    # save to disk\n",
    "    if training_args.save_best:\n",
    "        with open(\"training_args.json\", \"w\") as outfile:\n",
    "            json.dump(dataclasses.asdict(traing_args), outfile)\n",
    "        config.to_json_file(f\"{model_folder}/config.json\")\n",
    "        df_evaluation.to_csv(f\"{model_folder}/evaluation.csv\", index=False)\n",
    "        df_loss_per_step.to_csv(f\"{model_folder}/loss_per_step.csv\", index=False)\n",
    "    \n",
    "    display(df_evaluation) \n",
    "#     display(df_loss_per_step) # this is too long, not approporate to show directly\n",
    "\n",
    "    if training_args.save_best and training_args.load_best_at_end:\n",
    "        model = load_best_model(model, model_folder)\n",
    "    return df_evaluation, df_loss_per_step, model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923cbd92",
   "metadata": {},
   "source": [
    "## bertology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa453ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(p):\n",
    "    \"\"\" Compute the entropy of a probability distribution \"\"\"\n",
    "    plogp = p * torch.log(p)\n",
    "    plogp[p == 0] = 0\n",
    "    return -plogp.sum(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208f7bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_heads_importance(\n",
    "    model, eval_dataloader, training_args, compute_entropy=True, compute_importance=True, head_mask=None, \n",
    "    dont_normalize_importance_by_layer = True, dont_normalize_global_importance=True\n",
    "):\n",
    "    \"\"\" This method shows how to compute:\n",
    "        - head attention entropy\n",
    "        - head importance scores according to http://arxiv.org/abs/1905.10650\n",
    "    \"\"\"\n",
    "    model_folder = training_args.model_folder\n",
    "    \n",
    "    # Prepare our tensors\n",
    "    n_layers, n_heads = model.basemodel.config.num_hidden_layers, model.basemodel.config.num_attention_heads\n",
    "    head_importance = torch.zeros(n_layers, n_heads).to(device)\n",
    "    attn_entropy = torch.zeros(n_layers, n_heads).to(device)\n",
    "\n",
    "    if head_mask is None:\n",
    "        head_mask = torch.ones(n_layers, n_heads).to(device)\n",
    "    head_mask.requires_grad_(requires_grad=True)\n",
    "    preds = None\n",
    "    labels = None\n",
    "    tot_tokens = 0.0\n",
    "\n",
    "    for step, batch in enumerate(tqdm(eval_dataloader, desc=\"Iteration\")):\n",
    "        i_task, batch = batch\n",
    "        label_ids = batch['label'].to(device)\n",
    "        size = len(label_ids)\n",
    "        del batch['label']\n",
    "        batch = model.tokenizer(**batch, return_tensors='pt', padding=True, truncation=True, max_length=64).to(device)\n",
    "        input_ids, input_mask, segment_ids = batch['input_ids'], batch['attention_mask'], batch['token_type_ids']\n",
    "        \n",
    "        # Do a forward pass (not with torch.no_grad() since we need gradients for importance score - see below)\n",
    "        outputs = model(i_task=i_task,\n",
    "            input_ids=input_ids, token_type_ids=segment_ids, attention_mask=input_mask, label=label_ids, head_mask=head_mask, \n",
    "            output_attentions = True, \n",
    "        )\n",
    "        loss, logits, all_attentions = (\n",
    "            outputs.loss,\n",
    "            outputs.logits,\n",
    "            outputs.attentions,\n",
    "        )  # Loss and logits are the first, attention the last\n",
    "        loss.backward()  # Backpropagate to populate the gradients in the head mask\n",
    "\n",
    "        if compute_entropy:\n",
    "            for layer, attn in enumerate(all_attentions):\n",
    "                masked_entropy = entropy(attn.detach()) * input_mask.float().unsqueeze(1)\n",
    "                attn_entropy[layer] += masked_entropy.sum(-1).sum(0).detach()\n",
    "\n",
    "        if compute_importance:\n",
    "            head_importance += head_mask.grad.abs().detach()\n",
    "\n",
    "        # Also store our logits/labels if we want to compute metrics afterwards\n",
    "        if preds is None:\n",
    "            preds = logits.detach().cpu().numpy()\n",
    "            labels = label_ids.detach().cpu().numpy()\n",
    "        else:\n",
    "            preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
    "            labels = np.append(labels, label_ids.detach().cpu().numpy(), axis=0)\n",
    "\n",
    "        tot_tokens += input_mask.float().detach().sum().data\n",
    "\n",
    "    # Normalize\n",
    "    attn_entropy /= tot_tokens\n",
    "    head_importance /= tot_tokens\n",
    "    # Layerwise importance normalization\n",
    "    if not dont_normalize_importance_by_layer:\n",
    "        exponent = 2\n",
    "        norm_by_layer = torch.pow(torch.pow(head_importance, exponent).sum(-1), 1 / exponent)\n",
    "        head_importance /= norm_by_layer.unsqueeze(-1) + 1e-20\n",
    "\n",
    "    if not dont_normalize_global_importance:\n",
    "        head_importance = (head_importance - head_importance.min()) / (head_importance.max() - head_importance.min())\n",
    "\n",
    "    # save matrices\n",
    "    np.save(os.path.join(model_folder, \"attn_entropy.npy\"), attn_entropy.detach().cpu().numpy())\n",
    "    np.save(os.path.join(model_folder, \"head_importance.npy\"), head_importance.detach().cpu().numpy())\n",
    "\n",
    "    head_ranks = torch.zeros(head_importance.numel(), dtype=torch.long, device=device)\n",
    "    head_ranks[head_importance.view(-1).sort(descending=True)[1]] = torch.arange(\n",
    "        head_importance.numel(), device=device\n",
    "    )\n",
    "    head_ranks = head_ranks.view_as(head_importance)\n",
    "    \n",
    "    return attn_entropy, head_importance, preds, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad3c7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(torch_mat):\n",
    "    plt.imshow(torch_mat.detach().cpu().numpy())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22ff9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_args = TrainingArgs(selected_tasks=['VUA'],\n",
    "#                              base_model_name='bert-base-uncased',\n",
    "#                              freeze_bert=True,\n",
    "#                              use_pooler=True,\n",
    "#                              num_epoch=5,\n",
    "#                              data_limit=30000,\n",
    "#                             )\n",
    "\n",
    "# model = init_model(training_args)\n",
    "# freeze_model(model, training_args.freeze_bert)\n",
    "# df_evaluation, df_loss_per_step, model = train_model(model, training_args)\n",
    "\n",
    "# eval_dataloader = MultiTaskTestDataLoader(training_args, split='dev')\n",
    "# attn_entropy, head_importance, preds, labels = compute_heads_importance(model, eval_dataloader, training_args)\n",
    "\n",
    "# imshow(attn_entropy)\n",
    "# imshow(head_importance)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "372.4px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
