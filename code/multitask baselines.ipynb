{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "685574ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torchmetrics\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "import collections\n",
    "import pandas as pd\n",
    "import json\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "from datasets import load_metric\n",
    "import datasets\n",
    "from transformers import AutoConfig, AutoTokenizer, BertModel, RobertaModel\n",
    "from transformers import BertForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96c00161",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(f'{os.getcwd()}/SentEval')\n",
    "PATH_TO_DATA = f'{os.getcwd()}/SentEval/data'\n",
    "\n",
    "# Import SentEval\n",
    "import senteval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "391a036e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/huggingface/transformers/issues/5486\n",
    "# os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false-\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63bb1908",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22a33f8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CrowdFlower': 13,\n",
       " 'DailyDialog': 7,\n",
       " 'EmoBank_Valence': 1,\n",
       " 'EmoBank_Arousal': 1,\n",
       " 'EmoBank_Dominance': 1,\n",
       " 'HateOffensive': 3,\n",
       " 'PASTEL_age': 8,\n",
       " 'PASTEL_country': 2,\n",
       " 'PASTEL_education': 10,\n",
       " 'PASTEL_ethnic': 10,\n",
       " 'PASTEL_gender': 3,\n",
       " 'PASTEL_politics': 3,\n",
       " 'PASTEL_tod': 5,\n",
       " 'SARC': 2,\n",
       " 'SarcasmGhosh': 2,\n",
       " 'SentiTreeBank': 1,\n",
       " 'ShortHumor': 2,\n",
       " 'ShortJokeKaggle': 2,\n",
       " 'ShortRomance': 2,\n",
       " 'StanfordPoliteness': 1,\n",
       " 'TroFi': 2,\n",
       " 'VUA': 2}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('tasks.json', 'r') as f:\n",
    "    tasks = json.load(f)\n",
    "tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c3d477c",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abf7e4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset): \n",
    "    # currently it's a Mapping-style dataset. Not sure if a Iterable-style dataset will be better\n",
    "    def __init__(self, tsv_file):\n",
    "        self.df = pd.read_csv(tsv_file, sep='\\t')\n",
    "        self.df = self.df.dropna()\n",
    "        self.df = self.df.reset_index(drop=True)\n",
    "        self.encodings = tokenizer(self.df['text'].tolist(), truncation=True, padding=True, max_length=128)\n",
    "        if self.df['label'].dtype == 'float64':\n",
    "            self.df['label'] = self.df['label'].astype('float32')\n",
    "        self.labels = self.df['label'].tolist()\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor([self.labels[idx]])\n",
    "        return item\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19bae373",
   "metadata": {},
   "outputs": [],
   "source": [
    "pearsonr = load_metric(\"pearsonr\")\n",
    "spearmanr = load_metric(\"spearmanr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7bd4661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU memory usage: 6617 - 6680mb with bs 32\n",
    "# bs 64 gives OOM\n",
    "# bs 48 GPU memory 7894\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb9aaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_train(task):\n",
    "    torch.cuda.empty_cache()\n",
    "    model = None\n",
    "    trainer = None \n",
    "    \n",
    "    num_labels = tasks[task]\n",
    "    train_dataset = MyDataset(f'./processed/train/{task}.tsv')\n",
    "    test_dataset = MyDataset(f'./processed/test/{task}.tsv')\n",
    "    valid_dataset = MyDataset(f'./processed/dev/{task}.tsv')\n",
    "    \n",
    "    \n",
    "    singletaskbert = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=num_labels) \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f'./results/baselines/{task}',          # output directory\n",
    "        num_train_epochs=5,              # total number of training epochs\n",
    "        per_device_train_batch_size=batch_size,  # batch size per device during training\n",
    "        per_device_eval_batch_size=batch_size,   # batch size for evaluation\n",
    "        warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "        weight_decay=0.01,               # strength of weight decay\n",
    "        logging_dir=f'./results/baselines/{task}/logs',            # directory for storing logs\n",
    "        load_best_model_at_end=False,     # load the best model when finished training (default metric is loss)\n",
    "        # but you can specify `metric_for_best_model` argument to change to accuracy or other metric\n",
    "        logging_first_step = True, \n",
    "#         logging_steps=500,               # log & save weights each logging_steps\n",
    "#         save_steps=500,\n",
    "        evaluation_strategy=\"epoch\",     # evaluate each `logging_steps`\n",
    "        save_strategy='epoch'\n",
    "    )\n",
    "    from sklearn.metrics import precision_recall_fscore_support\n",
    "    \n",
    "    if num_labels == 1:\n",
    "        def compute_metrics(pred):\n",
    "            predictions, labels = pred\n",
    "            rmse = mean_squared_error(labels, predictions, squared=False)\n",
    "            return {\"rmse\": rmse}\n",
    "    elif num_labels == 2:\n",
    "        def compute_metrics(pred):\n",
    "            labels = pred.label_ids\n",
    "            preds = pred.predictions.argmax(-1)\n",
    "            precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "            acc = accuracy_score(labels, preds)\n",
    "            return {\n",
    "                'accuracy': acc,\n",
    "                'f1': f1,\n",
    "                'precision': precision,\n",
    "                'recall': recall\n",
    "            }\n",
    "    else:\n",
    "        def compute_metrics(pred):\n",
    "            labels = pred.label_ids\n",
    "            preds = pred.predictions.argmax(-1)\n",
    "            precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='macro')\n",
    "            acc = accuracy_score(labels, preds)\n",
    "            return {\n",
    "                'accuracy': acc,\n",
    "                'f1': f1,\n",
    "                'precision': precision,\n",
    "                'recall': recall\n",
    "            }\n",
    "\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=singletaskbert,   # the instantiated Transformers model to be trained\n",
    "        args=training_args,                  # training arguments, defined above\n",
    "        train_dataset=train_dataset,         # training dataset\n",
    "        eval_dataset=valid_dataset,          # evaluation dataset\n",
    "#         test_dataset=test_dataset,            # test dataset\n",
    "        compute_metrics=compute_metrics,     # the callback that computes metrics of interest\n",
    "    )\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d82cbed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8964a08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a519867d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e55689fb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skip CrowdFlower\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/joey/anaconda3/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 87170\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 13625\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13625' max='13625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13625/13625 1:52:49, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.394200</td>\n",
       "      <td>0.274151</td>\n",
       "      <td>0.897385</td>\n",
       "      <td>0.346008</td>\n",
       "      <td>0.387485</td>\n",
       "      <td>0.334119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.318800</td>\n",
       "      <td>0.267936</td>\n",
       "      <td>0.900483</td>\n",
       "      <td>0.393077</td>\n",
       "      <td>0.477817</td>\n",
       "      <td>0.369968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.247600</td>\n",
       "      <td>0.305294</td>\n",
       "      <td>0.889330</td>\n",
       "      <td>0.419818</td>\n",
       "      <td>0.548605</td>\n",
       "      <td>0.415887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.178000</td>\n",
       "      <td>0.355949</td>\n",
       "      <td>0.888710</td>\n",
       "      <td>0.422777</td>\n",
       "      <td>0.545004</td>\n",
       "      <td>0.415109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.126100</td>\n",
       "      <td>0.417322</td>\n",
       "      <td>0.891436</td>\n",
       "      <td>0.408869</td>\n",
       "      <td>0.473858</td>\n",
       "      <td>0.391538</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 8069\n",
      "  Batch size = 32\n",
      "/home/joey/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./results/baselines/DailyDialog/checkpoint-2725\n",
      "Configuration saved in ./results/baselines/DailyDialog/checkpoint-2725/config.json\n",
      "Model weights saved in ./results/baselines/DailyDialog/checkpoint-2725/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8069\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/baselines/DailyDialog/checkpoint-5450\n",
      "Configuration saved in ./results/baselines/DailyDialog/checkpoint-5450/config.json\n",
      "Model weights saved in ./results/baselines/DailyDialog/checkpoint-5450/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8069\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/baselines/DailyDialog/checkpoint-8175\n",
      "Configuration saved in ./results/baselines/DailyDialog/checkpoint-8175/config.json\n",
      "Model weights saved in ./results/baselines/DailyDialog/checkpoint-8175/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8069\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/baselines/DailyDialog/checkpoint-10900\n",
      "Configuration saved in ./results/baselines/DailyDialog/checkpoint-10900/config.json\n",
      "Model weights saved in ./results/baselines/DailyDialog/checkpoint-10900/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8069\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/baselines/DailyDialog/checkpoint-13625\n",
      "Configuration saved in ./results/baselines/DailyDialog/checkpoint-13625/config.json\n",
      "Model weights saved in ./results/baselines/DailyDialog/checkpoint-13625/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/joey/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/joey/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/home/joey/anaconda3/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 8816\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1380\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1380' max='1380' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1380/1380 11:12, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.537000</td>\n",
       "      <td>0.006395</td>\n",
       "      <td>0.079971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.037500</td>\n",
       "      <td>0.009230</td>\n",
       "      <td>0.096074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.037500</td>\n",
       "      <td>0.005136</td>\n",
       "      <td>0.071664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>0.005653</td>\n",
       "      <td>0.075187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>0.005192</td>\n",
       "      <td>0.072059</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 498\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/baselines/EmoBank_Valence/checkpoint-276\n",
      "Configuration saved in ./results/baselines/EmoBank_Valence/checkpoint-276/config.json\n",
      "Model weights saved in ./results/baselines/EmoBank_Valence/checkpoint-276/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 498\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/baselines/EmoBank_Valence/checkpoint-552\n",
      "Configuration saved in ./results/baselines/EmoBank_Valence/checkpoint-552/config.json\n",
      "Model weights saved in ./results/baselines/EmoBank_Valence/checkpoint-552/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 498\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/baselines/EmoBank_Valence/checkpoint-828\n",
      "Configuration saved in ./results/baselines/EmoBank_Valence/checkpoint-828/config.json\n",
      "Model weights saved in ./results/baselines/EmoBank_Valence/checkpoint-828/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 498\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/baselines/EmoBank_Valence/checkpoint-1104\n",
      "Configuration saved in ./results/baselines/EmoBank_Valence/checkpoint-1104/config.json\n",
      "Model weights saved in ./results/baselines/EmoBank_Valence/checkpoint-1104/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 498\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/baselines/EmoBank_Valence/checkpoint-1380\n",
      "Configuration saved in ./results/baselines/EmoBank_Valence/checkpoint-1380/config.json\n",
      "Model weights saved in ./results/baselines/EmoBank_Valence/checkpoint-1380/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/joey/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/joey/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/home/joey/anaconda3/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 8816\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1380\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1380' max='1380' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1380/1380 11:13, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.496500</td>\n",
       "      <td>0.007932</td>\n",
       "      <td>0.089059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.036700</td>\n",
       "      <td>0.009329</td>\n",
       "      <td>0.096588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.036700</td>\n",
       "      <td>0.008979</td>\n",
       "      <td>0.094760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.006000</td>\n",
       "      <td>0.007370</td>\n",
       "      <td>0.085847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.006000</td>\n",
       "      <td>0.007538</td>\n",
       "      <td>0.086821</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 498\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/baselines/EmoBank_Arousal/checkpoint-276\n",
      "Configuration saved in ./results/baselines/EmoBank_Arousal/checkpoint-276/config.json\n",
      "Model weights saved in ./results/baselines/EmoBank_Arousal/checkpoint-276/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 498\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/baselines/EmoBank_Arousal/checkpoint-552\n",
      "Configuration saved in ./results/baselines/EmoBank_Arousal/checkpoint-552/config.json\n",
      "Model weights saved in ./results/baselines/EmoBank_Arousal/checkpoint-552/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 498\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/baselines/EmoBank_Arousal/checkpoint-828\n",
      "Configuration saved in ./results/baselines/EmoBank_Arousal/checkpoint-828/config.json\n",
      "Model weights saved in ./results/baselines/EmoBank_Arousal/checkpoint-828/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 498\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/baselines/EmoBank_Arousal/checkpoint-1104\n",
      "Configuration saved in ./results/baselines/EmoBank_Arousal/checkpoint-1104/config.json\n",
      "Model weights saved in ./results/baselines/EmoBank_Arousal/checkpoint-1104/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 498\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/baselines/EmoBank_Arousal/checkpoint-1380\n",
      "Configuration saved in ./results/baselines/EmoBank_Arousal/checkpoint-1380/config.json\n",
      "Model weights saved in ./results/baselines/EmoBank_Arousal/checkpoint-1380/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/joey/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/joey/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/home/joey/anaconda3/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 8816\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1380\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1380' max='1380' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1380/1380 11:13, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.538500</td>\n",
       "      <td>0.007563</td>\n",
       "      <td>0.086963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.040700</td>\n",
       "      <td>0.007075</td>\n",
       "      <td>0.084112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.040700</td>\n",
       "      <td>0.006574</td>\n",
       "      <td>0.081078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.006100</td>\n",
       "      <td>0.006338</td>\n",
       "      <td>0.079614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.006100</td>\n",
       "      <td>0.006790</td>\n",
       "      <td>0.082399</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 498\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/baselines/EmoBank_Dominance/checkpoint-276\n",
      "Configuration saved in ./results/baselines/EmoBank_Dominance/checkpoint-276/config.json\n",
      "Model weights saved in ./results/baselines/EmoBank_Dominance/checkpoint-276/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 498\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/baselines/EmoBank_Dominance/checkpoint-552\n",
      "Configuration saved in ./results/baselines/EmoBank_Dominance/checkpoint-552/config.json\n",
      "Model weights saved in ./results/baselines/EmoBank_Dominance/checkpoint-552/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 498\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/baselines/EmoBank_Dominance/checkpoint-828\n",
      "Configuration saved in ./results/baselines/EmoBank_Dominance/checkpoint-828/config.json\n",
      "Model weights saved in ./results/baselines/EmoBank_Dominance/checkpoint-828/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 498\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/baselines/EmoBank_Dominance/checkpoint-1104\n",
      "Configuration saved in ./results/baselines/EmoBank_Dominance/checkpoint-1104/config.json\n",
      "Model weights saved in ./results/baselines/EmoBank_Dominance/checkpoint-1104/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 498\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/baselines/EmoBank_Dominance/checkpoint-1380\n",
      "Configuration saved in ./results/baselines/EmoBank_Dominance/checkpoint-1380/config.json\n",
      "Model weights saved in ./results/baselines/EmoBank_Dominance/checkpoint-1380/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/joey/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/joey/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/home/joey/anaconda3/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 22191\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3470\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3470' max='3470' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3470/3470 28:28, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.398700</td>\n",
       "      <td>0.210191</td>\n",
       "      <td>0.929656</td>\n",
       "      <td>0.839019</td>\n",
       "      <td>0.827381</td>\n",
       "      <td>0.853288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.210400</td>\n",
       "      <td>0.134268</td>\n",
       "      <td>0.963229</td>\n",
       "      <td>0.904348</td>\n",
       "      <td>0.931965</td>\n",
       "      <td>0.883646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.066100</td>\n",
       "      <td>0.137341</td>\n",
       "      <td>0.973621</td>\n",
       "      <td>0.936561</td>\n",
       "      <td>0.958109</td>\n",
       "      <td>0.918334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.027400</td>\n",
       "      <td>0.127036</td>\n",
       "      <td>0.976819</td>\n",
       "      <td>0.943459</td>\n",
       "      <td>0.960929</td>\n",
       "      <td>0.928807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.017000</td>\n",
       "      <td>0.140868</td>\n",
       "      <td>0.975220</td>\n",
       "      <td>0.941495</td>\n",
       "      <td>0.960122</td>\n",
       "      <td>0.925647</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1251\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/baselines/HateOffensive/checkpoint-694\n",
      "Configuration saved in ./results/baselines/HateOffensive/checkpoint-694/config.json\n",
      "Model weights saved in ./results/baselines/HateOffensive/checkpoint-694/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1251\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/baselines/HateOffensive/checkpoint-1388\n",
      "Configuration saved in ./results/baselines/HateOffensive/checkpoint-1388/config.json\n",
      "Model weights saved in ./results/baselines/HateOffensive/checkpoint-1388/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1251\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/baselines/HateOffensive/checkpoint-2082\n",
      "Configuration saved in ./results/baselines/HateOffensive/checkpoint-2082/config.json\n",
      "Model weights saved in ./results/baselines/HateOffensive/checkpoint-2082/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1251\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/baselines/HateOffensive/checkpoint-2776\n",
      "Configuration saved in ./results/baselines/HateOffensive/checkpoint-2776/config.json\n",
      "Model weights saved in ./results/baselines/HateOffensive/checkpoint-2776/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1251\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/baselines/HateOffensive/checkpoint-3470\n",
      "Configuration saved in ./results/baselines/HateOffensive/checkpoint-3470/config.json\n",
      "Model weights saved in ./results/baselines/HateOffensive/checkpoint-3470/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/joey/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/joey/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/home/joey/anaconda3/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 33240\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 5195\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5195' max='5195' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5195/5195 29:29, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.342900</td>\n",
       "      <td>1.317760</td>\n",
       "      <td>0.453430</td>\n",
       "      <td>0.174347</td>\n",
       "      <td>0.265279</td>\n",
       "      <td>0.179136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.226600</td>\n",
       "      <td>1.298108</td>\n",
       "      <td>0.476534</td>\n",
       "      <td>0.204737</td>\n",
       "      <td>0.275368</td>\n",
       "      <td>0.200825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.914800</td>\n",
       "      <td>1.441295</td>\n",
       "      <td>0.478219</td>\n",
       "      <td>0.220259</td>\n",
       "      <td>0.254030</td>\n",
       "      <td>0.215277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.529000</td>\n",
       "      <td>1.768355</td>\n",
       "      <td>0.467148</td>\n",
       "      <td>0.245341</td>\n",
       "      <td>0.363404</td>\n",
       "      <td>0.235721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.309500</td>\n",
       "      <td>2.183343</td>\n",
       "      <td>0.459928</td>\n",
       "      <td>0.245970</td>\n",
       "      <td>0.298616</td>\n",
       "      <td>0.239049</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 4155\n",
      "  Batch size = 32\n",
      "/home/joey/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./results/baselines/PASTEL_age/checkpoint-1039\n",
      "Configuration saved in ./results/baselines/PASTEL_age/checkpoint-1039/config.json\n",
      "Model weights saved in ./results/baselines/PASTEL_age/checkpoint-1039/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4155\n",
      "  Batch size = 32\n",
      "/home/joey/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./results/baselines/PASTEL_age/checkpoint-2078\n",
      "Configuration saved in ./results/baselines/PASTEL_age/checkpoint-2078/config.json\n",
      "Model weights saved in ./results/baselines/PASTEL_age/checkpoint-2078/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4155\n",
      "  Batch size = 32\n",
      "/home/joey/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./results/baselines/PASTEL_age/checkpoint-3117\n",
      "Configuration saved in ./results/baselines/PASTEL_age/checkpoint-3117/config.json\n",
      "Model weights saved in ./results/baselines/PASTEL_age/checkpoint-3117/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4155\n",
      "  Batch size = 32\n",
      "/home/joey/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./results/baselines/PASTEL_age/checkpoint-4156\n",
      "Configuration saved in ./results/baselines/PASTEL_age/checkpoint-4156/config.json\n",
      "Model weights saved in ./results/baselines/PASTEL_age/checkpoint-4156/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4155\n",
      "  Batch size = 32\n",
      "/home/joey/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./results/baselines/PASTEL_age/checkpoint-5195\n",
      "Configuration saved in ./results/baselines/PASTEL_age/checkpoint-5195/config.json\n",
      "Model weights saved in ./results/baselines/PASTEL_age/checkpoint-5195/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/joey/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/joey/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/home/joey/anaconda3/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 33224\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 5195\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5195' max='5195' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5195/5195 29:24, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.099400</td>\n",
       "      <td>0.100069</td>\n",
       "      <td>0.976403</td>\n",
       "      <td>0.988060</td>\n",
       "      <td>0.976403</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.078700</td>\n",
       "      <td>0.108778</td>\n",
       "      <td>0.976643</td>\n",
       "      <td>0.988181</td>\n",
       "      <td>0.976638</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.113277</td>\n",
       "      <td>0.974717</td>\n",
       "      <td>0.987184</td>\n",
       "      <td>0.977284</td>\n",
       "      <td>0.997287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.020600</td>\n",
       "      <td>0.164836</td>\n",
       "      <td>0.974476</td>\n",
       "      <td>0.987051</td>\n",
       "      <td>0.977971</td>\n",
       "      <td>0.996301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.010400</td>\n",
       "      <td>0.182554</td>\n",
       "      <td>0.974476</td>\n",
       "      <td>0.987048</td>\n",
       "      <td>0.978203</td>\n",
       "      <td>0.996054</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 4153\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/baselines/PASTEL_country/checkpoint-1039\n",
      "Configuration saved in ./results/baselines/PASTEL_country/checkpoint-1039/config.json\n",
      "Model weights saved in ./results/baselines/PASTEL_country/checkpoint-1039/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4153\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/baselines/PASTEL_country/checkpoint-2078\n",
      "Configuration saved in ./results/baselines/PASTEL_country/checkpoint-2078/config.json\n",
      "Model weights saved in ./results/baselines/PASTEL_country/checkpoint-2078/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4153\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/baselines/PASTEL_country/checkpoint-3117\n",
      "Configuration saved in ./results/baselines/PASTEL_country/checkpoint-3117/config.json\n",
      "Model weights saved in ./results/baselines/PASTEL_country/checkpoint-3117/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4153\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/baselines/PASTEL_country/checkpoint-4156\n",
      "Configuration saved in ./results/baselines/PASTEL_country/checkpoint-4156/config.json\n",
      "Model weights saved in ./results/baselines/PASTEL_country/checkpoint-4156/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4153\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/baselines/PASTEL_country/checkpoint-5195\n",
      "Configuration saved in ./results/baselines/PASTEL_country/checkpoint-5195/config.json\n",
      "Model weights saved in ./results/baselines/PASTEL_country/checkpoint-5195/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/joey/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/joey/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/home/joey/anaconda3/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 33240\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 5195\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5195' max='5195' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5195/5195 30:24, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.524100</td>\n",
       "      <td>1.481330</td>\n",
       "      <td>0.436101</td>\n",
       "      <td>0.156322</td>\n",
       "      <td>0.286494</td>\n",
       "      <td>0.178683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.406500</td>\n",
       "      <td>1.454466</td>\n",
       "      <td>0.454392</td>\n",
       "      <td>0.242987</td>\n",
       "      <td>0.340456</td>\n",
       "      <td>0.231095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.123800</td>\n",
       "      <td>1.546608</td>\n",
       "      <td>0.440193</td>\n",
       "      <td>0.257852</td>\n",
       "      <td>0.292032</td>\n",
       "      <td>0.250045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.716900</td>\n",
       "      <td>1.866608</td>\n",
       "      <td>0.428881</td>\n",
       "      <td>0.252176</td>\n",
       "      <td>0.285587</td>\n",
       "      <td>0.245995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.433700</td>\n",
       "      <td>2.177496</td>\n",
       "      <td>0.422383</td>\n",
       "      <td>0.263090</td>\n",
       "      <td>0.287689</td>\n",
       "      <td>0.255597</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 4155\n",
      "  Batch size = 32\n",
      "/home/joey/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./results/baselines/PASTEL_education/checkpoint-1039\n",
      "Configuration saved in ./results/baselines/PASTEL_education/checkpoint-1039/config.json\n",
      "Model weights saved in ./results/baselines/PASTEL_education/checkpoint-1039/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4155\n",
      "  Batch size = 32\n",
      "/home/joey/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./results/baselines/PASTEL_education/checkpoint-2078\n",
      "Configuration saved in ./results/baselines/PASTEL_education/checkpoint-2078/config.json\n",
      "Model weights saved in ./results/baselines/PASTEL_education/checkpoint-2078/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4155\n",
      "  Batch size = 32\n",
      "/home/joey/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./results/baselines/PASTEL_education/checkpoint-3117\n",
      "Configuration saved in ./results/baselines/PASTEL_education/checkpoint-3117/config.json\n",
      "Model weights saved in ./results/baselines/PASTEL_education/checkpoint-3117/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4155\n",
      "  Batch size = 32\n",
      "/home/joey/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./results/baselines/PASTEL_education/checkpoint-4156\n",
      "Configuration saved in ./results/baselines/PASTEL_education/checkpoint-4156/config.json\n",
      "Model weights saved in ./results/baselines/PASTEL_education/checkpoint-4156/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4155\n",
      "  Batch size = 32\n",
      "/home/joey/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./results/baselines/PASTEL_education/checkpoint-5195\n",
      "Configuration saved in ./results/baselines/PASTEL_education/checkpoint-5195/config.json\n",
      "Model weights saved in ./results/baselines/PASTEL_education/checkpoint-5195/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/joey/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/joey/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/home/joey/anaconda3/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 33240\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 5195\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5195' max='5195' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5195/5195 30:35, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.683300</td>\n",
       "      <td>0.687885</td>\n",
       "      <td>0.834416</td>\n",
       "      <td>0.205460</td>\n",
       "      <td>0.253930</td>\n",
       "      <td>0.205142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.607000</td>\n",
       "      <td>0.676828</td>\n",
       "      <td>0.838026</td>\n",
       "      <td>0.244631</td>\n",
       "      <td>0.356683</td>\n",
       "      <td>0.227845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.432800</td>\n",
       "      <td>0.776087</td>\n",
       "      <td>0.833694</td>\n",
       "      <td>0.251103</td>\n",
       "      <td>0.347318</td>\n",
       "      <td>0.234171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.238900</td>\n",
       "      <td>0.952446</td>\n",
       "      <td>0.804091</td>\n",
       "      <td>0.258753</td>\n",
       "      <td>0.290963</td>\n",
       "      <td>0.245538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.127200</td>\n",
       "      <td>1.155590</td>\n",
       "      <td>0.803851</td>\n",
       "      <td>0.259601</td>\n",
       "      <td>0.289519</td>\n",
       "      <td>0.245106</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 4155\n",
      "  Batch size = 32\n",
      "/home/joey/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./results/baselines/PASTEL_ethnic/checkpoint-1039\n",
      "Configuration saved in ./results/baselines/PASTEL_ethnic/checkpoint-1039/config.json\n",
      "Model weights saved in ./results/baselines/PASTEL_ethnic/checkpoint-1039/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4155\n",
      "  Batch size = 32\n",
      "/home/joey/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./results/baselines/PASTEL_ethnic/checkpoint-2078\n",
      "Configuration saved in ./results/baselines/PASTEL_ethnic/checkpoint-2078/config.json\n",
      "Model weights saved in ./results/baselines/PASTEL_ethnic/checkpoint-2078/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4155\n",
      "  Batch size = 32\n",
      "/home/joey/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./results/baselines/PASTEL_ethnic/checkpoint-3117\n",
      "Configuration saved in ./results/baselines/PASTEL_ethnic/checkpoint-3117/config.json\n",
      "Model weights saved in ./results/baselines/PASTEL_ethnic/checkpoint-3117/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4155\n",
      "  Batch size = 32\n",
      "/home/joey/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./results/baselines/PASTEL_ethnic/checkpoint-4156\n",
      "Configuration saved in ./results/baselines/PASTEL_ethnic/checkpoint-4156/config.json\n",
      "Model weights saved in ./results/baselines/PASTEL_ethnic/checkpoint-4156/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4155\n",
      "  Batch size = 32\n",
      "/home/joey/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./results/baselines/PASTEL_ethnic/checkpoint-5195\n",
      "Configuration saved in ./results/baselines/PASTEL_ethnic/checkpoint-5195/config.json\n",
      "Model weights saved in ./results/baselines/PASTEL_ethnic/checkpoint-5195/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/joey/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/joey/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/home/joey/anaconda3/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 33240\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 5195\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5195' max='5195' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5195/5195 29:59, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.575600</td>\n",
       "      <td>0.559227</td>\n",
       "      <td>0.748496</td>\n",
       "      <td>0.465729</td>\n",
       "      <td>0.526825</td>\n",
       "      <td>0.462577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.523800</td>\n",
       "      <td>0.583513</td>\n",
       "      <td>0.748977</td>\n",
       "      <td>0.469230</td>\n",
       "      <td>0.519745</td>\n",
       "      <td>0.465440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.372200</td>\n",
       "      <td>0.672592</td>\n",
       "      <td>0.737906</td>\n",
       "      <td>0.473761</td>\n",
       "      <td>0.488460</td>\n",
       "      <td>0.470388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.212400</td>\n",
       "      <td>0.927811</td>\n",
       "      <td>0.735259</td>\n",
       "      <td>0.495636</td>\n",
       "      <td>0.540490</td>\n",
       "      <td>0.484149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.128100</td>\n",
       "      <td>1.232693</td>\n",
       "      <td>0.732611</td>\n",
       "      <td>0.490561</td>\n",
       "      <td>0.505484</td>\n",
       "      <td>0.483999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 4155\n",
      "  Batch size = 32\n",
      "/home/joey/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./results/baselines/PASTEL_gender/checkpoint-1039\n",
      "Configuration saved in ./results/baselines/PASTEL_gender/checkpoint-1039/config.json\n",
      "Model weights saved in ./results/baselines/PASTEL_gender/checkpoint-1039/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4155\n",
      "  Batch size = 32\n",
      "/home/joey/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./results/baselines/PASTEL_gender/checkpoint-2078\n",
      "Configuration saved in ./results/baselines/PASTEL_gender/checkpoint-2078/config.json\n",
      "Model weights saved in ./results/baselines/PASTEL_gender/checkpoint-2078/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4155\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/baselines/PASTEL_gender/checkpoint-3117\n",
      "Configuration saved in ./results/baselines/PASTEL_gender/checkpoint-3117/config.json\n",
      "Model weights saved in ./results/baselines/PASTEL_gender/checkpoint-3117/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4155\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/baselines/PASTEL_gender/checkpoint-4156\n",
      "Configuration saved in ./results/baselines/PASTEL_gender/checkpoint-4156/config.json\n",
      "Model weights saved in ./results/baselines/PASTEL_gender/checkpoint-4156/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4155\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/baselines/PASTEL_gender/checkpoint-5195\n",
      "Configuration saved in ./results/baselines/PASTEL_gender/checkpoint-5195/config.json\n",
      "Model weights saved in ./results/baselines/PASTEL_gender/checkpoint-5195/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/joey/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/joey/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/home/joey/anaconda3/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 33240\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 5195\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5195' max='5195' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5195/5195 30:12, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.953200</td>\n",
       "      <td>0.937625</td>\n",
       "      <td>0.503730</td>\n",
       "      <td>0.362738</td>\n",
       "      <td>0.336440</td>\n",
       "      <td>0.398192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.887500</td>\n",
       "      <td>0.949823</td>\n",
       "      <td>0.506619</td>\n",
       "      <td>0.428365</td>\n",
       "      <td>0.493617</td>\n",
       "      <td>0.434389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.646600</td>\n",
       "      <td>1.080967</td>\n",
       "      <td>0.523706</td>\n",
       "      <td>0.470997</td>\n",
       "      <td>0.484400</td>\n",
       "      <td>0.466896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.360900</td>\n",
       "      <td>1.445178</td>\n",
       "      <td>0.511913</td>\n",
       "      <td>0.463791</td>\n",
       "      <td>0.465752</td>\n",
       "      <td>0.462475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.213500</td>\n",
       "      <td>1.952750</td>\n",
       "      <td>0.516245</td>\n",
       "      <td>0.469561</td>\n",
       "      <td>0.476377</td>\n",
       "      <td>0.465928</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 4155\n",
      "  Batch size = 32\n",
      "/home/joey/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./results/baselines/PASTEL_politics/checkpoint-1039\n",
      "Configuration saved in ./results/baselines/PASTEL_politics/checkpoint-1039/config.json\n",
      "Model weights saved in ./results/baselines/PASTEL_politics/checkpoint-1039/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4155\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/baselines/PASTEL_politics/checkpoint-2078\n",
      "Configuration saved in ./results/baselines/PASTEL_politics/checkpoint-2078/config.json\n",
      "Model weights saved in ./results/baselines/PASTEL_politics/checkpoint-2078/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4155\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/baselines/PASTEL_politics/checkpoint-3117\n",
      "Configuration saved in ./results/baselines/PASTEL_politics/checkpoint-3117/config.json\n",
      "Model weights saved in ./results/baselines/PASTEL_politics/checkpoint-3117/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4155\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/baselines/PASTEL_politics/checkpoint-4156\n",
      "Configuration saved in ./results/baselines/PASTEL_politics/checkpoint-4156/config.json\n",
      "Model weights saved in ./results/baselines/PASTEL_politics/checkpoint-4156/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4155\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/baselines/PASTEL_politics/checkpoint-5195\n",
      "Configuration saved in ./results/baselines/PASTEL_politics/checkpoint-5195/config.json\n",
      "Model weights saved in ./results/baselines/PASTEL_politics/checkpoint-5195/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/joey/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/joey/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/home/joey/anaconda3/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 33240\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 5195\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5195' max='5195' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5195/5195 30:05, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.289600</td>\n",
       "      <td>1.299677</td>\n",
       "      <td>0.468111</td>\n",
       "      <td>0.266570</td>\n",
       "      <td>0.400161</td>\n",
       "      <td>0.292322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.246100</td>\n",
       "      <td>1.280031</td>\n",
       "      <td>0.472202</td>\n",
       "      <td>0.272989</td>\n",
       "      <td>0.416880</td>\n",
       "      <td>0.296377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.051400</td>\n",
       "      <td>1.370571</td>\n",
       "      <td>0.453670</td>\n",
       "      <td>0.293912</td>\n",
       "      <td>0.355820</td>\n",
       "      <td>0.304157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.700700</td>\n",
       "      <td>1.667322</td>\n",
       "      <td>0.422383</td>\n",
       "      <td>0.315304</td>\n",
       "      <td>0.330849</td>\n",
       "      <td>0.315826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.442100</td>\n",
       "      <td>2.045131</td>\n",
       "      <td>0.435138</td>\n",
       "      <td>0.320340</td>\n",
       "      <td>0.335627</td>\n",
       "      <td>0.319113</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 4155\n",
      "  Batch size = 32\n",
      "/home/joey/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./results/baselines/PASTEL_tod/checkpoint-1039\n",
      "Configuration saved in ./results/baselines/PASTEL_tod/checkpoint-1039/config.json\n",
      "Model weights saved in ./results/baselines/PASTEL_tod/checkpoint-1039/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4155\n",
      "  Batch size = 32\n",
      "/home/joey/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./results/baselines/PASTEL_tod/checkpoint-2078\n",
      "Configuration saved in ./results/baselines/PASTEL_tod/checkpoint-2078/config.json\n",
      "Model weights saved in ./results/baselines/PASTEL_tod/checkpoint-2078/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4155\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/baselines/PASTEL_tod/checkpoint-3117\n",
      "Configuration saved in ./results/baselines/PASTEL_tod/checkpoint-3117/config.json\n",
      "Model weights saved in ./results/baselines/PASTEL_tod/checkpoint-3117/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4155\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/baselines/PASTEL_tod/checkpoint-4156\n",
      "Configuration saved in ./results/baselines/PASTEL_tod/checkpoint-4156/config.json\n",
      "Model weights saved in ./results/baselines/PASTEL_tod/checkpoint-4156/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4155\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/baselines/PASTEL_tod/checkpoint-5195\n",
      "Configuration saved in ./results/baselines/PASTEL_tod/checkpoint-5195/config.json\n",
      "Model weights saved in ./results/baselines/PASTEL_tod/checkpoint-5195/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/joey/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/joey/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/home/joey/anaconda3/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 205644\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 32135\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='32135' max='32135' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [32135/32135 4:40:11, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.558500</td>\n",
       "      <td>0.556077</td>\n",
       "      <td>0.710873</td>\n",
       "      <td>0.723430</td>\n",
       "      <td>0.692357</td>\n",
       "      <td>0.757422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.497800</td>\n",
       "      <td>0.570840</td>\n",
       "      <td>0.719296</td>\n",
       "      <td>0.723252</td>\n",
       "      <td>0.712149</td>\n",
       "      <td>0.734707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.343800</td>\n",
       "      <td>0.683438</td>\n",
       "      <td>0.708286</td>\n",
       "      <td>0.719824</td>\n",
       "      <td>0.691468</td>\n",
       "      <td>0.750604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.208800</td>\n",
       "      <td>0.900290</td>\n",
       "      <td>0.702354</td>\n",
       "      <td>0.713553</td>\n",
       "      <td>0.686712</td>\n",
       "      <td>0.742578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.128000</td>\n",
       "      <td>1.238347</td>\n",
       "      <td>0.703112</td>\n",
       "      <td>0.711916</td>\n",
       "      <td>0.690427</td>\n",
       "      <td>0.734785</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 51410\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/baselines/SARC/checkpoint-6427\n",
      "Configuration saved in ./results/baselines/SARC/checkpoint-6427/config.json\n",
      "Model weights saved in ./results/baselines/SARC/checkpoint-6427/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 51410\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/baselines/SARC/checkpoint-12854\n",
      "Configuration saved in ./results/baselines/SARC/checkpoint-12854/config.json\n",
      "Model weights saved in ./results/baselines/SARC/checkpoint-12854/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 51410\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/baselines/SARC/checkpoint-19281\n",
      "Configuration saved in ./results/baselines/SARC/checkpoint-19281/config.json\n",
      "Model weights saved in ./results/baselines/SARC/checkpoint-19281/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 51410\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/baselines/SARC/checkpoint-25708\n",
      "Configuration saved in ./results/baselines/SARC/checkpoint-25708/config.json\n",
      "Model weights saved in ./results/baselines/SARC/checkpoint-25708/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 51410\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/baselines/SARC/checkpoint-32135\n",
      "Configuration saved in ./results/baselines/SARC/checkpoint-32135/config.json\n",
      "Model weights saved in ./results/baselines/SARC/checkpoint-32135/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/joey/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/joey/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/home/joey/anaconda3/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 39780\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 6220\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6220' max='6220' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6220/6220 36:10, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.015800</td>\n",
       "      <td>0.184673</td>\n",
       "      <td>0.977085</td>\n",
       "      <td>0.640000</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>0.551724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.011100</td>\n",
       "      <td>0.188840</td>\n",
       "      <td>0.976448</td>\n",
       "      <td>0.633663</td>\n",
       "      <td>0.744186</td>\n",
       "      <td>0.551724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.005700</td>\n",
       "      <td>0.210745</td>\n",
       "      <td>0.977085</td>\n",
       "      <td>0.640000</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>0.551724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.003600</td>\n",
       "      <td>0.194174</td>\n",
       "      <td>0.977721</td>\n",
       "      <td>0.646465</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.551724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>0.207722</td>\n",
       "      <td>0.977721</td>\n",
       "      <td>0.646465</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.551724</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1571\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/baselines/SarcasmGhosh/checkpoint-1244\n",
      "Configuration saved in ./results/baselines/SarcasmGhosh/checkpoint-1244/config.json\n",
      "Model weights saved in ./results/baselines/SarcasmGhosh/checkpoint-1244/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1571\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/baselines/SarcasmGhosh/checkpoint-2488\n",
      "Configuration saved in ./results/baselines/SarcasmGhosh/checkpoint-2488/config.json\n",
      "Model weights saved in ./results/baselines/SarcasmGhosh/checkpoint-2488/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1571\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/baselines/SarcasmGhosh/checkpoint-3732\n",
      "Configuration saved in ./results/baselines/SarcasmGhosh/checkpoint-3732/config.json\n",
      "Model weights saved in ./results/baselines/SarcasmGhosh/checkpoint-3732/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1571\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/baselines/SarcasmGhosh/checkpoint-4976\n",
      "Configuration saved in ./results/baselines/SarcasmGhosh/checkpoint-4976/config.json\n",
      "Model weights saved in ./results/baselines/SarcasmGhosh/checkpoint-4976/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1571\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/baselines/SarcasmGhosh/checkpoint-6220\n",
      "Configuration saved in ./results/baselines/SarcasmGhosh/checkpoint-6220/config.json\n",
      "Model weights saved in ./results/baselines/SarcasmGhosh/checkpoint-6220/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/joey/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/joey/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/home/joey/anaconda3/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 236076\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 36890\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='36890' max='36890' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [36890/36890 2:41:43, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.009400</td>\n",
       "      <td>0.019820</td>\n",
       "      <td>0.140785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.007300</td>\n",
       "      <td>0.015958</td>\n",
       "      <td>0.126324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.006200</td>\n",
       "      <td>0.014956</td>\n",
       "      <td>0.122293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.005300</td>\n",
       "      <td>0.013766</td>\n",
       "      <td>0.117327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.004200</td>\n",
       "      <td>0.014728</td>\n",
       "      <td>0.121360</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1044\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/baselines/SentiTreeBank/checkpoint-7378\n",
      "Configuration saved in ./results/baselines/SentiTreeBank/checkpoint-7378/config.json\n",
      "Model weights saved in ./results/baselines/SentiTreeBank/checkpoint-7378/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1044\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/baselines/SentiTreeBank/checkpoint-14756\n",
      "Configuration saved in ./results/baselines/SentiTreeBank/checkpoint-14756/config.json\n",
      "Model weights saved in ./results/baselines/SentiTreeBank/checkpoint-14756/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1044\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/baselines/SentiTreeBank/checkpoint-22134\n",
      "Configuration saved in ./results/baselines/SentiTreeBank/checkpoint-22134/config.json\n",
      "Model weights saved in ./results/baselines/SentiTreeBank/checkpoint-22134/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1044\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/baselines/SentiTreeBank/checkpoint-29512\n",
      "Configuration saved in ./results/baselines/SentiTreeBank/checkpoint-29512/config.json\n",
      "Model weights saved in ./results/baselines/SentiTreeBank/checkpoint-29512/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1044\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/baselines/SentiTreeBank/checkpoint-36890\n",
      "Configuration saved in ./results/baselines/SentiTreeBank/checkpoint-36890/config.json\n",
      "Model weights saved in ./results/baselines/SentiTreeBank/checkpoint-36890/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/joey/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/joey/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/home/joey/anaconda3/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 37801\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 5910\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5910' max='5910' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5910/5910 47:55, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.165600</td>\n",
       "      <td>0.143976</td>\n",
       "      <td>0.948563</td>\n",
       "      <td>0.948847</td>\n",
       "      <td>0.940358</td>\n",
       "      <td>0.957490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>0.170564</td>\n",
       "      <td>0.958144</td>\n",
       "      <td>0.958562</td>\n",
       "      <td>0.945813</td>\n",
       "      <td>0.971660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.022900</td>\n",
       "      <td>0.201493</td>\n",
       "      <td>0.963691</td>\n",
       "      <td>0.964000</td>\n",
       "      <td>0.952569</td>\n",
       "      <td>0.975709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.007100</td>\n",
       "      <td>0.254436</td>\n",
       "      <td>0.965204</td>\n",
       "      <td>0.965483</td>\n",
       "      <td>0.954500</td>\n",
       "      <td>0.976721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.241933</td>\n",
       "      <td>0.968734</td>\n",
       "      <td>0.969000</td>\n",
       "      <td>0.957510</td>\n",
       "      <td>0.980769</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1983\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/baselines/ShortHumor/checkpoint-1182\n",
      "Configuration saved in ./results/baselines/ShortHumor/checkpoint-1182/config.json\n",
      "Model weights saved in ./results/baselines/ShortHumor/checkpoint-1182/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1983\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/baselines/ShortHumor/checkpoint-2364\n",
      "Configuration saved in ./results/baselines/ShortHumor/checkpoint-2364/config.json\n",
      "Model weights saved in ./results/baselines/ShortHumor/checkpoint-2364/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1983\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/baselines/ShortHumor/checkpoint-3546\n",
      "Configuration saved in ./results/baselines/ShortHumor/checkpoint-3546/config.json\n",
      "Model weights saved in ./results/baselines/ShortHumor/checkpoint-3546/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1983\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/baselines/ShortHumor/checkpoint-4728\n",
      "Configuration saved in ./results/baselines/ShortHumor/checkpoint-4728/config.json\n",
      "Model weights saved in ./results/baselines/ShortHumor/checkpoint-4728/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1983\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/baselines/ShortHumor/checkpoint-5910\n",
      "Configuration saved in ./results/baselines/ShortHumor/checkpoint-5910/config.json\n",
      "Model weights saved in ./results/baselines/ShortHumor/checkpoint-5910/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/joey/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/joey/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/home/joey/anaconda3/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 406682\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 63545\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='63545' max='63545' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [63545/63545 8:39:21, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.095300</td>\n",
       "      <td>0.083775</td>\n",
       "      <td>0.971260</td>\n",
       "      <td>0.971138</td>\n",
       "      <td>0.973701</td>\n",
       "      <td>0.968589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.059000</td>\n",
       "      <td>0.090684</td>\n",
       "      <td>0.976590</td>\n",
       "      <td>0.976504</td>\n",
       "      <td>0.978554</td>\n",
       "      <td>0.974462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.026900</td>\n",
       "      <td>0.099773</td>\n",
       "      <td>0.980544</td>\n",
       "      <td>0.980446</td>\n",
       "      <td>0.983784</td>\n",
       "      <td>0.977131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.013600</td>\n",
       "      <td>0.096889</td>\n",
       "      <td>0.981166</td>\n",
       "      <td>0.980976</td>\n",
       "      <td>0.989321</td>\n",
       "      <td>0.972771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.006700</td>\n",
       "      <td>0.110260</td>\n",
       "      <td>0.982010</td>\n",
       "      <td>0.981812</td>\n",
       "      <td>0.991114</td>\n",
       "      <td>0.972682</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 22512\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/baselines/ShortJokeKaggle/checkpoint-12709\n",
      "Configuration saved in ./results/baselines/ShortJokeKaggle/checkpoint-12709/config.json\n",
      "Model weights saved in ./results/baselines/ShortJokeKaggle/checkpoint-12709/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 22512\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/baselines/ShortJokeKaggle/checkpoint-25418\n",
      "Configuration saved in ./results/baselines/ShortJokeKaggle/checkpoint-25418/config.json\n",
      "Model weights saved in ./results/baselines/ShortJokeKaggle/checkpoint-25418/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 22512\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/baselines/ShortJokeKaggle/checkpoint-38127\n",
      "Configuration saved in ./results/baselines/ShortJokeKaggle/checkpoint-38127/config.json\n",
      "Model weights saved in ./results/baselines/ShortJokeKaggle/checkpoint-38127/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 22512\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/baselines/ShortJokeKaggle/checkpoint-50836\n",
      "Configuration saved in ./results/baselines/ShortJokeKaggle/checkpoint-50836/config.json\n",
      "Model weights saved in ./results/baselines/ShortJokeKaggle/checkpoint-50836/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 22512\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/baselines/ShortJokeKaggle/checkpoint-63545\n",
      "Configuration saved in ./results/baselines/ShortJokeKaggle/checkpoint-63545/config.json\n",
      "Model weights saved in ./results/baselines/ShortJokeKaggle/checkpoint-63545/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/joey/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/joey/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/home/joey/anaconda3/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1902\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 300\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='300' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [300/300 01:34, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.686800</td>\n",
       "      <td>0.426754</td>\n",
       "      <td>0.924528</td>\n",
       "      <td>0.918367</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.849057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.686800</td>\n",
       "      <td>0.041883</td>\n",
       "      <td>0.990566</td>\n",
       "      <td>0.990476</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.981132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.686800</td>\n",
       "      <td>0.001855</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.686800</td>\n",
       "      <td>0.006017</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.686800</td>\n",
       "      <td>0.005312</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 106\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/baselines/ShortRomance/checkpoint-60\n",
      "Configuration saved in ./results/baselines/ShortRomance/checkpoint-60/config.json\n",
      "Model weights saved in ./results/baselines/ShortRomance/checkpoint-60/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 106\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/baselines/ShortRomance/checkpoint-120\n",
      "Configuration saved in ./results/baselines/ShortRomance/checkpoint-120/config.json\n",
      "Model weights saved in ./results/baselines/ShortRomance/checkpoint-120/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 106\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/baselines/ShortRomance/checkpoint-180\n",
      "Configuration saved in ./results/baselines/ShortRomance/checkpoint-180/config.json\n",
      "Model weights saved in ./results/baselines/ShortRomance/checkpoint-180/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 106\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/baselines/ShortRomance/checkpoint-240\n",
      "Configuration saved in ./results/baselines/ShortRomance/checkpoint-240/config.json\n",
      "Model weights saved in ./results/baselines/ShortRomance/checkpoint-240/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 106\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/baselines/ShortRomance/checkpoint-300\n",
      "Configuration saved in ./results/baselines/ShortRomance/checkpoint-300/config.json\n",
      "Model weights saved in ./results/baselines/ShortRomance/checkpoint-300/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/joey/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/joey/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/home/joey/anaconda3/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 9859\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1545\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1545' max='1545' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1545/1545 12:51, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.687100</td>\n",
       "      <td>0.008698</td>\n",
       "      <td>0.093265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.045400</td>\n",
       "      <td>0.010219</td>\n",
       "      <td>0.101089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.045400</td>\n",
       "      <td>0.008931</td>\n",
       "      <td>0.094507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.007400</td>\n",
       "      <td>0.008753</td>\n",
       "      <td>0.093558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.004500</td>\n",
       "      <td>0.008664</td>\n",
       "      <td>0.093080</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 530\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/baselines/StanfordPoliteness/checkpoint-309\n",
      "Configuration saved in ./results/baselines/StanfordPoliteness/checkpoint-309/config.json\n",
      "Model weights saved in ./results/baselines/StanfordPoliteness/checkpoint-309/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 530\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/baselines/StanfordPoliteness/checkpoint-618\n",
      "Configuration saved in ./results/baselines/StanfordPoliteness/checkpoint-618/config.json\n",
      "Model weights saved in ./results/baselines/StanfordPoliteness/checkpoint-618/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 530\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/baselines/StanfordPoliteness/checkpoint-927\n",
      "Configuration saved in ./results/baselines/StanfordPoliteness/checkpoint-927/config.json\n",
      "Model weights saved in ./results/baselines/StanfordPoliteness/checkpoint-927/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 530\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/baselines/StanfordPoliteness/checkpoint-1236\n",
      "Configuration saved in ./results/baselines/StanfordPoliteness/checkpoint-1236/config.json\n",
      "Model weights saved in ./results/baselines/StanfordPoliteness/checkpoint-1236/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 530\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/baselines/StanfordPoliteness/checkpoint-1545\n",
      "Configuration saved in ./results/baselines/StanfordPoliteness/checkpoint-1545/config.json\n",
      "Model weights saved in ./results/baselines/StanfordPoliteness/checkpoint-1545/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/joey/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/joey/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/home/joey/anaconda3/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 3335\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 525\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='525' max='525' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [525/525 03:58, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.716800</td>\n",
       "      <td>0.666300</td>\n",
       "      <td>0.548571</td>\n",
       "      <td>0.048193</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.024691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.716800</td>\n",
       "      <td>0.618478</td>\n",
       "      <td>0.645714</td>\n",
       "      <td>0.630952</td>\n",
       "      <td>0.609195</td>\n",
       "      <td>0.654321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.716800</td>\n",
       "      <td>0.576512</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.671053</td>\n",
       "      <td>0.718310</td>\n",
       "      <td>0.629630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.716800</td>\n",
       "      <td>0.819517</td>\n",
       "      <td>0.651429</td>\n",
       "      <td>0.590604</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.543210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.478200</td>\n",
       "      <td>0.777154</td>\n",
       "      <td>0.742857</td>\n",
       "      <td>0.676259</td>\n",
       "      <td>0.810345</td>\n",
       "      <td>0.580247</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 175\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/baselines/TroFi/checkpoint-105\n",
      "Configuration saved in ./results/baselines/TroFi/checkpoint-105/config.json\n",
      "Model weights saved in ./results/baselines/TroFi/checkpoint-105/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 175\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/baselines/TroFi/checkpoint-210\n",
      "Configuration saved in ./results/baselines/TroFi/checkpoint-210/config.json\n",
      "Model weights saved in ./results/baselines/TroFi/checkpoint-210/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 175\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/baselines/TroFi/checkpoint-315\n",
      "Configuration saved in ./results/baselines/TroFi/checkpoint-315/config.json\n",
      "Model weights saved in ./results/baselines/TroFi/checkpoint-315/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 175\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/baselines/TroFi/checkpoint-420\n",
      "Configuration saved in ./results/baselines/TroFi/checkpoint-420/config.json\n",
      "Model weights saved in ./results/baselines/TroFi/checkpoint-420/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 175\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/baselines/TroFi/checkpoint-525\n",
      "Configuration saved in ./results/baselines/TroFi/checkpoint-525/config.json\n",
      "Model weights saved in ./results/baselines/TroFi/checkpoint-525/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/joey/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/joey/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/home/joey/anaconda3/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 15157\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2370\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2370' max='2370' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2370/2370 19:48, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.701700</td>\n",
       "      <td>0.401460</td>\n",
       "      <td>0.822955</td>\n",
       "      <td>0.650602</td>\n",
       "      <td>0.697674</td>\n",
       "      <td>0.609481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.505300</td>\n",
       "      <td>0.349731</td>\n",
       "      <td>0.851038</td>\n",
       "      <td>0.720183</td>\n",
       "      <td>0.731935</td>\n",
       "      <td>0.708804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.334800</td>\n",
       "      <td>0.445463</td>\n",
       "      <td>0.863858</td>\n",
       "      <td>0.721598</td>\n",
       "      <td>0.807263</td>\n",
       "      <td>0.652370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.170300</td>\n",
       "      <td>0.629261</td>\n",
       "      <td>0.860806</td>\n",
       "      <td>0.726619</td>\n",
       "      <td>0.774936</td>\n",
       "      <td>0.683973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.073000</td>\n",
       "      <td>0.754388</td>\n",
       "      <td>0.862637</td>\n",
       "      <td>0.736225</td>\n",
       "      <td>0.765854</td>\n",
       "      <td>0.708804</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1638\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/baselines/VUA/checkpoint-474\n",
      "Configuration saved in ./results/baselines/VUA/checkpoint-474/config.json\n",
      "Model weights saved in ./results/baselines/VUA/checkpoint-474/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1638\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/baselines/VUA/checkpoint-948\n",
      "Configuration saved in ./results/baselines/VUA/checkpoint-948/config.json\n",
      "Model weights saved in ./results/baselines/VUA/checkpoint-948/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1638\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/baselines/VUA/checkpoint-1422\n",
      "Configuration saved in ./results/baselines/VUA/checkpoint-1422/config.json\n",
      "Model weights saved in ./results/baselines/VUA/checkpoint-1422/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1638\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/baselines/VUA/checkpoint-1896\n",
      "Configuration saved in ./results/baselines/VUA/checkpoint-1896/config.json\n",
      "Model weights saved in ./results/baselines/VUA/checkpoint-1896/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1638\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/baselines/VUA/checkpoint-2370\n",
      "Configuration saved in ./results/baselines/VUA/checkpoint-2370/config.json\n",
      "Model weights saved in ./results/baselines/VUA/checkpoint-2370/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for task in tasks:\n",
    "    if os.path.isdir(f'./results/baselines/{task}'):\n",
    "        print(f'skip {task}')\n",
    "        continue\n",
    "    torch.cuda.empty_cache()\n",
    "    model = None\n",
    "    trainer = None \n",
    "    \n",
    "    num_labels = tasks[task]\n",
    "    train_dataset = MyDataset(f'./processed/train/{task}.tsv')\n",
    "    test_dataset = MyDataset(f'./processed/test/{task}.tsv')\n",
    "    valid_dataset = MyDataset(f'./processed/dev/{task}.tsv')\n",
    "    \n",
    "    \n",
    "    singletaskbert = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=num_labels) \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f'./results/baselines/{task}',          # output directory\n",
    "        num_train_epochs=5,              # total number of training epochs\n",
    "        per_device_train_batch_size=batch_size,  # batch size per device during training\n",
    "        per_device_eval_batch_size=batch_size,   # batch size for evaluation\n",
    "        warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "        weight_decay=0.01,               # strength of weight decay\n",
    "        logging_dir=f'./results/baselines/{task}/logs',            # directory for storing logs\n",
    "        load_best_model_at_end=False,     # load the best model when finished training (default metric is loss)\n",
    "        # but you can specify `metric_for_best_model` argument to change to accuracy or other metric\n",
    "        logging_first_step = True, \n",
    "#         logging_steps=500,               # log & save weights each logging_steps\n",
    "#         save_steps=500,\n",
    "        evaluation_strategy=\"epoch\",     # evaluate each `logging_steps`\n",
    "        save_strategy='epoch'\n",
    "    )\n",
    "    from sklearn.metrics import precision_recall_fscore_support\n",
    "    \n",
    "    if num_labels == 1:\n",
    "        def compute_metrics(pred):\n",
    "            predictions, labels = pred\n",
    "            rmse = mean_squared_error(labels, predictions, squared=False)\n",
    "            return {\"rmse\": rmse}\n",
    "    elif num_labels == 2:\n",
    "        def compute_metrics(pred):\n",
    "            labels = pred.label_ids\n",
    "            preds = pred.predictions.argmax(-1)\n",
    "            precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "            acc = accuracy_score(labels, preds)\n",
    "            return {\n",
    "                'accuracy': acc,\n",
    "                'f1': f1,\n",
    "                'precision': precision,\n",
    "                'recall': recall\n",
    "            }\n",
    "    else:\n",
    "        def compute_metrics(pred):\n",
    "            labels = pred.label_ids\n",
    "            preds = pred.predictions.argmax(-1)\n",
    "            precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='macro')\n",
    "            acc = accuracy_score(labels, preds)\n",
    "            return {\n",
    "                'accuracy': acc,\n",
    "                'f1': f1,\n",
    "                'precision': precision,\n",
    "                'recall': recall\n",
    "            }\n",
    "\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=singletaskbert,   # the instantiated Transformers model to be trained\n",
    "        args=training_args,                  # training arguments, defined above\n",
    "        train_dataset=train_dataset,         # training dataset\n",
    "        eval_dataset=valid_dataset,          # evaluation dataset\n",
    "#         test_dataset=test_dataset,            # test dataset\n",
    "        compute_metrics=compute_metrics,     # the callback that computes metrics of interest\n",
    "    )\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accb4db9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93cb936",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
