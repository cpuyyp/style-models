{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b815f3bc",
   "metadata": {},
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#imports-and-functions\" data-toc-modified-id=\"imports-and-functions-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>imports and functions</a></span></li><li><span><a href=\"#experiments\" data-toc-modified-id=\"experiments-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>experiments</a></span><ul class=\"toc-item\"><li><span><a href=\"#12\" data-toc-modified-id=\"12-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>12</a></span></li><li><span><a href=\"#13\" data-toc-modified-id=\"13-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>13</a></span></li><li><span><a href=\"#14\" data-toc-modified-id=\"14-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>14</a></span></li><li><span><a href=\"#15\" data-toc-modified-id=\"15-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>15</a></span></li><li><span><a href=\"#16\" data-toc-modified-id=\"16-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>16</a></span><ul class=\"toc-item\"><li><span><a href=\"#debug\" data-toc-modified-id=\"debug-2.5.1\"><span class=\"toc-item-num\">2.5.1&nbsp;&nbsp;</span>debug</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffabe0a0",
   "metadata": {},
   "source": [
    "# imports and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901c5aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import optim\n",
    "from torch.optim import lr_scheduler\n",
    "# import torchmetrics\n",
    "\n",
    "import datasets\n",
    "from datasets import load_metric\n",
    "from transformers import AutoConfig, AutoTokenizer, BertModel, RobertaModel\n",
    "from transformers import BertForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, precision_recall_fscore_support\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f77a72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/huggingface/transformers/issues/5486\n",
    "# os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5336c72c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7cb7f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CrowdFlower': 13,\n",
       " 'DailyDialog': 7,\n",
       " 'EmoBank_Valence': 1,\n",
       " 'EmoBank_Arousal': 1,\n",
       " 'EmoBank_Dominance': 1,\n",
       " 'HateOffensive': 3,\n",
       " 'PASTEL_age': 8,\n",
       " 'PASTEL_country': 2,\n",
       " 'PASTEL_education': 10,\n",
       " 'PASTEL_ethnic': 10,\n",
       " 'PASTEL_gender': 3,\n",
       " 'PASTEL_politics': 3,\n",
       " 'PASTEL_tod': 5,\n",
       " 'SARC': 2,\n",
       " 'SarcasmGhosh': 2,\n",
       " 'SentiTreeBank': 1,\n",
       " 'ShortHumor': 2,\n",
       " 'ShortJokeKaggle': 2,\n",
       " 'ShortRomance': 2,\n",
       " 'StanfordPoliteness': 1,\n",
       " 'TroFi': 2,\n",
       " 'VUA': 2}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../../data/xslue/tasks.json', 'r') as f:\n",
    "    tasks = json.load(f)\n",
    "tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9092e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset): \n",
    "    # currently it's a Mapping-style dataset. Not sure if a Iterable-style dataset will be better\n",
    "    def __init__(self, tsv_file):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "        self.df = pd.read_csv(tsv_file, sep='\\t')\n",
    "        self.df = self.df.dropna()\n",
    "        self.df = self.df.reset_index(drop=True)\n",
    "        self.encodings = self.tokenizer(self.df['text'].tolist(), truncation=True, padding=True, max_length=128)\n",
    "        if self.df['label'].dtype == 'float64':\n",
    "            self.df['label'] = self.df['label'].astype('float32')\n",
    "        self.labels = self.df['label'].tolist()\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor([self.labels[idx]])\n",
    "        return item\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ec8dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pearsonr = load_metric(\"pearsonr\")\n",
    "spearmanr = load_metric(\"spearmanr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57e779d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU memory usage: 6617 - 6680mb with bs 32\n",
    "# bs 64 gives OOM\n",
    "# bs 48 GPU memory 7894\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44297cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_baseline(task, freeze_bert=False):\n",
    "    torch.cuda.empty_cache()\n",
    "    model = None\n",
    "    trainer = None \n",
    "    num_labels = tasks[task]\n",
    "    \n",
    "    data_folder = '../../data/xslue'\n",
    "    train_dataset = MyDataset(f'{data_folder}/processed/train/{task}.tsv')\n",
    "    test_dataset = MyDataset(f'{data_folder}/processed/test/{task}.tsv')\n",
    "    valid_dataset = MyDataset(f'{data_folder}/processed/dev/{task}.tsv')\n",
    "    \n",
    "    singletaskbert = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=num_labels) \n",
    "    if freeze_bert:\n",
    "        for param in singletaskbert.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "    result_folder = '../../result'\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"{result_folder}/baselines/{task+'_freezed' if freeze_bert else task}\",   # output directory\n",
    "        num_train_epochs=5,              # total number of training epochs\n",
    "        per_device_train_batch_size=batch_size,  # batch size per device during training\n",
    "        per_device_eval_batch_size=batch_size,   # batch size for evaluation\n",
    "        warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "        weight_decay=0.01,               # strength of weight decay\n",
    "        logging_dir=f\"{result_folder}/baselines/{task+'_freezed' if freeze_bert else task}/logs\",  # directory for storing logs\n",
    "#         logging_first_step = True, \n",
    "#         logging_steps=500,               # log & save weights each logging_steps\n",
    "#         save_steps=500,\n",
    "        evaluation_strategy=\"epoch\",     # evaluate each `logging_steps`\n",
    "        save_total_limit = 1,\n",
    "        save_strategy = 'epoch',\n",
    "        load_best_model_at_end=True, # decide on loss\n",
    "    )\n",
    "    \n",
    "    if num_labels == 1:\n",
    "        def compute_metrics(pred):\n",
    "            predictions, labels = pred\n",
    "            rmse = mean_squared_error(labels, predictions, squared=False)\n",
    "            return {\"rmse\": rmse}\n",
    "    elif num_labels == 2:\n",
    "        def compute_metrics(pred):\n",
    "            labels = pred.label_ids\n",
    "            preds = pred.predictions.argmax(-1)\n",
    "            precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "            acc = accuracy_score(labels, preds)\n",
    "            return {\n",
    "                'accuracy': acc,\n",
    "                'f1': f1,\n",
    "                'precision': precision,\n",
    "                'recall': recall\n",
    "            }\n",
    "    else:\n",
    "        def compute_metrics(pred):\n",
    "            labels = pred.label_ids\n",
    "            preds = pred.predictions.argmax(-1)\n",
    "            precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='macro')\n",
    "            acc = accuracy_score(labels, preds)\n",
    "            return {\n",
    "                'accuracy': acc,\n",
    "                'f1': f1,\n",
    "                'precision': precision,\n",
    "                'recall': recall\n",
    "            }\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=singletaskbert,   # the instantiated Transformers model to be trained\n",
    "        args=training_args,                  # training arguments, defined above\n",
    "        train_dataset=train_dataset,         # training dataset\n",
    "        eval_dataset=valid_dataset,          # evaluation dataset\n",
    "#         test_dataset=test_dataset,            # test dataset\n",
    "        compute_metrics=compute_metrics,     # the callback that computes metrics of interest\n",
    "    )\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff2f130",
   "metadata": {},
   "source": [
    "# experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7c2d20",
   "metadata": {},
   "source": [
    "## 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6156b973",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 33240\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 5195\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5195' max='5195' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5195/5195 25:51, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.296100</td>\n",
       "      <td>1.299696</td>\n",
       "      <td>0.470036</td>\n",
       "      <td>0.266865</td>\n",
       "      <td>0.404191</td>\n",
       "      <td>0.292907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.264400</td>\n",
       "      <td>1.298557</td>\n",
       "      <td>0.471480</td>\n",
       "      <td>0.271082</td>\n",
       "      <td>0.441297</td>\n",
       "      <td>0.295143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.101700</td>\n",
       "      <td>1.368624</td>\n",
       "      <td>0.429122</td>\n",
       "      <td>0.307349</td>\n",
       "      <td>0.328053</td>\n",
       "      <td>0.309603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.728000</td>\n",
       "      <td>1.682379</td>\n",
       "      <td>0.427196</td>\n",
       "      <td>0.306962</td>\n",
       "      <td>0.323563</td>\n",
       "      <td>0.310037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.448200</td>\n",
       "      <td>2.056610</td>\n",
       "      <td>0.419495</td>\n",
       "      <td>0.313106</td>\n",
       "      <td>0.323609</td>\n",
       "      <td>0.311995</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 4155\n",
      "  Batch size = 32\n",
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ../../result/baselines/PASTEL_tod/checkpoint-1039\n",
      "Configuration saved in ../../result/baselines/PASTEL_tod/checkpoint-1039/config.json\n",
      "Model weights saved in ../../result/baselines/PASTEL_tod/checkpoint-1039/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4155\n",
      "  Batch size = 32\n",
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ../../result/baselines/PASTEL_tod/checkpoint-2078\n",
      "Configuration saved in ../../result/baselines/PASTEL_tod/checkpoint-2078/config.json\n",
      "Model weights saved in ../../result/baselines/PASTEL_tod/checkpoint-2078/pytorch_model.bin\n",
      "Deleting older checkpoint [../../result/baselines/PASTEL_tod/checkpoint-1039] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4155\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ../../result/baselines/PASTEL_tod/checkpoint-3117\n",
      "Configuration saved in ../../result/baselines/PASTEL_tod/checkpoint-3117/config.json\n",
      "Model weights saved in ../../result/baselines/PASTEL_tod/checkpoint-3117/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4155\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ../../result/baselines/PASTEL_tod/checkpoint-4156\n",
      "Configuration saved in ../../result/baselines/PASTEL_tod/checkpoint-4156/config.json\n",
      "Model weights saved in ../../result/baselines/PASTEL_tod/checkpoint-4156/pytorch_model.bin\n",
      "Deleting older checkpoint [../../result/baselines/PASTEL_tod/checkpoint-3117] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4155\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ../../result/baselines/PASTEL_tod/checkpoint-5195\n",
      "Configuration saved in ../../result/baselines/PASTEL_tod/checkpoint-5195/config.json\n",
      "Model weights saved in ../../result/baselines/PASTEL_tod/checkpoint-5195/pytorch_model.bin\n",
      "Deleting older checkpoint [../../result/baselines/PASTEL_tod/checkpoint-4156] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ../../result/baselines/PASTEL_tod/checkpoint-2078 (score: 1.298557162284851).\n"
     ]
    }
   ],
   "source": [
    "task = list(tasks.keys())[12]\n",
    "train_baseline(task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9a7f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/jz17d/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/jz17d/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/jz17d/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/jz17d/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/jz17d/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/jz17d/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/jz17d/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/jz17d/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/jz17d/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/jz17d/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running training *****\n",
      "  Num examples = 33240\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 5195\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5195' max='5195' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5195/5195 09:35, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.426500</td>\n",
       "      <td>1.421628</td>\n",
       "      <td>0.401444</td>\n",
       "      <td>0.184231</td>\n",
       "      <td>0.165445</td>\n",
       "      <td>0.237363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.422200</td>\n",
       "      <td>1.416608</td>\n",
       "      <td>0.407702</td>\n",
       "      <td>0.184889</td>\n",
       "      <td>0.173160</td>\n",
       "      <td>0.239852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.414500</td>\n",
       "      <td>1.413576</td>\n",
       "      <td>0.409386</td>\n",
       "      <td>0.186414</td>\n",
       "      <td>0.173394</td>\n",
       "      <td>0.241145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.421700</td>\n",
       "      <td>1.413384</td>\n",
       "      <td>0.410830</td>\n",
       "      <td>0.184860</td>\n",
       "      <td>0.177410</td>\n",
       "      <td>0.241053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.415300</td>\n",
       "      <td>1.412030</td>\n",
       "      <td>0.408664</td>\n",
       "      <td>0.185410</td>\n",
       "      <td>0.174045</td>\n",
       "      <td>0.240421</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 4155\n",
      "  Batch size = 32\n",
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ../../result/baselines/PASTEL_tod_freezed/checkpoint-1039\n",
      "Configuration saved in ../../result/baselines/PASTEL_tod_freezed/checkpoint-1039/config.json\n",
      "Model weights saved in ../../result/baselines/PASTEL_tod_freezed/checkpoint-1039/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4155\n",
      "  Batch size = 32\n",
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ../../result/baselines/PASTEL_tod_freezed/checkpoint-2078\n",
      "Configuration saved in ../../result/baselines/PASTEL_tod_freezed/checkpoint-2078/config.json\n",
      "Model weights saved in ../../result/baselines/PASTEL_tod_freezed/checkpoint-2078/pytorch_model.bin\n",
      "Deleting older checkpoint [../../result/baselines/PASTEL_tod_freezed/checkpoint-1039] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4155\n",
      "  Batch size = 32\n",
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ../../result/baselines/PASTEL_tod_freezed/checkpoint-3117\n",
      "Configuration saved in ../../result/baselines/PASTEL_tod_freezed/checkpoint-3117/config.json\n",
      "Model weights saved in ../../result/baselines/PASTEL_tod_freezed/checkpoint-3117/pytorch_model.bin\n",
      "Deleting older checkpoint [../../result/baselines/PASTEL_tod_freezed/checkpoint-2078] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4155\n",
      "  Batch size = 32\n",
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ../../result/baselines/PASTEL_tod_freezed/checkpoint-4156\n",
      "Configuration saved in ../../result/baselines/PASTEL_tod_freezed/checkpoint-4156/config.json\n",
      "Model weights saved in ../../result/baselines/PASTEL_tod_freezed/checkpoint-4156/pytorch_model.bin\n",
      "Deleting older checkpoint [../../result/baselines/PASTEL_tod_freezed/checkpoint-3117] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4155\n",
      "  Batch size = 32\n",
      "/home/jz17d/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ../../result/baselines/PASTEL_tod_freezed/checkpoint-5195\n",
      "Configuration saved in ../../result/baselines/PASTEL_tod_freezed/checkpoint-5195/config.json\n",
      "Model weights saved in ../../result/baselines/PASTEL_tod_freezed/checkpoint-5195/pytorch_model.bin\n",
      "Deleting older checkpoint [../../result/baselines/PASTEL_tod_freezed/checkpoint-4156] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ../../result/baselines/PASTEL_tod_freezed/checkpoint-5195 (score: 1.41202974319458).\n"
     ]
    }
   ],
   "source": [
    "task = list(tasks.keys())[12]\n",
    "train_baseline(task, freeze_bert=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5cc70d",
   "metadata": {},
   "source": [
    "## 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1f1fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/jz17d/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/jz17d/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/jz17d/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/jz17d/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/jz17d/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/jz17d/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/jz17d/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/jz17d/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/jz17d/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/jz17d/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running training *****\n",
      "  Num examples = 205644\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 32135\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='32135' max='32135' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [32135/32135 3:35:57, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.560500</td>\n",
       "      <td>0.555968</td>\n",
       "      <td>0.714375</td>\n",
       "      <td>0.724823</td>\n",
       "      <td>0.698260</td>\n",
       "      <td>0.753487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.487500</td>\n",
       "      <td>0.569209</td>\n",
       "      <td>0.720288</td>\n",
       "      <td>0.726626</td>\n",
       "      <td>0.709497</td>\n",
       "      <td>0.744604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.353700</td>\n",
       "      <td>0.669095</td>\n",
       "      <td>0.712274</td>\n",
       "      <td>0.716726</td>\n",
       "      <td>0.704768</td>\n",
       "      <td>0.729097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.205900</td>\n",
       "      <td>0.887727</td>\n",
       "      <td>0.702801</td>\n",
       "      <td>0.715671</td>\n",
       "      <td>0.685013</td>\n",
       "      <td>0.749201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.124200</td>\n",
       "      <td>1.239568</td>\n",
       "      <td>0.702840</td>\n",
       "      <td>0.711281</td>\n",
       "      <td>0.690645</td>\n",
       "      <td>0.733188</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 51410\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ../../result/baselines/SARC/checkpoint-6427\n",
      "Configuration saved in ../../result/baselines/SARC/checkpoint-6427/config.json\n",
      "Model weights saved in ../../result/baselines/SARC/checkpoint-6427/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 51410\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ../../result/baselines/SARC/checkpoint-12854\n",
      "Configuration saved in ../../result/baselines/SARC/checkpoint-12854/config.json\n",
      "Model weights saved in ../../result/baselines/SARC/checkpoint-12854/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 51410\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ../../result/baselines/SARC/checkpoint-19281\n",
      "Configuration saved in ../../result/baselines/SARC/checkpoint-19281/config.json\n",
      "Model weights saved in ../../result/baselines/SARC/checkpoint-19281/pytorch_model.bin\n",
      "Deleting older checkpoint [../../result/baselines/SARC/checkpoint-12854] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 51410\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ../../result/baselines/SARC/checkpoint-25708\n",
      "Configuration saved in ../../result/baselines/SARC/checkpoint-25708/config.json\n",
      "Model weights saved in ../../result/baselines/SARC/checkpoint-25708/pytorch_model.bin\n",
      "Deleting older checkpoint [../../result/baselines/SARC/checkpoint-19281] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 51410\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ../../result/baselines/SARC/checkpoint-32135\n",
      "Configuration saved in ../../result/baselines/SARC/checkpoint-32135/config.json\n",
      "Model weights saved in ../../result/baselines/SARC/checkpoint-32135/pytorch_model.bin\n",
      "Deleting older checkpoint [../../result/baselines/SARC/checkpoint-25708] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ../../result/baselines/SARC/checkpoint-6427 (score: 0.5559675097465515).\n"
     ]
    }
   ],
   "source": [
    "task = list(tasks.keys())[13]\n",
    "train_baseline(task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3568f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/jz17d/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/jz17d/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/jz17d/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/jz17d/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/jz17d/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/jz17d/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/jz17d/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/jz17d/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/jz17d/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/jz17d/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running training *****\n",
      "  Num examples = 205644\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 32135\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='32135' max='32135' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [32135/32135 1:26:25, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.680100</td>\n",
       "      <td>0.672663</td>\n",
       "      <td>0.592531</td>\n",
       "      <td>0.603032</td>\n",
       "      <td>0.587035</td>\n",
       "      <td>0.619925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.673100</td>\n",
       "      <td>0.666404</td>\n",
       "      <td>0.600350</td>\n",
       "      <td>0.599509</td>\n",
       "      <td>0.599860</td>\n",
       "      <td>0.599158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.671500</td>\n",
       "      <td>0.663911</td>\n",
       "      <td>0.599300</td>\n",
       "      <td>0.610114</td>\n",
       "      <td>0.593228</td>\n",
       "      <td>0.627990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.668300</td>\n",
       "      <td>0.662713</td>\n",
       "      <td>0.608247</td>\n",
       "      <td>0.576161</td>\n",
       "      <td>0.626442</td>\n",
       "      <td>0.533352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.670400</td>\n",
       "      <td>0.662071</td>\n",
       "      <td>0.606691</td>\n",
       "      <td>0.596149</td>\n",
       "      <td>0.611589</td>\n",
       "      <td>0.581470</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 51410\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ../../result/baselines/SARC_freezed/checkpoint-6427\n",
      "Configuration saved in ../../result/baselines/SARC_freezed/checkpoint-6427/config.json\n",
      "Model weights saved in ../../result/baselines/SARC_freezed/checkpoint-6427/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 51410\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ../../result/baselines/SARC_freezed/checkpoint-12854\n",
      "Configuration saved in ../../result/baselines/SARC_freezed/checkpoint-12854/config.json\n",
      "Model weights saved in ../../result/baselines/SARC_freezed/checkpoint-12854/pytorch_model.bin\n",
      "Deleting older checkpoint [../../result/baselines/SARC_freezed/checkpoint-6427] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 51410\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ../../result/baselines/SARC_freezed/checkpoint-19281\n",
      "Configuration saved in ../../result/baselines/SARC_freezed/checkpoint-19281/config.json\n",
      "Model weights saved in ../../result/baselines/SARC_freezed/checkpoint-19281/pytorch_model.bin\n",
      "Deleting older checkpoint [../../result/baselines/SARC_freezed/checkpoint-12854] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 51410\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ../../result/baselines/SARC_freezed/checkpoint-25708\n",
      "Configuration saved in ../../result/baselines/SARC_freezed/checkpoint-25708/config.json\n",
      "Model weights saved in ../../result/baselines/SARC_freezed/checkpoint-25708/pytorch_model.bin\n",
      "Deleting older checkpoint [../../result/baselines/SARC_freezed/checkpoint-19281] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 51410\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ../../result/baselines/SARC_freezed/checkpoint-32135\n",
      "Configuration saved in ../../result/baselines/SARC_freezed/checkpoint-32135/config.json\n",
      "Model weights saved in ../../result/baselines/SARC_freezed/checkpoint-32135/pytorch_model.bin\n",
      "Deleting older checkpoint [../../result/baselines/SARC_freezed/checkpoint-25708] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ../../result/baselines/SARC_freezed/checkpoint-32135 (score: 0.6620712280273438).\n"
     ]
    }
   ],
   "source": [
    "task = list(tasks.keys())[13]\n",
    "train_baseline(task, freeze_bert=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084a2db6",
   "metadata": {},
   "source": [
    "## 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb9b16f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/jz17d/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/jz17d/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/jz17d/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/jz17d/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/jz17d/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/jz17d/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/jz17d/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/jz17d/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/jz17d/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/jz17d/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running training *****\n",
      "  Num examples = 39780\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 6220\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6220' max='6220' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6220/6220 29:16, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.013800</td>\n",
       "      <td>0.172218</td>\n",
       "      <td>0.977085</td>\n",
       "      <td>0.640000</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>0.551724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.010300</td>\n",
       "      <td>0.199137</td>\n",
       "      <td>0.977085</td>\n",
       "      <td>0.640000</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>0.551724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.005900</td>\n",
       "      <td>0.212433</td>\n",
       "      <td>0.977721</td>\n",
       "      <td>0.646465</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.551724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.002600</td>\n",
       "      <td>0.230788</td>\n",
       "      <td>0.977721</td>\n",
       "      <td>0.646465</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.551724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.239562</td>\n",
       "      <td>0.977721</td>\n",
       "      <td>0.646465</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.551724</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1571\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ../../result/baselines/SarcasmGhosh/checkpoint-1244\n",
      "Configuration saved in ../../result/baselines/SarcasmGhosh/checkpoint-1244/config.json\n",
      "Model weights saved in ../../result/baselines/SarcasmGhosh/checkpoint-1244/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1571\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ../../result/baselines/SarcasmGhosh/checkpoint-2488\n",
      "Configuration saved in ../../result/baselines/SarcasmGhosh/checkpoint-2488/config.json\n",
      "Model weights saved in ../../result/baselines/SarcasmGhosh/checkpoint-2488/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1571\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ../../result/baselines/SarcasmGhosh/checkpoint-3732\n",
      "Configuration saved in ../../result/baselines/SarcasmGhosh/checkpoint-3732/config.json\n",
      "Model weights saved in ../../result/baselines/SarcasmGhosh/checkpoint-3732/pytorch_model.bin\n",
      "Deleting older checkpoint [../../result/baselines/SarcasmGhosh/checkpoint-2488] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1571\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ../../result/baselines/SarcasmGhosh/checkpoint-4976\n",
      "Configuration saved in ../../result/baselines/SarcasmGhosh/checkpoint-4976/config.json\n",
      "Model weights saved in ../../result/baselines/SarcasmGhosh/checkpoint-4976/pytorch_model.bin\n",
      "Deleting older checkpoint [../../result/baselines/SarcasmGhosh/checkpoint-3732] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1571\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ../../result/baselines/SarcasmGhosh/checkpoint-6220\n",
      "Configuration saved in ../../result/baselines/SarcasmGhosh/checkpoint-6220/config.json\n",
      "Model weights saved in ../../result/baselines/SarcasmGhosh/checkpoint-6220/pytorch_model.bin\n",
      "Deleting older checkpoint [../../result/baselines/SarcasmGhosh/checkpoint-4976] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ../../result/baselines/SarcasmGhosh/checkpoint-1244 (score: 0.17221812903881073).\n"
     ]
    }
   ],
   "source": [
    "task = list(tasks.keys())[14]\n",
    "train_baseline(task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d353f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/jz17d/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/jz17d/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/jz17d/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/jz17d/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/jz17d/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/jz17d/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/jz17d/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/jz17d/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/jz17d/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/jz17d/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running training *****\n",
      "  Num examples = 39780\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 6220\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6220' max='6220' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6220/6220 10:36, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.550487</td>\n",
       "      <td>0.810312</td>\n",
       "      <td>0.207447</td>\n",
       "      <td>0.122642</td>\n",
       "      <td>0.672414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.536100</td>\n",
       "      <td>0.504274</td>\n",
       "      <td>0.788670</td>\n",
       "      <td>0.198068</td>\n",
       "      <td>0.115169</td>\n",
       "      <td>0.706897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.472600</td>\n",
       "      <td>0.473396</td>\n",
       "      <td>0.793762</td>\n",
       "      <td>0.201970</td>\n",
       "      <td>0.117816</td>\n",
       "      <td>0.706897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.454600</td>\n",
       "      <td>0.452626</td>\n",
       "      <td>0.805856</td>\n",
       "      <td>0.211886</td>\n",
       "      <td>0.124620</td>\n",
       "      <td>0.706897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.434900</td>\n",
       "      <td>0.450173</td>\n",
       "      <td>0.802673</td>\n",
       "      <td>0.209184</td>\n",
       "      <td>0.122754</td>\n",
       "      <td>0.706897</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1571\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ../../result/baselines/SarcasmGhosh_freezed/checkpoint-1244\n",
      "Configuration saved in ../../result/baselines/SarcasmGhosh_freezed/checkpoint-1244/config.json\n",
      "Model weights saved in ../../result/baselines/SarcasmGhosh_freezed/checkpoint-1244/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1571\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ../../result/baselines/SarcasmGhosh_freezed/checkpoint-2488\n",
      "Configuration saved in ../../result/baselines/SarcasmGhosh_freezed/checkpoint-2488/config.json\n",
      "Model weights saved in ../../result/baselines/SarcasmGhosh_freezed/checkpoint-2488/pytorch_model.bin\n",
      "Deleting older checkpoint [../../result/baselines/SarcasmGhosh_freezed/checkpoint-1244] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1571\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ../../result/baselines/SarcasmGhosh_freezed/checkpoint-3732\n",
      "Configuration saved in ../../result/baselines/SarcasmGhosh_freezed/checkpoint-3732/config.json\n",
      "Model weights saved in ../../result/baselines/SarcasmGhosh_freezed/checkpoint-3732/pytorch_model.bin\n",
      "Deleting older checkpoint [../../result/baselines/SarcasmGhosh_freezed/checkpoint-2488] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1571\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ../../result/baselines/SarcasmGhosh_freezed/checkpoint-4976\n",
      "Configuration saved in ../../result/baselines/SarcasmGhosh_freezed/checkpoint-4976/config.json\n",
      "Model weights saved in ../../result/baselines/SarcasmGhosh_freezed/checkpoint-4976/pytorch_model.bin\n",
      "Deleting older checkpoint [../../result/baselines/SarcasmGhosh_freezed/checkpoint-3732] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1571\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ../../result/baselines/SarcasmGhosh_freezed/checkpoint-6220\n",
      "Configuration saved in ../../result/baselines/SarcasmGhosh_freezed/checkpoint-6220/config.json\n",
      "Model weights saved in ../../result/baselines/SarcasmGhosh_freezed/checkpoint-6220/pytorch_model.bin\n",
      "Deleting older checkpoint [../../result/baselines/SarcasmGhosh_freezed/checkpoint-4976] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ../../result/baselines/SarcasmGhosh_freezed/checkpoint-6220 (score: 0.4501734673976898).\n"
     ]
    }
   ],
   "source": [
    "task = list(tasks.keys())[14]\n",
    "train_baseline(task, freeze_bert=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff445af7",
   "metadata": {},
   "source": [
    "## 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef153cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/jz17d/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/jz17d/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/jz17d/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/jz17d/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/jz17d/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/jz17d/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/jz17d/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/jz17d/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/jz17d/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/jz17d/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running training *****\n",
      "  Num examples = 236076\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 36890\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='36890' max='36890' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [36890/36890 2:12:21, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.009300</td>\n",
       "      <td>0.020625</td>\n",
       "      <td>0.143613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.007400</td>\n",
       "      <td>0.013541</td>\n",
       "      <td>0.116365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.006100</td>\n",
       "      <td>0.013954</td>\n",
       "      <td>0.118127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.005100</td>\n",
       "      <td>0.015877</td>\n",
       "      <td>0.126003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.004200</td>\n",
       "      <td>0.014584</td>\n",
       "      <td>0.120764</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1044\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ../../result/baselines/SentiTreeBank/checkpoint-7378\n",
      "Configuration saved in ../../result/baselines/SentiTreeBank/checkpoint-7378/config.json\n",
      "Model weights saved in ../../result/baselines/SentiTreeBank/checkpoint-7378/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1044\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ../../result/baselines/SentiTreeBank/checkpoint-14756\n",
      "Configuration saved in ../../result/baselines/SentiTreeBank/checkpoint-14756/config.json\n",
      "Model weights saved in ../../result/baselines/SentiTreeBank/checkpoint-14756/pytorch_model.bin\n",
      "Deleting older checkpoint [../../result/baselines/SentiTreeBank/checkpoint-7378] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1044\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ../../result/baselines/SentiTreeBank/checkpoint-22134\n",
      "Configuration saved in ../../result/baselines/SentiTreeBank/checkpoint-22134/config.json\n",
      "Model weights saved in ../../result/baselines/SentiTreeBank/checkpoint-22134/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1044\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ../../result/baselines/SentiTreeBank/checkpoint-29512\n",
      "Configuration saved in ../../result/baselines/SentiTreeBank/checkpoint-29512/config.json\n",
      "Model weights saved in ../../result/baselines/SentiTreeBank/checkpoint-29512/pytorch_model.bin\n",
      "Deleting older checkpoint [../../result/baselines/SentiTreeBank/checkpoint-22134] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1044\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ../../result/baselines/SentiTreeBank/checkpoint-36890\n",
      "Configuration saved in ../../result/baselines/SentiTreeBank/checkpoint-36890/config.json\n",
      "Model weights saved in ../../result/baselines/SentiTreeBank/checkpoint-36890/pytorch_model.bin\n",
      "Deleting older checkpoint [../../result/baselines/SentiTreeBank/checkpoint-29512] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ../../result/baselines/SentiTreeBank/checkpoint-14756 (score: 0.013540763407945633).\n"
     ]
    }
   ],
   "source": [
    "task = list(tasks.keys())[15]\n",
    "train_baseline(task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d39cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/jz17d/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/jz17d/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/jz17d/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/jz17d/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/jz17d/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/jz17d/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/jz17d/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/jz17d/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/jz17d/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/jz17d/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running training *****\n",
      "  Num examples = 236076\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 36890\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='36890' max='36890' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [36890/36890 45:02, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.026600</td>\n",
       "      <td>0.046444</td>\n",
       "      <td>0.215510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.024200</td>\n",
       "      <td>0.042352</td>\n",
       "      <td>0.205795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.023700</td>\n",
       "      <td>0.041683</td>\n",
       "      <td>0.204163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.023100</td>\n",
       "      <td>0.042541</td>\n",
       "      <td>0.206255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.024000</td>\n",
       "      <td>0.041688</td>\n",
       "      <td>0.204176</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1044\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ../../result/baselines/SentiTreeBank_freezed/checkpoint-7378\n",
      "Configuration saved in ../../result/baselines/SentiTreeBank_freezed/checkpoint-7378/config.json\n",
      "Model weights saved in ../../result/baselines/SentiTreeBank_freezed/checkpoint-7378/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1044\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ../../result/baselines/SentiTreeBank_freezed/checkpoint-14756\n",
      "Configuration saved in ../../result/baselines/SentiTreeBank_freezed/checkpoint-14756/config.json\n",
      "Model weights saved in ../../result/baselines/SentiTreeBank_freezed/checkpoint-14756/pytorch_model.bin\n",
      "Deleting older checkpoint [../../result/baselines/SentiTreeBank_freezed/checkpoint-7378] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1044\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ../../result/baselines/SentiTreeBank_freezed/checkpoint-22134\n",
      "Configuration saved in ../../result/baselines/SentiTreeBank_freezed/checkpoint-22134/config.json\n",
      "Model weights saved in ../../result/baselines/SentiTreeBank_freezed/checkpoint-22134/pytorch_model.bin\n",
      "Deleting older checkpoint [../../result/baselines/SentiTreeBank_freezed/checkpoint-14756] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1044\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ../../result/baselines/SentiTreeBank_freezed/checkpoint-29512\n",
      "Configuration saved in ../../result/baselines/SentiTreeBank_freezed/checkpoint-29512/config.json\n",
      "Model weights saved in ../../result/baselines/SentiTreeBank_freezed/checkpoint-29512/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1044\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ../../result/baselines/SentiTreeBank_freezed/checkpoint-36890\n",
      "Configuration saved in ../../result/baselines/SentiTreeBank_freezed/checkpoint-36890/config.json\n",
      "Model weights saved in ../../result/baselines/SentiTreeBank_freezed/checkpoint-36890/pytorch_model.bin\n",
      "Deleting older checkpoint [../../result/baselines/SentiTreeBank_freezed/checkpoint-29512] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ../../result/baselines/SentiTreeBank_freezed/checkpoint-22134 (score: 0.041682567447423935).\n"
     ]
    }
   ],
   "source": [
    "task = list(tasks.keys())[15]\n",
    "train_baseline(task, freeze_bert=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856ff266",
   "metadata": {},
   "source": [
    "## 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7b99fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/jz17d/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/jz17d/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/jz17d/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/jz17d/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/jz17d/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/jz17d/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/jz17d/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/jz17d/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/jz17d/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/jz17d/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running training *****\n",
      "  Num examples = 37801\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 5910\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5910' max='5910' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5910/5910 39:02, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.169300</td>\n",
       "      <td>0.135504</td>\n",
       "      <td>0.956631</td>\n",
       "      <td>0.956784</td>\n",
       "      <td>0.950100</td>\n",
       "      <td>0.963563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.077000</td>\n",
       "      <td>0.091944</td>\n",
       "      <td>0.971760</td>\n",
       "      <td>0.971602</td>\n",
       "      <td>0.973577</td>\n",
       "      <td>0.969636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.018900</td>\n",
       "      <td>0.152957</td>\n",
       "      <td>0.966717</td>\n",
       "      <td>0.967131</td>\n",
       "      <td>0.951961</td>\n",
       "      <td>0.982794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.006200</td>\n",
       "      <td>0.198445</td>\n",
       "      <td>0.968230</td>\n",
       "      <td>0.968358</td>\n",
       "      <td>0.961117</td>\n",
       "      <td>0.975709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.001900</td>\n",
       "      <td>0.218333</td>\n",
       "      <td>0.967726</td>\n",
       "      <td>0.967904</td>\n",
       "      <td>0.959245</td>\n",
       "      <td>0.976721</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1983\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ../../result/baselines/ShortHumor/checkpoint-1182\n",
      "Configuration saved in ../../result/baselines/ShortHumor/checkpoint-1182/config.json\n",
      "Model weights saved in ../../result/baselines/ShortHumor/checkpoint-1182/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1983\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ../../result/baselines/ShortHumor/checkpoint-2364\n",
      "Configuration saved in ../../result/baselines/ShortHumor/checkpoint-2364/config.json\n",
      "Model weights saved in ../../result/baselines/ShortHumor/checkpoint-2364/pytorch_model.bin\n",
      "Deleting older checkpoint [../../result/baselines/ShortHumor/checkpoint-1182] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1983\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ../../result/baselines/ShortHumor/checkpoint-3546\n",
      "Configuration saved in ../../result/baselines/ShortHumor/checkpoint-3546/config.json\n",
      "Model weights saved in ../../result/baselines/ShortHumor/checkpoint-3546/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1983\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ../../result/baselines/ShortHumor/checkpoint-4728\n",
      "Configuration saved in ../../result/baselines/ShortHumor/checkpoint-4728/config.json\n",
      "Model weights saved in ../../result/baselines/ShortHumor/checkpoint-4728/pytorch_model.bin\n",
      "Deleting older checkpoint [../../result/baselines/ShortHumor/checkpoint-3546] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1983\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ../../result/baselines/ShortHumor/checkpoint-5910\n",
      "Configuration saved in ../../result/baselines/ShortHumor/checkpoint-5910/config.json\n",
      "Model weights saved in ../../result/baselines/ShortHumor/checkpoint-5910/pytorch_model.bin\n",
      "Deleting older checkpoint [../../result/baselines/ShortHumor/checkpoint-4728] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ../../result/baselines/ShortHumor/checkpoint-2364 (score: 0.09194370359182358).\n"
     ]
    }
   ],
   "source": [
    "task = list(tasks.keys())[16]\n",
    "train_baseline(task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd58465",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "singletaskbert.load_state_dict(torch.load('../../result/baselines/ShortHumor/checkpoint-5910/pytorch_model.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce9f58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model=singletaskbert, args=training_args, compute_metrics=compute_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c8fd95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 37801\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1182' max='1182' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1182/1182 03:29]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.00048118969425559044,\n",
       " 'eval_accuracy': 0.9998941826935795,\n",
       " 'eval_f1': 0.9998942414467769,\n",
       " 'eval_precision': 0.9998413705583756,\n",
       " 'eval_recall': 0.9999471179270227,\n",
       " 'eval_runtime': 209.4594,\n",
       " 'eval_samples_per_second': 180.469,\n",
       " 'eval_steps_per_second': 5.643}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d3bbe6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d21407c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/jz17d/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/jz17d/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/jz17d/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/jz17d/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/jz17d/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/jz17d/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/jz17d/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/jz17d/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/jz17d/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/jz17d/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running training *****\n",
      "  Num examples = 37801\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 5910\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5910' max='5910' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5910/5910 14:06, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.618600</td>\n",
       "      <td>0.573215</td>\n",
       "      <td>0.702975</td>\n",
       "      <td>0.737171</td>\n",
       "      <td>0.659218</td>\n",
       "      <td>0.836032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.575800</td>\n",
       "      <td>0.541005</td>\n",
       "      <td>0.729198</td>\n",
       "      <td>0.748242</td>\n",
       "      <td>0.696943</td>\n",
       "      <td>0.807692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.550400</td>\n",
       "      <td>0.535516</td>\n",
       "      <td>0.726677</td>\n",
       "      <td>0.756951</td>\n",
       "      <td>0.679549</td>\n",
       "      <td>0.854251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.548000</td>\n",
       "      <td>0.518722</td>\n",
       "      <td>0.745840</td>\n",
       "      <td>0.756757</td>\n",
       "      <td>0.723247</td>\n",
       "      <td>0.793522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.542300</td>\n",
       "      <td>0.518570</td>\n",
       "      <td>0.739284</td>\n",
       "      <td>0.755786</td>\n",
       "      <td>0.708592</td>\n",
       "      <td>0.809717</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1983\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ../../result/baselines/ShortHumor_freezed/checkpoint-1182\n",
      "Configuration saved in ../../result/baselines/ShortHumor_freezed/checkpoint-1182/config.json\n",
      "Model weights saved in ../../result/baselines/ShortHumor_freezed/checkpoint-1182/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1983\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ../../result/baselines/ShortHumor_freezed/checkpoint-2364\n",
      "Configuration saved in ../../result/baselines/ShortHumor_freezed/checkpoint-2364/config.json\n",
      "Model weights saved in ../../result/baselines/ShortHumor_freezed/checkpoint-2364/pytorch_model.bin\n",
      "Deleting older checkpoint [../../result/baselines/ShortHumor_freezed/checkpoint-1182] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1983\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ../../result/baselines/ShortHumor_freezed/checkpoint-3546\n",
      "Configuration saved in ../../result/baselines/ShortHumor_freezed/checkpoint-3546/config.json\n",
      "Model weights saved in ../../result/baselines/ShortHumor_freezed/checkpoint-3546/pytorch_model.bin\n",
      "Deleting older checkpoint [../../result/baselines/ShortHumor_freezed/checkpoint-2364] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1983\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ../../result/baselines/ShortHumor_freezed/checkpoint-4728\n",
      "Configuration saved in ../../result/baselines/ShortHumor_freezed/checkpoint-4728/config.json\n",
      "Model weights saved in ../../result/baselines/ShortHumor_freezed/checkpoint-4728/pytorch_model.bin\n",
      "Deleting older checkpoint [../../result/baselines/ShortHumor_freezed/checkpoint-3546] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1983\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ../../result/baselines/ShortHumor_freezed/checkpoint-5910\n",
      "Configuration saved in ../../result/baselines/ShortHumor_freezed/checkpoint-5910/config.json\n",
      "Model weights saved in ../../result/baselines/ShortHumor_freezed/checkpoint-5910/pytorch_model.bin\n",
      "Deleting older checkpoint [../../result/baselines/ShortHumor_freezed/checkpoint-4728] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ../../result/baselines/ShortHumor_freezed/checkpoint-5910 (score: 0.5185704827308655).\n"
     ]
    }
   ],
   "source": [
    "task = list(tasks.keys())[16]\n",
    "train_baseline(task, freeze_bert=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b026919",
   "metadata": {},
   "source": [
    "### debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134f0fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "task = list(tasks.keys())[16]\n",
    "freeze_bert = True\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "model = None\n",
    "trainer = None \n",
    "num_labels = tasks[task]\n",
    "\n",
    "data_folder = '../../data/xslue'\n",
    "train_dataset = MyDataset(f'{data_folder}/processed/train/{task}.tsv')\n",
    "test_dataset = MyDataset(f'{data_folder}/processed/test/{task}.tsv')\n",
    "valid_dataset = MyDataset(f'{data_folder}/processed/dev/{task}.tsv')\n",
    "\n",
    "singletaskbert = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=num_labels) \n",
    "if freeze_bert:\n",
    "    for param in singletaskbert.bert.parameters():\n",
    "        param.requires_grad = False\n",
    "result_folder = '../../result'\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"{result_folder}/baselines/{task+'_freezed' if freeze_bert else task}\",   # output directory\n",
    "    num_train_epochs=5,              # total number of training epochs\n",
    "    per_device_train_batch_size=batch_size,  # batch size per device during training\n",
    "    per_device_eval_batch_size=batch_size,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir=f\"{result_folder}/baselines/{task+'_freezed' if freeze_bert else task}/logs\",  # directory for storing logs\n",
    "#         logging_first_step = True, \n",
    "#         logging_steps=500,               # log & save weights each logging_steps\n",
    "#         save_steps=500,\n",
    "    evaluation_strategy=\"epoch\",     # evaluate each `logging_steps`\n",
    "    save_total_limit = 1,\n",
    "    save_strategy = 'epoch',\n",
    "    load_best_model_at_end=True, # decide on loss\n",
    ")\n",
    "\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=singletaskbert,   # the instantiated Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=valid_dataset,          # evaluation dataset\n",
    "#         test_dataset=test_dataset,            # test dataset\n",
    "    compute_metrics=compute_metrics,     # the callback that computes metrics of interest\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68a5970",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "singletaskbert.bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fcfaa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-0.0013, -0.0381, -0.0158,  ...,  0.0244, -0.0008,  0.0240],\n",
       "         [ 0.0020,  0.0151,  0.0033,  ...,  0.0180, -0.0023,  0.0231],\n",
       "         [-0.0386,  0.0145,  0.0621,  ...,  0.0374, -0.0105, -0.0395],\n",
       "         ...,\n",
       "         [-0.0111,  0.0136,  0.0541,  ...,  0.0666,  0.0017, -0.0090],\n",
       "         [ 0.0001,  0.0024, -0.0125,  ...,  0.0046, -0.0014, -0.0079],\n",
       "         [ 0.0415,  0.0751,  0.0305,  ...,  0.0317,  0.0479,  0.0080]],\n",
       "        device='cuda:0'),\n",
       " Parameter containing:\n",
       " tensor([-3.5976e-02, -3.8954e-03,  5.1814e-02,  2.2247e-02, -4.9372e-03,\n",
       "         -1.1203e-03,  2.6339e-02,  9.0639e-03,  3.8723e-02, -9.6557e-02,\n",
       "          2.3311e-02, -2.0086e-02,  6.5490e-02, -5.1680e-02,  6.6156e-02,\n",
       "         -1.4721e-02, -4.5325e-03, -8.4059e-04,  1.3027e-02, -2.9770e-02,\n",
       "          4.3693e-02, -1.4292e-02,  4.5489e-02,  4.5346e-03,  7.6859e-03,\n",
       "         -4.0926e-02, -1.1412e-02,  6.6037e-02,  6.7376e-02,  4.2859e-02,\n",
       "         -2.6975e-02,  3.5281e-05, -8.1483e-02, -3.7569e-03,  4.0570e-02,\n",
       "         -5.7694e-02, -8.3285e-03, -2.6218e-02, -4.7924e-03, -8.2249e-03,\n",
       "         -6.4672e-02,  1.9579e-03,  7.2015e-02, -2.2552e-02,  3.4110e-03,\n",
       "         -1.4251e-02, -8.3488e-03,  3.8957e-03, -5.5111e-02, -4.8916e-02,\n",
       "         -4.0505e-02, -3.5423e-02,  6.1806e-03,  9.7887e-04,  2.5234e-03,\n",
       "          3.8691e-03, -7.2486e-03, -2.1082e-03,  1.9743e-03, -1.5894e-02,\n",
       "         -1.4470e-02,  3.2000e-03,  2.3056e-02, -4.6876e-02, -3.6722e-02,\n",
       "         -7.3898e-02, -4.4587e-03,  5.0023e-03,  6.9043e-03,  3.3589e-03,\n",
       "          2.2775e-02, -3.1106e-03,  2.4767e-02, -4.8293e-02, -5.0632e-02,\n",
       "          3.7559e-03, -8.8531e-03,  1.7421e-02, -5.2642e-03, -7.4719e-02,\n",
       "         -3.0047e-02, -5.3653e-02,  8.8173e-03,  6.1905e-02, -6.8883e-02,\n",
       "         -2.1225e-02, -8.6340e-03, -1.7548e-03, -8.5851e-02,  5.9378e-03,\n",
       "          1.1967e-02,  2.0897e-03, -4.2413e-02,  8.7467e-03,  4.5203e-02,\n",
       "         -7.6098e-03,  1.8549e-04,  2.8614e-02,  2.0781e-03, -4.9508e-03,\n",
       "         -8.8196e-03, -3.4198e-03,  7.3986e-03, -1.7159e-03,  3.8593e-03,\n",
       "         -9.6255e-03, -1.3085e-02, -1.6207e-02, -4.1687e-02,  9.7658e-03,\n",
       "          8.9834e-03, -5.8610e-03, -1.4935e-03, -5.3537e-02,  5.5583e-03,\n",
       "         -1.0870e-04, -8.4393e-02, -2.0534e-02, -8.6620e-02,  3.6078e-02,\n",
       "          5.2120e-03, -1.4668e-02,  6.6004e-02,  3.5172e-02,  4.0267e-03,\n",
       "         -3.8512e-03,  4.1188e-02, -9.5374e-03,  8.4221e-03,  4.1511e-03,\n",
       "          3.0034e-02, -6.3468e-04, -7.1196e-02, -7.6416e-02,  1.1497e-02,\n",
       "          6.4726e-02,  2.7181e-03,  6.8666e-02, -4.0454e-03,  5.6817e-02,\n",
       "          4.0652e-02,  1.6854e-02, -3.1005e-02, -1.3064e-02,  2.8483e-03,\n",
       "          1.6498e-02, -3.0347e-02,  8.9853e-03, -7.7755e-03, -1.8113e-02,\n",
       "         -2.2885e-03, -5.0442e-03,  4.7131e-02, -6.0013e-02, -8.7461e-03,\n",
       "          5.9925e-02,  2.1879e-02,  6.8031e-02,  6.3506e-02, -4.2697e-03,\n",
       "         -1.4504e-02,  4.1974e-02,  9.1194e-03,  3.0824e-03,  2.6392e-02,\n",
       "          1.1135e-04, -6.1943e-02,  5.2578e-03, -3.5484e-02,  1.7566e-02,\n",
       "          3.5716e-03, -4.5841e-03,  4.8374e-02, -7.5280e-02, -4.6712e-03,\n",
       "          1.6970e-02,  8.2794e-02,  4.9445e-02,  4.4020e-03, -2.7126e-02,\n",
       "         -1.0707e-02, -2.4882e-02, -6.9837e-02,  7.4775e-02,  6.0947e-03,\n",
       "          1.8029e-03,  6.6490e-03, -1.5068e-02, -4.7108e-02, -4.8279e-02,\n",
       "          2.2051e-02,  1.0339e-02, -4.7205e-02, -4.5817e-03, -9.0287e-03,\n",
       "         -2.7196e-03,  3.0730e-03,  3.3383e-02,  5.3510e-03, -1.1000e-02,\n",
       "         -4.3553e-03,  6.9025e-02,  2.8795e-02,  3.3619e-02, -1.4701e-02,\n",
       "          1.4060e-02, -4.8320e-02, -3.1883e-02, -1.4480e-03,  6.0081e-03,\n",
       "          2.3593e-03,  8.4787e-02,  1.2406e-02,  2.5789e-03, -5.0564e-02,\n",
       "         -8.2032e-02, -9.4141e-04, -5.5241e-02, -7.6775e-03, -2.4861e-02,\n",
       "          9.4571e-03,  5.8841e-03, -4.7154e-02,  3.5717e-03, -4.3507e-02,\n",
       "         -4.6676e-02, -1.2599e-03, -8.0774e-03,  2.4329e-03, -7.5354e-03,\n",
       "          1.5403e-02, -3.6249e-02, -2.7785e-02,  2.7119e-02,  6.7980e-02,\n",
       "          3.1430e-02, -5.2584e-02,  3.7485e-02, -4.8816e-03,  3.0093e-02,\n",
       "         -1.1456e-02,  5.6422e-02, -1.2345e-02,  6.0983e-03, -6.2267e-02,\n",
       "          4.3605e-02, -3.8197e-02,  5.2499e-02,  1.6630e-04, -7.5969e-02,\n",
       "         -6.4865e-02,  1.3884e-02, -1.3189e-03,  6.7656e-02, -9.5227e-03,\n",
       "          3.3197e-02, -2.3533e-02, -7.0145e-02, -1.0432e-02,  2.8274e-02,\n",
       "         -8.1062e-02, -3.1947e-02, -2.7126e-03, -1.3334e-02, -2.8135e-03,\n",
       "         -1.0706e-02, -6.9299e-02,  3.4644e-02, -2.9170e-03,  4.5919e-02,\n",
       "         -8.9445e-03, -2.6880e-02,  1.2639e-02, -6.7561e-02, -7.4783e-04,\n",
       "          6.1452e-03,  5.0698e-02, -8.5238e-03, -5.8640e-02,  1.7157e-03,\n",
       "          2.5632e-02,  1.0620e-02,  5.4907e-02,  3.9250e-02,  1.3627e-02,\n",
       "          7.0662e-02,  5.1439e-02,  3.4894e-02,  1.0701e-02, -1.6566e-02,\n",
       "          1.0428e-01,  3.2651e-02, -1.1483e-02, -6.2328e-02, -1.2810e-02,\n",
       "          3.5336e-02, -1.5002e-02, -1.1252e-02,  1.3703e-04, -5.9601e-02,\n",
       "         -4.1940e-02,  6.4488e-02,  3.5795e-02, -1.1073e-02,  4.2960e-02,\n",
       "          6.6363e-02, -2.1350e-02, -2.8048e-02, -7.8367e-03,  5.9328e-02,\n",
       "         -1.1968e-02,  1.9762e-02,  1.6436e-03, -1.9586e-04,  4.5457e-02,\n",
       "         -3.3673e-02,  2.8175e-02,  3.0951e-02, -8.4503e-03,  4.3550e-03,\n",
       "         -1.8311e-02, -6.0869e-02,  7.6723e-03, -8.1489e-03, -2.4181e-02,\n",
       "         -6.5763e-02, -3.1222e-03, -2.7059e-02,  5.2479e-03,  2.2024e-03,\n",
       "          2.2695e-03, -3.7777e-02,  2.9974e-05, -5.2179e-02,  8.6420e-03,\n",
       "          1.5367e-02, -2.6896e-02, -2.3252e-02,  1.3346e-02, -2.8418e-02,\n",
       "          5.2154e-02, -6.8118e-02,  6.3350e-02, -2.9627e-03, -3.7981e-02,\n",
       "          2.4390e-02, -2.4529e-03, -5.2262e-02,  4.4196e-03,  4.0819e-03,\n",
       "         -5.2573e-04,  1.5752e-02, -4.7064e-03, -7.4060e-02, -1.7793e-02,\n",
       "         -1.8469e-02,  4.7606e-03, -4.6088e-03,  7.6716e-02, -1.5433e-04,\n",
       "          3.2812e-02,  9.1863e-03,  7.8530e-02, -7.5982e-02, -3.3673e-02,\n",
       "         -4.5526e-02, -7.1196e-02,  6.6536e-02,  6.6131e-02,  3.4918e-03,\n",
       "         -4.2715e-02, -7.3834e-04,  5.9887e-02, -9.4803e-03, -5.1243e-02,\n",
       "          4.2864e-03,  2.6319e-02, -9.7941e-03,  5.9664e-02, -2.4174e-02,\n",
       "         -1.1204e-02, -7.2384e-03,  4.9288e-03,  1.6533e-02, -2.4369e-02,\n",
       "          6.4933e-03, -1.3649e-02, -2.4182e-03, -6.2799e-03, -1.7210e-02,\n",
       "         -7.2431e-02, -7.8740e-03,  1.2792e-02,  2.0920e-02, -1.2329e-02,\n",
       "          3.8972e-02, -7.1581e-03, -1.2284e-02,  1.9917e-03,  7.4051e-03,\n",
       "         -1.4321e-02, -3.5394e-02, -2.4015e-02, -3.2418e-02, -7.5521e-02,\n",
       "          3.2948e-02,  1.5901e-03, -2.8555e-03,  2.7893e-03, -3.4312e-03,\n",
       "          2.5345e-03, -1.6758e-02, -4.9462e-02,  3.2866e-03,  1.2931e-02,\n",
       "         -5.2172e-02,  7.3928e-02, -4.0985e-03,  9.5864e-03,  3.0872e-02,\n",
       "          5.2427e-02, -1.4434e-02, -2.9415e-03, -4.2015e-03, -6.0903e-02,\n",
       "          5.1717e-03, -6.3032e-02,  6.9126e-02, -3.7929e-02,  7.9866e-03,\n",
       "          6.0435e-03, -8.5405e-03,  1.0294e-02, -9.4331e-03,  2.0938e-02,\n",
       "         -8.2480e-03,  3.0896e-02,  1.7897e-02, -2.8527e-02, -1.5033e-02,\n",
       "          3.5082e-03,  5.9243e-02, -3.4988e-03, -1.2596e-03, -6.2648e-02,\n",
       "         -5.5414e-02, -1.9697e-02, -4.5624e-02, -7.3769e-02,  3.9488e-02,\n",
       "          2.1077e-02,  7.9621e-03,  8.1903e-03, -6.0914e-03, -1.0476e-02,\n",
       "         -2.5105e-02, -4.3828e-03, -6.4321e-02,  4.9943e-02, -1.9390e-02,\n",
       "          1.1147e-02, -4.4764e-03,  6.2568e-03, -6.4056e-02,  7.2374e-02,\n",
       "          1.0556e-02,  1.0156e-03,  7.9476e-03, -2.9068e-02,  1.5874e-02,\n",
       "         -2.6810e-02,  2.9566e-02, -5.4999e-03,  1.3515e-02, -1.0681e-02,\n",
       "         -5.0631e-02,  4.0818e-02,  2.0823e-02,  3.9909e-03,  7.8542e-03,\n",
       "         -4.6300e-02,  2.4748e-03,  6.9978e-02,  4.5245e-02, -1.6835e-02,\n",
       "         -1.0396e-02,  1.5637e-02, -5.6454e-03, -7.2900e-02,  5.3020e-02,\n",
       "          1.3894e-02, -1.8712e-05, -3.6843e-04,  5.5773e-03,  4.1098e-02,\n",
       "         -8.0191e-04, -2.4093e-03, -2.5919e-02,  1.2519e-03, -2.3039e-03,\n",
       "         -8.6203e-03,  1.2129e-02,  1.0854e-04, -1.5088e-03, -7.4610e-02,\n",
       "          2.2519e-02, -3.8466e-02,  1.0974e-02,  6.0302e-02, -5.1620e-02,\n",
       "          1.3340e-02, -8.0559e-03,  4.1242e-04,  1.9552e-02, -7.9269e-03,\n",
       "         -8.1603e-03,  1.7111e-03,  6.7728e-03,  6.4310e-02, -1.3457e-02,\n",
       "         -7.4335e-02, -3.4480e-02, -7.3911e-04, -6.2072e-02, -2.2311e-02,\n",
       "         -1.7109e-03,  5.9918e-03, -1.9664e-03, -7.1251e-03,  2.2825e-02,\n",
       "         -4.8720e-03, -6.5003e-02, -2.1775e-03, -1.8444e-03,  7.3265e-02,\n",
       "          5.2221e-04, -1.2954e-02, -5.3259e-02, -3.7084e-02, -3.8334e-03,\n",
       "          4.3280e-02, -6.0462e-02,  7.0455e-02, -6.5359e-02,  1.0842e-03,\n",
       "          1.0533e-02,  6.1542e-03, -3.7020e-02,  1.8747e-04, -2.1195e-02,\n",
       "         -4.4640e-04,  1.5528e-03,  1.6410e-02, -6.1031e-02, -2.0254e-03,\n",
       "         -6.1408e-03,  5.8679e-03, -2.9146e-03, -1.9892e-03,  4.3533e-02,\n",
       "          1.5581e-03, -1.3258e-02, -3.4477e-03, -3.1063e-03,  1.6558e-03,\n",
       "          1.2330e-02, -1.3658e-02,  1.4830e-05,  2.3483e-03, -2.5652e-03,\n",
       "         -4.6861e-02, -3.8526e-03,  1.4891e-04,  3.5026e-03,  2.9304e-02,\n",
       "         -1.2515e-02, -1.6651e-02, -6.5505e-02, -7.0729e-03,  5.6096e-02,\n",
       "          1.6492e-02, -1.0102e-02, -5.1001e-02,  4.6134e-02,  5.9714e-02,\n",
       "          2.7749e-02, -3.9069e-03,  2.8098e-02, -4.3307e-02,  2.9125e-03,\n",
       "         -1.1555e-03, -5.4332e-04,  1.7716e-02,  3.9426e-02, -1.1650e-02,\n",
       "          5.4143e-03,  7.6668e-04, -1.4501e-02, -3.2781e-02, -1.7109e-04,\n",
       "         -4.5489e-03,  1.3235e-02, -3.1242e-02, -6.5027e-02,  4.4712e-03,\n",
       "         -8.8002e-03, -4.7278e-02,  1.5585e-02, -1.2312e-03, -7.9780e-03,\n",
       "          3.0320e-02,  5.8618e-02,  2.9539e-02, -1.0512e-02,  1.3233e-02,\n",
       "         -9.1499e-03, -1.4123e-02,  4.5389e-03, -4.5021e-02,  8.7648e-02,\n",
       "          1.5873e-02,  3.4661e-02,  2.9271e-03,  8.5943e-04,  6.3336e-02,\n",
       "          3.6434e-03,  2.7110e-02,  3.9728e-03,  1.6938e-02,  7.7805e-03,\n",
       "         -5.5892e-02,  1.3069e-02, -6.5158e-02, -1.4633e-03, -5.5059e-02,\n",
       "          7.2858e-03,  5.3833e-03,  6.2287e-02, -2.0552e-03,  5.5169e-02,\n",
       "          4.2386e-02, -6.4451e-03,  1.9792e-02,  5.1887e-02,  1.0740e-03,\n",
       "         -6.3414e-02, -7.8450e-02, -8.0298e-02,  6.7308e-03, -4.0420e-03,\n",
       "         -7.0452e-04,  1.4065e-02,  6.0994e-03,  4.5103e-03,  1.0812e-02,\n",
       "         -2.0521e-02,  6.0328e-02,  5.4586e-03, -6.6997e-02,  7.8335e-02,\n",
       "         -1.5540e-02,  5.5494e-03,  1.2440e-02, -7.8808e-02, -4.0115e-02,\n",
       "          3.3492e-04,  1.2926e-03,  2.9311e-02,  1.7346e-02,  5.1262e-02,\n",
       "         -1.8666e-03, -8.3860e-03, -1.1787e-02,  2.8252e-02, -1.7593e-02,\n",
       "         -8.7817e-02,  5.2170e-03,  4.4334e-02, -3.3100e-02,  7.3799e-02,\n",
       "         -4.1801e-02, -4.6398e-03,  5.7726e-02,  2.2281e-02,  3.2628e-02,\n",
       "          3.7614e-02,  2.2902e-02, -5.2283e-04,  2.9742e-02,  5.6447e-02,\n",
       "          4.1772e-02,  7.6728e-02,  4.9035e-02,  2.5668e-02,  2.9138e-02,\n",
       "          4.4800e-03,  1.5523e-02, -6.5518e-02, -1.5646e-03,  7.2878e-03,\n",
       "          3.4111e-02, -9.0368e-04, -3.1689e-04, -3.5873e-02,  2.4688e-02,\n",
       "         -5.3862e-04,  1.4132e-02, -1.1777e-02,  7.0576e-03, -1.1229e-02,\n",
       "         -1.0256e-02, -3.0035e-02, -3.0638e-03,  1.3067e-02,  1.0861e-02,\n",
       "          5.8566e-02, -7.6218e-03, -5.1887e-03, -4.6288e-03,  5.5355e-03,\n",
       "          7.2679e-02, -5.8685e-02,  1.8479e-02, -5.3238e-03,  6.6855e-02,\n",
       "         -1.4153e-02, -1.0367e-02,  1.9953e-02, -4.0999e-02, -2.1670e-03,\n",
       "         -4.4959e-03, -3.5326e-02,  3.8980e-02, -1.1234e-02, -3.1199e-03,\n",
       "         -1.5884e-02,  2.0611e-02,  2.4461e-03, -2.1509e-03,  5.7122e-02,\n",
       "          5.6231e-02, -6.3469e-03,  3.8378e-03,  7.6445e-03, -3.4256e-03,\n",
       "         -1.8695e-02, -4.3274e-04,  2.8806e-02, -4.9924e-02,  3.1271e-02,\n",
       "         -4.2970e-02,  2.0461e-02, -5.6234e-02,  2.5810e-05, -1.4020e-03,\n",
       "         -1.7969e-02, -1.1418e-02,  8.4572e-03,  5.4102e-03,  2.1549e-02,\n",
       "         -3.1038e-02,  4.6863e-02, -1.0834e-02,  5.6062e-02,  1.6208e-02,\n",
       "          1.5080e-02, -1.9071e-02,  5.0027e-02], device='cuda:0')]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(singletaskbert.bert.pooler.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8e0e71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-0.0414, -0.0009, -0.0389,  ..., -0.0488,  0.0483, -0.0530],\n",
       "         [-0.0649, -0.0347, -0.0141,  ...,  0.0332, -0.0758,  0.0059],\n",
       "         [-0.0320,  0.0619, -0.0941,  ...,  0.0221, -0.0589, -0.0778],\n",
       "         ...,\n",
       "         [ 0.0453,  0.0145, -0.0392,  ..., -0.0525, -0.0572,  0.0079],\n",
       "         [ 0.0710, -0.0197,  0.0472,  ...,  0.0008, -0.0789, -0.0107],\n",
       "         [-0.0549,  0.0375,  0.0308,  ...,  0.0273,  0.0536, -0.0444]],\n",
       "        device='cuda:0'),\n",
       " Parameter containing:\n",
       " tensor([ 5.0130e-02,  1.1887e-01,  2.1868e-01,  8.7210e-02, -2.0477e-01,\n",
       "          2.2648e-01, -3.4360e-01,  6.4057e-02, -4.1554e-01,  1.1557e-01,\n",
       "          2.8522e-01, -1.0724e-01,  2.1037e-01,  6.5960e-02, -8.9728e-02,\n",
       "         -6.1263e-02,  1.1344e-01, -1.9265e-01, -1.2109e-02, -2.4905e-01,\n",
       "          7.1558e-02, -2.0746e-01, -1.8167e-01, -1.9616e-02,  5.7593e-02,\n",
       "         -4.0425e-02,  3.1487e-01, -1.6641e-01,  3.6615e-01,  2.8458e-02,\n",
       "         -8.9834e-02, -4.3720e-02, -1.2218e-01,  1.1063e-01,  4.6565e-02,\n",
       "          5.6489e-02,  2.5875e-01,  1.0859e-01, -2.2724e-01, -1.5846e-01,\n",
       "         -2.0762e-01, -2.8562e-01, -1.6824e-01,  1.7382e-01,  2.8304e-02,\n",
       "         -8.5946e-02, -1.2882e-01, -3.9624e-05,  1.3995e-01, -1.4809e-01,\n",
       "         -1.4983e-01,  8.0181e-02, -2.4638e-01,  1.3984e-01, -4.8627e-01,\n",
       "         -3.8021e-02,  7.8774e-02, -5.4215e-01,  1.8015e-01, -2.4764e-01,\n",
       "         -8.2056e-02,  2.0403e-01, -1.2871e-01,  1.7048e-01,  3.3818e-01,\n",
       "         -5.3207e-01,  2.6975e-01,  1.5490e-01,  2.6109e-01, -6.6078e-02,\n",
       "         -1.0768e-01,  6.7982e-02,  5.1441e-03, -1.7760e-01, -8.6212e-03,\n",
       "         -1.2084e-01,  2.7072e-02,  9.5277e-02, -1.1946e-01, -1.7435e-01,\n",
       "          1.2826e-02, -1.4399e-01,  5.4034e-02, -7.1218e-02, -1.0277e-01,\n",
       "         -8.0027e-02,  1.2395e-01,  1.2472e-01,  2.8782e-01, -2.4318e-01,\n",
       "          6.9833e-02, -1.5565e-01,  5.3963e-02,  2.2090e-01, -6.5430e-02,\n",
       "         -2.9557e-01,  1.9449e-01, -2.0094e-02, -3.5260e-02, -1.2373e-01,\n",
       "          1.1984e-01,  1.1227e-01,  4.9598e-02,  2.9100e-01, -1.6697e-01,\n",
       "          9.0494e-02, -5.6501e-02,  1.8282e-01,  1.1605e-01, -1.1072e-01,\n",
       "         -1.3768e-01, -1.3056e-01,  5.1009e-03,  3.5995e-02, -2.2659e-01,\n",
       "         -1.7014e-01, -3.6584e-02,  1.4101e-01,  1.6270e-01, -1.3676e-01,\n",
       "          4.6082e-01, -1.5044e-02,  2.6074e-01,  2.4027e-01, -3.2432e-01,\n",
       "         -1.0082e-01, -6.3582e-03, -1.2997e-01,  1.7895e-01, -5.6416e-03,\n",
       "         -3.2919e-02,  9.1986e-03,  3.9944e-02, -4.8234e-03, -1.0553e-02,\n",
       "          4.5920e-02, -5.7454e-02, -5.0634e-03,  3.1936e-02,  2.5940e-02,\n",
       "          2.4509e-02,  5.9364e-03, -7.3236e-02,  4.9799e-02, -6.5353e-02,\n",
       "         -3.0103e-02, -5.1781e-02,  1.8562e-02, -1.2511e-02, -2.4903e-02,\n",
       "         -1.9358e-02, -2.2507e-02, -5.2231e-02,  8.5270e-02,  5.6999e-03,\n",
       "          5.2514e-02, -7.0874e-02,  3.2521e-02,  5.5978e-02,  3.1149e-02,\n",
       "         -3.7748e-02, -1.6950e-02, -1.9682e-02, -7.0004e-02,  6.0100e-02,\n",
       "         -3.0003e-02, -1.9542e-02, -2.6614e-02, -5.4306e-02, -3.2268e-02,\n",
       "         -2.3941e-02,  4.8159e-02, -6.7734e-02,  9.6824e-03,  6.2304e-02,\n",
       "          1.5483e-02,  4.7038e-02, -4.1295e-02, -2.6080e-02,  7.5054e-02,\n",
       "         -9.9345e-03,  6.7070e-02, -5.5948e-03, -4.9686e-02,  6.9155e-02,\n",
       "         -1.2513e-02, -9.6080e-03,  1.4420e-02,  8.9716e-02, -3.2649e-02,\n",
       "          1.3103e-02, -5.8897e-02, -2.8231e-01, -2.1987e-01,  7.4569e-02,\n",
       "         -1.9329e-01, -2.3904e-01, -2.0547e-01,  1.4832e-01, -2.0592e-01,\n",
       "         -1.0556e-01, -3.3246e-01,  2.9869e-01,  1.4742e-02, -3.4112e-02,\n",
       "         -5.1247e-02,  1.7858e-01, -1.5464e-01,  4.4350e-01, -3.4233e-01,\n",
       "          6.0810e-02, -4.8418e-01, -1.7234e-01,  5.5829e-01, -1.9054e-01,\n",
       "         -1.7085e-01, -6.5740e-02,  4.6172e-01,  1.1685e-01, -1.0867e-01,\n",
       "          2.5521e-02,  9.3015e-02, -3.3565e-01,  9.7723e-02,  1.1447e-01,\n",
       "          2.5188e-01,  2.4162e-01, -3.2859e-02,  1.1418e-01, -1.3454e-01,\n",
       "          1.1214e-01,  1.9344e-01,  2.8808e-01,  3.6662e-02, -9.1874e-02,\n",
       "         -1.8925e-02,  8.3655e-02, -2.9657e-01,  4.3850e-01, -3.1043e-01,\n",
       "          2.6670e-01, -2.4112e-01,  1.0252e-01, -1.7780e-04, -7.8823e-02,\n",
       "         -2.7052e-01, -8.6674e-03,  1.0519e-02,  2.3936e-01, -2.9538e-01,\n",
       "          8.4221e-02,  3.3672e-02,  1.9706e-01, -6.2164e-02, -2.4718e-02,\n",
       "          8.7717e-02, -2.5071e-01,  1.5706e-01,  2.3768e-01,  3.9946e-01,\n",
       "          1.1246e-01, -1.7711e-01, -3.4676e-01, -4.5355e-01,  7.5780e-02,\n",
       "         -7.4163e-02, -5.9656e-03,  1.8497e-01,  4.2538e-01,  2.2415e-02,\n",
       "         -1.9526e-01, -8.4955e-02, -2.0987e-01, -3.9819e-01, -1.6228e-01,\n",
       "         -1.0210e-01,  1.6659e-01,  1.4552e-01, -2.1444e-01, -1.1108e-01,\n",
       "          7.1328e-02, -1.7965e-01,  5.0596e-03, -1.4955e-01, -2.5073e-01,\n",
       "         -2.3035e-01,  1.9926e-01, -1.9052e-01, -5.7047e-02, -2.4461e-02,\n",
       "         -4.0321e-01,  1.3474e-01, -4.3975e-03, -2.7324e-01, -1.3796e-01,\n",
       "          6.3967e-02,  2.7403e-01,  1.9344e-01, -1.2892e-01, -1.6142e-02,\n",
       "          3.8657e-01,  1.5079e-01,  9.9633e-02,  6.6772e-01,  1.5956e-01,\n",
       "          1.2797e-01,  1.6047e-01,  1.4334e-01, -7.4570e-02, -3.9579e-01,\n",
       "          1.7510e-02, -3.4705e-02,  1.4462e-01,  1.2939e-01, -1.1103e-01,\n",
       "          4.3722e-02, -1.4519e-01,  6.8075e-02,  3.0203e-02, -4.9669e-02,\n",
       "          3.4470e-01,  1.4825e-02, -1.2054e-01, -3.6352e-01,  1.1949e-02,\n",
       "         -1.1544e-01,  5.5563e-01,  6.3900e-01, -9.1271e-02,  5.7076e-02,\n",
       "          1.8457e-02, -1.9434e-01,  5.8411e-01,  1.5039e-01, -2.1969e-01,\n",
       "          8.3831e-02,  3.5248e-01, -2.1035e-01,  1.2112e-01, -1.5372e-01,\n",
       "          7.9632e-02, -2.5212e-01, -4.9995e-02,  4.1693e-01,  1.3844e-01,\n",
       "          8.8435e-02,  7.5546e-01,  2.8799e-01,  7.1233e-03,  1.6018e-01,\n",
       "         -4.3708e-01,  4.0694e-02, -2.8835e-01, -1.8012e-01,  7.6160e-01,\n",
       "          6.1730e-02,  2.0175e-01,  2.5984e-01, -1.8264e-01, -1.3711e-01,\n",
       "          1.8038e-01,  9.9823e-02,  2.9867e-03,  3.7477e-01, -7.3096e-02,\n",
       "         -5.5421e-02,  7.9605e-01, -3.7314e-01,  4.5567e-01,  3.8785e-01,\n",
       "         -4.6460e-01, -3.1176e-01, -9.7430e-02,  1.1275e-01,  1.7523e-01,\n",
       "         -1.7209e-01,  2.6395e-01,  5.0246e-01, -2.1430e-01, -1.3977e-01,\n",
       "          1.2911e-01,  2.6221e-01,  1.2876e-01,  1.8243e-02,  1.9331e-02,\n",
       "          2.0036e-01, -3.7061e-03, -5.2489e-02, -3.8090e-02,  1.7509e-01,\n",
       "          1.4210e-01,  2.6394e-01, -2.4669e-01, -2.0842e-01,  7.3437e-02,\n",
       "          8.0304e-02, -8.8610e-02,  9.5805e-02, -2.8193e-03,  9.6245e-02,\n",
       "         -5.9579e-02,  1.8303e-02, -5.2196e-02,  2.4602e-03, -8.9981e-03,\n",
       "         -1.4188e-01,  1.6982e-01,  1.4684e-02,  8.0432e-02,  1.4415e-01,\n",
       "         -7.9289e-02, -4.3563e-02, -1.3048e-01,  9.0140e-02, -1.1332e-01,\n",
       "          6.5817e-02, -2.8919e-02, -5.0692e-02, -6.8233e-02,  1.2676e-01,\n",
       "         -7.7853e-03, -1.1944e-01, -7.1633e-02,  3.8406e-02,  5.1482e-02,\n",
       "         -2.3815e-01, -7.2538e-02,  5.0957e-01, -7.4238e-02, -1.2476e-01,\n",
       "         -7.3363e-02,  2.0673e-01, -3.3949e-02, -1.0945e-04, -1.1571e-02,\n",
       "         -2.1345e-01,  2.1062e-01,  4.1730e-02, -5.9223e-03,  3.1824e-01,\n",
       "         -1.7664e-01, -2.7752e-02, -1.7509e-02, -2.5941e-01, -5.6525e-02,\n",
       "          2.2949e-01,  5.6018e-02,  5.1439e-02,  4.4295e-01, -6.6992e-02,\n",
       "          1.8303e-02,  1.6685e-01,  1.9616e-01,  8.3671e-02,  5.5415e-01,\n",
       "         -1.4475e-01, -3.9549e-01, -1.6309e-02,  3.3830e-01,  1.1835e-01,\n",
       "         -3.6023e-01, -4.5967e-01, -1.0929e-01,  1.9510e-01, -8.4783e-02,\n",
       "          1.7358e-01, -1.9851e-01,  3.5490e-02,  9.7441e-02, -3.1306e-01,\n",
       "          2.6871e-01,  1.3092e-01, -7.5986e-02, -5.5100e-01, -1.9846e-01,\n",
       "         -1.8693e-01,  2.4242e-01, -3.1841e-01,  3.2628e-02, -2.6412e-01,\n",
       "          6.1704e-02,  1.4636e-01,  2.6656e-01,  2.6279e-01,  1.6979e-01,\n",
       "          3.0427e-01,  3.4861e-02,  1.0094e-01,  3.0639e-01, -1.0081e-01,\n",
       "          3.4950e-01,  7.8413e-01,  5.2060e-01,  2.1603e-03, -7.0045e-03,\n",
       "         -4.1164e-01, -4.8645e-01, -4.4651e-01,  3.3513e-01, -1.1313e-01,\n",
       "         -3.9717e-01,  3.8403e-01, -2.3518e-01, -6.8511e-02, -1.0130e-01,\n",
       "         -1.5456e-01, -3.1723e-01,  1.4869e-01, -4.2325e-02, -5.3928e-01,\n",
       "         -1.3634e-02,  4.4899e-01,  2.1111e-01,  8.5819e-02, -4.3049e-01,\n",
       "         -8.3070e-01, -2.2593e-01, -2.0729e-01,  2.3948e-01, -1.5970e-01,\n",
       "         -3.6727e-01,  8.4436e-02,  8.1711e-03, -5.3988e-02,  1.2152e-01,\n",
       "          5.3374e-01,  1.3877e-01, -1.0957e-01, -7.8825e-02, -2.0775e-01,\n",
       "         -1.7110e-01,  9.0019e-02, -5.4240e-02,  1.1426e-02,  5.0588e-01,\n",
       "          9.0979e-02,  2.6900e-01, -2.9941e-01, -4.1123e-01,  1.7487e-01,\n",
       "         -4.8576e-02,  2.7232e-01, -4.5879e-01,  1.1320e-01,  7.7917e-02,\n",
       "         -1.9511e-01,  9.0838e-02, -2.7221e-02,  3.0969e-01,  1.3033e-01,\n",
       "         -4.9067e-01,  1.0965e-01, -3.1539e-01, -2.0408e-02, -8.4141e-02,\n",
       "          4.9824e-01,  2.5152e-01,  1.5938e-01,  1.8462e-01,  3.6651e-01,\n",
       "         -9.1929e-02, -5.0355e-01, -5.0853e-01,  2.4071e-01, -1.8947e-01,\n",
       "         -1.9674e-01,  2.3726e-01,  2.3691e-01,  3.6136e-01, -1.4146e-01,\n",
       "         -2.5364e-01, -1.4089e-01,  5.2075e-01, -2.7502e-01,  5.7724e-02,\n",
       "          1.7701e-01, -2.4592e-02,  6.1815e-02, -1.7824e-02,  7.1862e-02,\n",
       "          1.9092e-01, -1.4360e-01,  6.1019e-02,  2.2350e-01, -7.7964e-02,\n",
       "         -4.3328e-02,  1.7803e-01,  7.6450e-02,  1.6623e-01, -6.7891e-02,\n",
       "         -2.3149e-01,  1.7047e-01, -2.0200e-02, -3.5068e-02,  8.5247e-03,\n",
       "         -9.5607e-02, -2.3400e-01, -1.5214e-01, -4.5329e-02,  2.6065e-01,\n",
       "          1.1526e-01,  1.8007e-01, -3.8886e-02, -1.6848e-01,  2.4758e-01,\n",
       "          5.4496e-02, -5.3286e-02, -8.9750e-02, -1.1491e-01, -1.1738e-01,\n",
       "         -1.7055e-01, -4.1841e-02,  1.2854e-01,  8.8451e-02,  1.2169e-01,\n",
       "         -1.9706e-01, -3.5176e-02,  4.0673e-02, -1.5466e-01,  3.3641e-01,\n",
       "          1.1405e-01,  2.8445e-02, -1.4156e-01, -1.5457e-01,  9.3087e-02,\n",
       "          3.6545e-02,  4.2470e-02, -8.3606e-02,  1.0506e-01,  8.1112e-02,\n",
       "         -1.2319e-01, -7.9738e-02,  3.0404e-02,  2.2610e-01, -2.3057e-01,\n",
       "          1.2007e-01,  6.6130e-02, -1.0911e-01,  2.1609e-01,  7.3638e-02,\n",
       "          1.5479e-01,  3.4500e-01, -2.7938e-01,  5.7017e-01, -1.1569e-01,\n",
       "          2.4812e-01, -6.9649e-02, -2.7978e-01, -1.2075e-01,  1.1278e-01,\n",
       "         -1.6568e-01, -4.5727e-02,  2.3313e-02,  3.2640e-01, -2.2461e-01,\n",
       "         -8.2861e-02, -5.8357e-01, -3.9771e-02,  2.9434e-01, -2.3609e-01,\n",
       "         -5.3624e-01,  1.0189e-01, -1.8835e-01,  2.4618e-01, -1.0786e-01,\n",
       "         -7.9863e-02, -7.3921e-02,  3.8703e-02, -2.5798e-01,  4.7645e-03,\n",
       "         -2.3076e-01,  9.6446e-02,  1.0595e-01,  2.1178e-01,  3.2419e-01,\n",
       "         -1.3776e-01, -1.5145e-01,  3.0578e-01,  4.3816e-02,  8.1696e-02,\n",
       "          3.4741e-03,  3.6626e-01,  1.7579e-01,  1.0635e-01,  1.5077e-01,\n",
       "         -3.0174e-01, -1.8396e-01, -1.2838e-01,  1.3289e-01, -1.0084e-02,\n",
       "          6.4295e-02, -3.6652e-03, -1.7156e-01,  2.1100e-01,  4.2541e-01,\n",
       "         -2.0022e-01,  9.4931e-02,  1.8836e-01, -2.3158e-01,  3.2177e-01,\n",
       "         -1.6557e-02, -1.4628e-02,  2.9085e-02, -1.9244e-01,  4.9188e-02,\n",
       "          1.0409e-01,  4.2626e-01,  2.3523e-01,  4.4313e-01, -3.6384e-02,\n",
       "          1.1373e-01, -9.6126e-03,  2.3600e-01, -1.0489e-01, -1.2586e-01,\n",
       "          3.0308e-01,  6.5636e-01,  1.8376e-01, -1.0827e-02, -1.0237e-01,\n",
       "         -2.8121e-01,  4.9903e-01, -9.4880e-02, -1.5334e-01, -5.1055e-02,\n",
       "          8.2555e-02, -4.1501e-01,  1.9477e-01,  5.8070e-02,  7.0947e-02,\n",
       "          1.0771e-02, -4.8819e-02,  3.6631e-01,  8.3896e-02,  4.0893e-01,\n",
       "          4.5631e-01,  3.2684e-01,  4.7213e-01,  2.4745e-01,  8.8205e-03,\n",
       "         -1.4258e-01, -8.1988e-01, -2.4345e-01, -7.6428e-02,  1.5556e-01,\n",
       "         -2.0622e-01, -5.8932e-01,  1.5509e-01,  5.9741e-02,  1.1991e-01,\n",
       "         -1.1299e-01, -1.5199e-01,  4.3523e-01, -1.5052e-01,  9.4984e-03,\n",
       "          4.5568e-01,  9.4621e-03,  1.6628e-01,  6.1444e-02,  5.3525e-02,\n",
       "         -8.2070e-02,  2.8694e-01,  9.8581e-02, -1.5380e-01, -1.9750e-01,\n",
       "          2.6130e-01,  1.8281e-01,  1.5343e-01], device='cuda:0'),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0263, -0.0122, -0.0140,  ...,  0.0246,  0.0719, -0.0255],\n",
       "         [-0.0136, -0.0583,  0.0358,  ..., -0.0417,  0.0430,  0.0056],\n",
       "         [-0.0608, -0.0095,  0.0293,  ..., -0.0390, -0.0027,  0.0691],\n",
       "         ...,\n",
       "         [ 0.0440, -0.0391, -0.0065,  ...,  0.0465, -0.0178, -0.0106],\n",
       "         [-0.0055, -0.0124,  0.0750,  ...,  0.0077, -0.0265,  0.0786],\n",
       "         [ 0.0482,  0.0488, -0.0022,  ...,  0.0537, -0.0033, -0.0484]],\n",
       "        device='cuda:0'),\n",
       " Parameter containing:\n",
       " tensor([-7.7980e-04, -6.5806e-04, -1.0521e-04,  1.1914e-03, -1.8009e-03,\n",
       "         -2.7502e-03, -7.8233e-04,  2.5860e-03, -2.2570e-02, -2.6881e-03,\n",
       "          2.3669e-03,  3.2777e-03,  1.7142e-03,  1.7085e-03,  4.7215e-04,\n",
       "         -4.3301e-03,  1.2966e-03,  3.8041e-03,  5.5218e-05, -2.4422e-03,\n",
       "         -2.8807e-03, -3.4955e-03, -3.4653e-03,  2.6404e-03, -5.9517e-03,\n",
       "         -3.8325e-03,  5.8708e-03, -2.6597e-03,  4.1050e-03,  2.5653e-03,\n",
       "         -3.9080e-03,  4.9746e-03, -2.3052e-03,  1.9391e-03,  6.0000e-03,\n",
       "          2.3973e-03,  7.9802e-05,  4.9902e-04, -2.0522e-03, -2.3937e-03,\n",
       "         -1.8747e-03,  3.5503e-03, -1.5680e-04,  5.1923e-04, -3.8865e-04,\n",
       "         -5.5323e-03,  2.4720e-03, -1.7647e-03,  1.8318e-03,  4.0571e-03,\n",
       "         -1.6462e-03,  1.8299e-04, -2.5929e-03, -4.7880e-03, -7.7311e-04,\n",
       "          4.5449e-03,  2.3443e-03, -3.8923e-03,  3.0964e-03, -6.2480e-03,\n",
       "          2.9857e-03,  4.5695e-03,  5.4555e-04, -7.6326e-03, -6.1901e-03,\n",
       "          2.1954e-03, -2.0032e-03, -5.5015e-03,  4.7041e-03,  8.0262e-03,\n",
       "         -6.5731e-04,  5.1535e-03,  6.6659e-03, -1.6829e-03, -1.2338e-03,\n",
       "         -4.1165e-03,  5.8288e-03,  6.5286e-03, -2.8862e-03, -5.5435e-03,\n",
       "         -4.1922e-03,  3.4933e-03, -2.5085e-03,  2.6705e-03, -1.9673e-03,\n",
       "          1.4505e-03, -5.2694e-04,  3.4777e-03, -2.1290e-03, -1.0827e-02,\n",
       "          5.7160e-04, -1.2771e-02,  1.2197e-02,  9.2011e-04, -7.0105e-05,\n",
       "         -2.5185e-03, -2.5450e-03,  1.4186e-03,  3.5430e-03, -1.0059e-02,\n",
       "         -9.0462e-03, -2.1483e-03, -7.2961e-03,  1.6446e-03, -1.9546e-03,\n",
       "          1.6745e-03,  3.0747e-05,  6.8765e-04,  4.6925e-05, -4.6175e-03,\n",
       "          1.4915e-03, -1.8102e-03,  5.8387e-03,  2.6187e-03,  8.0551e-04,\n",
       "         -3.7605e-04,  4.7826e-03,  4.5015e-03,  1.8880e-03,  2.0086e-03,\n",
       "         -3.0556e-03,  1.1215e-03, -3.3826e-03, -1.6811e-03,  2.6084e-03,\n",
       "         -2.3130e-03,  9.7316e-03,  4.0729e-03,  3.3158e-02, -3.9357e-03,\n",
       "         -5.8574e-03, -8.3691e-03, -4.3808e-03,  9.6372e-04,  2.7483e-03,\n",
       "          9.2655e-04, -9.5048e-03,  5.4070e-03,  4.4320e-03, -5.7101e-03,\n",
       "          7.0762e-03,  3.9604e-03, -4.0571e-03,  5.1850e-03, -2.2990e-03,\n",
       "         -1.6759e-03,  3.5689e-03, -8.1071e-04, -8.3714e-03,  2.4655e-06,\n",
       "          8.8419e-03, -4.9797e-03,  2.9355e-03,  3.7562e-03,  2.3886e-03,\n",
       "         -1.3101e-02,  2.7496e-03, -3.6806e-03,  5.4301e-04,  3.3448e-04,\n",
       "         -3.5470e-03, -3.4434e-04,  2.0638e-02,  6.3679e-03,  4.3933e-03,\n",
       "         -7.7175e-03,  2.5263e-03,  6.5277e-03,  1.0303e-04,  9.2934e-03,\n",
       "          4.5492e-03, -3.1950e-03,  5.3094e-03, -8.3932e-03,  1.7044e-03,\n",
       "         -1.6819e-03,  1.6287e-03,  2.5236e-03, -8.7690e-04, -8.4405e-03,\n",
       "          5.1847e-03,  2.9581e-03, -8.6386e-03, -4.4322e-03, -2.2902e-03,\n",
       "          7.4777e-03, -5.2038e-03,  1.0842e-03,  6.3830e-03,  4.6294e-03,\n",
       "         -4.4676e-03, -1.4646e-02, -2.4057e-03, -1.0346e-03, -1.0573e-03,\n",
       "          1.2500e-03,  2.8151e-03,  6.4751e-03, -3.0001e-03,  1.8604e-03,\n",
       "          3.2033e-03,  3.3507e-03,  3.8772e-03, -1.0555e-03,  3.5869e-03,\n",
       "          2.9678e-03,  2.2149e-03, -6.3072e-05,  2.4339e-04, -1.5107e-03,\n",
       "          8.0292e-04,  5.0392e-03, -1.8044e-03, -3.5083e-03, -4.3115e-04,\n",
       "         -1.2534e-03,  4.0202e-03, -2.8886e-03,  1.6242e-03, -1.4304e-03,\n",
       "          2.4730e-03,  3.2120e-04,  1.5405e-03,  2.3828e-03, -1.3154e-02,\n",
       "         -9.1298e-03, -3.3335e-03, -1.6285e-03, -7.3392e-03, -4.1661e-03,\n",
       "         -7.2247e-04,  6.0622e-04, -1.3110e-04, -3.1256e-03, -1.6078e-03,\n",
       "          8.9988e-03,  5.2480e-03, -3.8470e-03,  2.5489e-03, -8.3292e-04,\n",
       "         -9.3382e-04, -2.3466e-03, -6.4202e-03, -5.4375e-03, -1.0460e-03,\n",
       "         -8.6690e-04,  9.9264e-04, -1.3279e-03, -3.3971e-03, -4.1026e-03,\n",
       "         -2.0531e-03, -4.4115e-03,  4.9086e-04, -2.7030e-03,  4.8619e-03,\n",
       "         -4.1838e-03, -2.6691e-04,  3.2127e-03,  2.1037e-03,  5.1669e-03,\n",
       "         -2.0005e-04, -3.6263e-03, -5.4068e-03, -2.5229e-03,  6.4348e-03,\n",
       "          3.3690e-04, -4.2479e-03,  8.0654e-04, -7.7276e-03,  9.2269e-04,\n",
       "         -2.4067e-03, -3.0091e-03,  3.1880e-03, -8.5397e-03, -2.6227e-03,\n",
       "          1.1948e-03, -1.8824e-03, -2.7381e-03,  1.9049e-03, -9.1579e-03,\n",
       "          6.0436e-03,  2.6585e-03,  2.3298e-03, -6.6079e-03,  1.2441e-02,\n",
       "          3.5928e-03,  1.5441e-03, -5.5536e-03, -3.7747e-03,  2.9258e-03,\n",
       "         -3.2777e-03, -3.6607e-03, -2.5393e-03,  3.6387e-03,  2.5710e-03,\n",
       "         -5.8111e-04,  1.0417e-02,  3.9542e-03, -5.3282e-03, -1.0404e-03,\n",
       "          7.7495e-03, -4.3610e-03, -1.1252e-03,  5.4011e-04,  3.0494e-03,\n",
       "          5.4364e-03, -3.0751e-03,  3.2636e-03, -1.8717e-03,  2.1970e-03,\n",
       "         -5.7621e-04,  2.5957e-03,  1.4918e-03, -2.4306e-03, -1.9329e-03,\n",
       "         -3.2307e-03, -1.1912e-02,  8.8809e-03,  2.5453e-03, -1.9595e-03,\n",
       "         -6.3161e-03, -1.3533e-02, -1.4055e-02, -5.8536e-03, -5.1156e-03,\n",
       "         -9.1114e-05,  6.3963e-03,  1.4257e-02, -6.1021e-04,  2.2210e-03,\n",
       "          5.9605e-03,  2.2709e-04,  6.7200e-03, -5.4176e-03, -2.1599e-03,\n",
       "         -3.0826e-03, -3.7544e-03,  1.6585e-03, -2.3840e-03, -1.1991e-03,\n",
       "         -1.1667e-03, -5.3543e-03, -4.0198e-04,  5.4359e-03, -3.5473e-03,\n",
       "         -4.3826e-03,  6.3052e-03,  5.3327e-03, -1.7406e-03, -1.3597e-02,\n",
       "          9.1391e-04,  3.7985e-03, -3.9346e-03, -2.1155e-03,  8.7951e-03,\n",
       "         -1.1909e-02,  5.5996e-03, -3.5501e-03,  1.5334e-02,  6.5179e-03,\n",
       "         -3.9729e-03, -7.4206e-03,  7.4139e-03, -1.9055e-03,  2.3612e-03,\n",
       "         -1.1238e-02, -2.9476e-03, -1.7415e-02, -1.7678e-03, -1.1767e-03,\n",
       "         -2.2893e-03, -4.6260e-03,  5.0488e-03,  1.9066e-04, -4.3138e-03,\n",
       "         -9.5225e-04, -5.9966e-04,  7.7655e-04,  4.9846e-03, -4.6708e-04,\n",
       "          5.4228e-03,  8.7103e-03, -4.8813e-03,  3.7611e-03,  2.1191e-03,\n",
       "         -6.8024e-03, -2.7951e-03,  3.1339e-03, -9.4608e-05, -1.0741e-03,\n",
       "          5.4122e-03,  9.2780e-03,  2.5490e-03,  4.9811e-03,  7.9523e-03,\n",
       "          3.1308e-03, -3.8672e-03, -3.1193e-03,  5.7576e-04,  2.2750e-03,\n",
       "          3.5901e-03, -1.6208e-03,  8.4059e-04, -7.0913e-04, -1.6487e-03,\n",
       "         -3.3045e-03, -7.6376e-04, -3.9418e-04, -3.9272e-04,  4.0530e-03,\n",
       "          2.5568e-03, -8.0097e-03, -5.9956e-04,  1.4689e-03, -4.9968e-03,\n",
       "         -4.1456e-03,  1.7927e-03, -1.9524e-03, -2.9090e-03, -1.7050e-03,\n",
       "         -9.6934e-04, -4.9693e-03, -2.8804e-03,  2.8753e-03,  1.8471e-03,\n",
       "         -3.5075e-03, -3.2588e-03,  3.5774e-03, -1.5532e-03,  4.1343e-03,\n",
       "         -2.8292e-04,  8.5009e-03,  7.4510e-03, -5.7594e-03, -1.9755e-03,\n",
       "         -1.7901e-03, -8.2944e-04, -1.7002e-03, -3.0182e-03, -9.5820e-04,\n",
       "         -7.4499e-04, -2.5865e-03,  3.4379e-04, -3.4571e-03,  2.8764e-03,\n",
       "          1.0027e-03, -1.4246e-04, -3.7130e-03, -1.4341e-03,  1.8129e-03,\n",
       "          1.8307e-03, -4.8006e-03,  1.9188e-03,  1.1575e-02, -5.8070e-03,\n",
       "         -1.0208e-02,  1.2869e-02,  2.2097e-04, -9.7891e-04,  1.7213e-03,\n",
       "          2.0133e-03,  6.4054e-03,  6.5047e-04,  1.9842e-03,  6.1641e-03,\n",
       "         -6.0819e-03, -5.5761e-03,  5.0444e-03,  3.0619e-03,  2.5571e-03,\n",
       "          3.7050e-03, -5.4695e-03,  4.3742e-04,  3.5363e-03, -3.4058e-03,\n",
       "         -4.0434e-03, -1.8076e-02,  4.2477e-03, -1.8634e-03, -6.6455e-03,\n",
       "         -2.7215e-03, -5.4903e-03, -1.2667e-03,  7.8642e-03,  2.2318e-03,\n",
       "          4.2974e-04,  4.3467e-03,  2.2900e-03, -1.2665e-03, -6.3440e-03,\n",
       "         -2.4504e-03, -8.1339e-03, -6.3484e-03,  2.1217e-04, -3.9087e-04,\n",
       "         -2.7943e-03,  3.5598e-03, -3.3474e-04,  5.6040e-03,  9.1944e-03,\n",
       "         -2.2236e-03, -1.8260e-03,  1.2461e-03, -1.7190e-03, -9.4363e-04,\n",
       "          4.5357e-04, -5.1931e-03,  1.9887e-03, -9.6306e-04,  2.2765e-03,\n",
       "         -2.0058e-02,  4.2592e-03,  1.7020e-03,  3.1692e-03,  7.1921e-04,\n",
       "         -2.8597e-03, -6.8326e-03, -2.6993e-03, -9.3574e-03,  6.2351e-03,\n",
       "         -7.3901e-04,  5.7935e-03, -1.0155e-03,  4.8164e-03,  2.6249e-03,\n",
       "         -2.8891e-03, -3.3473e-03, -2.9976e-03,  1.9446e-03,  1.3205e-03,\n",
       "         -2.5491e-03, -1.9851e-03,  3.9177e-03,  3.7030e-03,  2.3339e-03,\n",
       "         -2.8783e-03,  1.4636e-03, -3.2535e-03, -9.5034e-03,  2.8271e-03,\n",
       "         -9.3190e-03, -8.2312e-05,  2.7465e-03,  2.1135e-04, -7.4581e-03,\n",
       "          1.3575e-02,  2.9917e-03, -6.5620e-03,  2.8324e-04,  8.0884e-03,\n",
       "         -8.2170e-03, -4.3072e-03,  3.2660e-03, -1.5638e-04,  1.1665e-02,\n",
       "          3.5811e-03, -2.6998e-03,  8.6581e-03,  9.2929e-03,  9.2497e-03,\n",
       "         -7.1043e-03, -4.5374e-04,  5.2602e-03,  9.8420e-04, -7.9500e-03,\n",
       "         -8.1448e-03, -3.7335e-03, -8.4095e-03,  9.2634e-03,  2.8333e-03,\n",
       "         -3.6720e-03, -2.3433e-03, -2.0668e-03, -1.2748e-02, -3.5524e-03,\n",
       "         -2.0240e-03,  4.8800e-04, -9.0661e-05,  1.9597e-03,  2.5359e-03,\n",
       "         -1.1524e-03, -3.3582e-03, -3.0711e-03,  3.3554e-03,  4.8448e-03,\n",
       "          8.4079e-03,  3.9626e-03, -7.8786e-03, -7.3669e-03,  5.0285e-03,\n",
       "          4.0662e-03,  4.0981e-03, -8.1725e-03,  2.9604e-03,  5.4628e-04,\n",
       "          3.1695e-03, -2.4278e-03,  8.9728e-05,  8.1722e-03,  1.4756e-03,\n",
       "          1.0930e-03, -3.0006e-04, -3.2150e-04,  5.7529e-03,  5.6333e-03,\n",
       "         -8.4237e-03, -7.3909e-04, -3.0721e-03, -5.4863e-04,  6.4530e-03,\n",
       "          5.0608e-03, -1.1604e-03,  3.7420e-03, -3.3024e-03,  1.5683e-04,\n",
       "         -4.7267e-03,  3.1898e-03,  2.2888e-03, -4.3417e-03,  2.1898e-04,\n",
       "         -7.8783e-03,  2.8381e-04,  2.8753e-03,  9.1599e-03,  8.3369e-03,\n",
       "         -2.4673e-03,  1.9995e-03, -1.7099e-03,  8.8134e-03, -4.3299e-03,\n",
       "         -4.0968e-03,  3.0996e-05,  5.8185e-03,  6.4429e-03,  8.1068e-04,\n",
       "          1.9685e-03,  2.0342e-03,  1.5221e-03,  4.4179e-03,  3.7360e-03,\n",
       "         -1.8440e-03,  3.3737e-03, -4.5799e-03,  1.1131e-02, -3.5976e-03,\n",
       "         -7.1801e-03,  8.4121e-04, -3.9281e-03, -3.8446e-03,  4.5133e-03,\n",
       "         -5.2526e-03,  3.4947e-03, -1.9892e-03, -4.8708e-05, -6.0389e-03,\n",
       "          3.9629e-03, -1.4668e-02,  8.8518e-03,  1.9909e-03,  5.2217e-03,\n",
       "         -6.5975e-03, -5.1146e-03, -1.4401e-03,  4.9188e-03, -7.7127e-03,\n",
       "          3.1345e-03,  4.1862e-03,  6.2971e-03, -4.5204e-03, -2.1628e-03,\n",
       "         -4.7647e-03, -4.5725e-03,  7.6832e-03, -4.1930e-03,  5.7594e-03,\n",
       "          2.0022e-03, -4.5950e-03,  4.5008e-03, -3.0466e-03,  3.1537e-04,\n",
       "          1.4641e-03,  2.9766e-02, -8.8563e-04, -3.7466e-03,  3.6527e-03,\n",
       "         -6.1827e-03, -1.3231e-03,  4.5193e-03,  2.3934e-03,  3.9589e-03,\n",
       "          3.6224e-03,  1.6189e-04, -3.7789e-03,  1.1938e-03,  3.0674e-03,\n",
       "         -6.5973e-03, -4.1290e-03,  9.0389e-03, -6.8314e-03,  4.1216e-03,\n",
       "          1.2163e-03, -2.6441e-04,  2.2211e-03, -2.0252e-03,  3.6908e-03,\n",
       "         -1.3886e-03, -2.6085e-03, -5.1436e-04,  8.6401e-04,  3.0427e-03,\n",
       "          2.9934e-03, -1.2365e-02, -1.0787e-02, -8.6920e-03,  1.5860e-03,\n",
       "         -1.6784e-03, -1.3433e-03, -1.1842e-03,  2.9026e-03,  7.1351e-03,\n",
       "          1.0433e-02, -3.9235e-03, -1.6798e-02, -6.1654e-03, -8.9753e-03,\n",
       "         -7.4827e-04,  5.2294e-03, -4.9602e-03,  4.1733e-04,  1.2334e-03,\n",
       "         -1.0352e-02,  5.2679e-03, -3.7480e-04, -4.4957e-03, -7.8214e-03,\n",
       "          2.0334e-03,  4.6368e-04, -7.7520e-03,  4.9632e-03,  4.0213e-03,\n",
       "          3.9320e-03,  6.0079e-03, -2.0121e-03, -2.2320e-03,  6.9779e-03,\n",
       "          6.6463e-03,  8.9440e-03, -1.9337e-03, -5.5393e-04, -8.6614e-04,\n",
       "         -1.0080e-02, -1.4497e-03, -6.3666e-03, -2.2946e-03,  3.6482e-04,\n",
       "         -4.6345e-03,  4.6946e-04, -2.0178e-03, -2.7923e-03, -1.1921e-02,\n",
       "          2.0607e-03,  3.1313e-03,  3.3156e-03,  1.0050e-02,  1.4181e-04,\n",
       "         -7.5899e-03,  3.1369e-03, -1.5074e-03], device='cuda:0'),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0352, -0.0539,  0.0050,  ...,  0.0362,  0.0308, -0.0604],\n",
       "         [-0.0068, -0.0244,  0.0292,  ..., -0.0088,  0.0237, -0.0474],\n",
       "         [-0.0288,  0.0568,  0.0531,  ..., -0.0101,  0.0289, -0.0680],\n",
       "         ...,\n",
       "         [ 0.0159,  0.0165, -0.0316,  ...,  0.0406,  0.0096,  0.0099],\n",
       "         [-0.0500, -0.0191,  0.0224,  ...,  0.0392,  0.1194, -0.0104],\n",
       "         [ 0.0028, -0.0149,  0.0481,  ..., -0.0908,  0.0080, -0.0616]],\n",
       "        device='cuda:0'),\n",
       " Parameter containing:\n",
       " tensor([ 1.5023e-02, -5.3094e-03,  2.3572e-04,  2.0522e-03, -5.7804e-03,\n",
       "          8.1983e-03, -1.1096e-02, -4.4216e-03,  1.0920e-03, -5.7501e-03,\n",
       "         -1.2049e-03,  3.3312e-03, -5.7881e-03,  3.2863e-03,  1.6009e-02,\n",
       "         -3.8654e-03,  7.0702e-03,  9.0583e-03,  9.7348e-03, -8.1410e-03,\n",
       "          1.1158e-02, -5.4021e-03, -1.4808e-02, -6.7811e-03,  5.0396e-03,\n",
       "          4.8309e-03,  1.2264e-02,  1.7033e-03,  3.8622e-03, -5.0164e-03,\n",
       "          1.0777e-02,  2.2679e-03,  1.2602e-02,  1.7358e-03, -8.8981e-03,\n",
       "         -1.6617e-03, -5.4248e-03,  1.5820e-02, -2.7107e-03,  9.3095e-03,\n",
       "          1.4642e-02,  1.8524e-03, -6.2581e-03,  1.3867e-02,  1.6465e-02,\n",
       "         -9.9101e-03, -5.9330e-03,  3.1781e-03,  6.2056e-03, -5.7156e-03,\n",
       "          1.5173e-02,  8.2161e-03, -7.5105e-03, -6.8956e-03,  1.1878e-02,\n",
       "          1.2468e-02, -1.9451e-03,  1.4344e-02,  3.4198e-03,  9.5356e-03,\n",
       "          1.9738e-03, -2.3401e-03,  2.2367e-03, -9.8477e-04,  2.0704e-02,\n",
       "          1.1214e-02,  2.2082e-02, -2.3108e-03, -2.9715e-02,  6.5029e-02,\n",
       "          2.0563e-02, -1.7909e-03,  4.6880e-02,  3.9289e-02, -1.5563e-02,\n",
       "         -1.6345e-02, -1.1725e-02,  8.4864e-03, -2.1302e-03,  1.5644e-03,\n",
       "         -2.6728e-02,  4.6210e-03, -8.3575e-04,  2.1788e-02, -1.8891e-02,\n",
       "          4.3932e-02, -3.4591e-02,  2.0594e-02,  2.4536e-02, -4.4848e-02,\n",
       "         -9.8070e-04, -2.2324e-02, -1.9449e-02,  4.4326e-02, -6.9930e-02,\n",
       "         -2.1216e-02,  1.3391e-02,  2.3917e-02,  2.3278e-02,  7.0228e-03,\n",
       "          4.4435e-02,  1.3210e-02, -5.3480e-02, -4.0530e-02, -3.2905e-02,\n",
       "         -3.2087e-02, -2.6361e-02,  1.2619e-02,  6.9959e-03,  3.2442e-02,\n",
       "         -1.8067e-02, -1.1462e-02, -3.3291e-02, -3.1658e-03, -7.0824e-03,\n",
       "          3.4511e-03, -2.0612e-02,  1.9700e-03,  2.2766e-02,  3.1088e-02,\n",
       "          7.8084e-03,  1.4979e-02,  8.9085e-03, -4.2074e-02, -2.5084e-02,\n",
       "          2.5190e-02,  1.0802e-02,  7.8169e-02,  7.3822e-03, -1.6420e-02,\n",
       "          1.7475e-04, -1.7904e-02, -1.2519e-02, -4.5338e-03,  3.0484e-02,\n",
       "         -1.1422e-02,  2.1765e-02, -1.9468e-03, -8.3815e-03, -7.3409e-03,\n",
       "          1.0873e-02, -1.2188e-02, -1.0292e-02,  8.7603e-03,  7.4500e-03,\n",
       "          1.7819e-03,  2.2365e-02,  1.7328e-02, -1.2085e-02,  8.4014e-03,\n",
       "          2.1003e-02,  5.0341e-03, -5.2389e-03,  1.8634e-02,  5.7822e-03,\n",
       "         -1.0785e-02,  3.9486e-02,  5.5078e-03, -7.9926e-03, -8.9409e-03,\n",
       "         -1.6955e-02, -1.5862e-02,  1.1029e-02, -1.6837e-02, -8.8895e-04,\n",
       "          2.1774e-02,  1.0489e-02, -1.5112e-02,  1.0510e-02,  8.1326e-03,\n",
       "         -1.4907e-02,  1.4861e-02, -4.6929e-03,  3.4145e-02, -1.2089e-02,\n",
       "         -1.2213e-02,  2.2625e-02, -3.7597e-03, -8.0968e-03,  4.7368e-03,\n",
       "          1.8800e-02, -9.8079e-02, -4.5514e-02, -2.0916e-02, -1.4395e-02,\n",
       "         -2.4690e-02, -2.6879e-03,  1.1432e-02, -6.5998e-03,  1.1431e-02,\n",
       "         -1.4127e-04,  2.0039e-02, -2.1528e-02, -1.9943e-02, -5.8667e-03,\n",
       "         -2.6670e-02, -7.7820e-03, -1.3542e-02, -7.0426e-03, -1.3945e-02,\n",
       "          1.1313e-02, -6.0147e-03,  1.9826e-02, -3.6162e-03,  1.1581e-02,\n",
       "         -6.5605e-03, -1.2457e-02,  1.5682e-02,  2.9204e-02,  3.3701e-03,\n",
       "         -6.8273e-03,  5.6615e-04, -1.4298e-03,  1.8995e-02,  3.9813e-02,\n",
       "          3.1168e-02,  2.4817e-02,  2.0974e-02, -4.7637e-03,  1.4747e-02,\n",
       "          2.1972e-02, -5.0200e-03,  1.1263e-02,  5.1302e-02,  5.6657e-03,\n",
       "          5.3651e-04, -2.1440e-02,  1.2138e-02, -1.0497e-02,  4.6990e-03,\n",
       "          2.7727e-02, -2.7869e-02, -5.4554e-04,  3.2653e-02,  1.5652e-02,\n",
       "          1.1317e-02, -1.0897e-02, -4.0616e-02, -3.4058e-02, -7.5577e-03,\n",
       "          6.4053e-03,  6.6166e-03, -2.4033e-02,  1.1459e-02, -1.3984e-02,\n",
       "         -8.1502e-03,  1.1163e-02,  8.2542e-03, -5.3897e-03,  2.8768e-02,\n",
       "          1.1388e-02,  1.0066e-02, -2.4167e-03,  1.8377e-02,  1.2755e-02,\n",
       "          3.8524e-02,  1.9355e-02, -1.7824e-03, -1.0501e-02, -2.3361e-02,\n",
       "          7.0351e-03,  9.4095e-03,  1.2125e-02,  1.9675e-02, -7.4414e-03,\n",
       "          7.8398e-05,  3.4587e-03, -1.0619e-02,  1.5135e-02, -4.1784e-02,\n",
       "          2.5068e-02, -6.0265e-04,  5.0474e-03, -1.8176e-02,  1.2636e-02,\n",
       "          2.9639e-03, -2.2596e-02, -1.4335e-02,  1.3512e-02, -1.1183e-02,\n",
       "          1.5760e-02,  1.0042e-03,  1.4593e-02,  3.8581e-03, -7.9545e-03,\n",
       "         -1.2658e-02, -1.6685e-02, -3.6695e-02, -1.0958e-02, -4.0801e-04,\n",
       "          2.7571e-03,  2.4460e-02, -2.5951e-03, -1.9919e-02,  9.2233e-04,\n",
       "          7.4014e-04,  2.2471e-02,  2.5441e-03,  2.8494e-02,  7.1286e-03,\n",
       "         -2.4914e-02,  1.2981e-02, -3.0747e-03, -3.3040e-02,  2.0458e-02,\n",
       "         -1.9150e-02,  2.1457e-02, -2.2732e-02,  4.8147e-03, -2.1482e-02,\n",
       "         -2.3176e-02,  1.6821e-02,  1.2087e-02,  2.5303e-02, -3.5760e-03,\n",
       "          3.3084e-02,  1.6775e-02, -1.6464e-02, -1.8466e-02,  5.6927e-03,\n",
       "         -5.8718e-02, -5.9075e-03,  2.9579e-03,  4.8263e-02,  1.8685e-02,\n",
       "          2.6903e-02, -1.6432e-02,  8.4332e-03,  4.9153e-02,  1.7822e-02,\n",
       "         -4.2425e-02,  1.8471e-02,  5.7121e-02,  1.2551e-02, -1.5298e-02,\n",
       "         -1.2078e-02,  2.3887e-02,  8.0117e-03,  5.2497e-02,  3.8526e-02,\n",
       "         -3.4434e-02, -2.6399e-02, -4.5675e-02,  3.6598e-02, -3.1015e-02,\n",
       "          1.6242e-02, -3.9804e-02, -4.9304e-03, -1.7245e-02, -2.5515e-02,\n",
       "         -1.2602e-02, -4.1680e-03,  3.8595e-02, -5.9159e-02, -7.4405e-03,\n",
       "          4.8126e-02, -2.0069e-02, -9.4412e-03, -2.6123e-02,  6.4682e-03,\n",
       "          5.1684e-03, -4.6512e-04, -4.9542e-03, -2.4016e-02,  3.6240e-02,\n",
       "         -1.5456e-02,  9.1785e-03,  1.7381e-02,  2.5564e-02, -1.5134e-02,\n",
       "         -1.9038e-02, -3.8031e-02,  6.5497e-03, -1.2592e-02, -1.2116e-02,\n",
       "          3.5311e-02,  4.4152e-02,  1.0126e-03, -4.1217e-04,  1.1888e-02,\n",
       "          1.3587e-02, -1.6456e-02,  4.7564e-02, -3.4366e-02,  4.2985e-04,\n",
       "          8.2520e-03, -4.6107e-03,  1.0151e-03, -8.5547e-03, -1.0867e-02,\n",
       "          1.1150e-02,  1.0995e-02, -6.8069e-03,  8.5477e-03,  1.4168e-02,\n",
       "         -2.4357e-04,  1.0649e-02, -1.5016e-02,  5.9280e-03,  3.4130e-03,\n",
       "          7.2499e-03, -2.3598e-02, -2.2433e-03,  1.0498e-03,  2.2947e-03,\n",
       "         -9.5744e-03, -5.1591e-03,  2.3983e-02, -8.9972e-03,  6.9834e-03,\n",
       "          4.4013e-02,  1.4017e-02,  6.8826e-03,  1.8283e-03, -5.9012e-03,\n",
       "         -1.1004e-02, -7.9089e-03,  2.0125e-03, -3.0178e-03, -6.6287e-03,\n",
       "         -1.4866e-02,  9.6949e-03,  1.2629e-02, -9.4153e-03, -1.8098e-03,\n",
       "          6.4564e-04,  5.0525e-03,  8.1944e-03, -1.4976e-02,  1.5315e-02,\n",
       "         -1.5144e-02, -3.7738e-03, -2.8949e-04,  7.7387e-03, -5.3487e-03,\n",
       "         -1.7106e-02,  1.8400e-02,  4.3561e-03,  1.9203e-02,  8.5607e-03,\n",
       "          8.2309e-03,  5.0004e-03, -1.8184e-02,  9.7382e-03,  2.7278e-04,\n",
       "          1.4275e-02,  5.5432e-03,  1.2724e-02,  1.6008e-02, -1.7855e-02,\n",
       "          1.2448e-02,  1.1468e-02, -2.9315e-03, -2.9225e-02,  4.0265e-02,\n",
       "          1.6124e-03, -7.0005e-03,  2.3734e-04,  3.2821e-03,  2.4101e-02,\n",
       "         -3.6280e-02,  2.0089e-02,  3.1981e-02,  3.9811e-03, -3.0766e-02,\n",
       "          3.8253e-03,  2.3385e-02,  8.5327e-03, -7.5271e-03,  1.5639e-03,\n",
       "          6.1634e-06, -1.1759e-02,  3.0260e-03,  1.0960e-02, -7.2339e-03,\n",
       "         -2.1711e-02,  1.7470e-02,  1.9915e-02,  2.2215e-02, -2.2295e-03,\n",
       "         -1.8607e-03,  1.7004e-02,  5.7661e-03, -5.6229e-03, -5.3306e-03,\n",
       "         -1.1477e-02, -6.7000e-04,  9.3079e-03, -2.9755e-03, -4.7655e-03,\n",
       "          3.3435e-03, -1.1307e-02,  3.9731e-03,  1.0513e-02, -3.2250e-02,\n",
       "         -9.2486e-03, -2.2905e-02,  7.3241e-03, -3.5092e-02, -1.1060e-02,\n",
       "          9.0543e-03, -4.7568e-03, -3.6439e-04, -7.6407e-04,  2.9272e-02,\n",
       "         -2.7470e-02,  1.0781e-02,  1.6108e-02,  1.9351e-02,  1.5294e-02,\n",
       "          1.4930e-02,  2.7142e-02, -1.6444e-03,  5.1867e-02, -4.1590e-02,\n",
       "          2.7485e-02, -1.8159e-02,  1.4616e-02, -2.8223e-02, -2.1836e-02,\n",
       "         -3.7035e-02, -1.2575e-02,  1.2293e-02, -5.4237e-03,  2.2001e-02,\n",
       "         -7.5271e-03, -5.5768e-02, -1.6298e-02,  3.0502e-03, -1.3728e-02,\n",
       "          3.3698e-03, -4.3384e-04, -6.9684e-03, -3.1232e-02,  4.5009e-02,\n",
       "          2.0616e-02, -1.0409e-02, -2.3090e-02, -3.4293e-02,  1.1018e-03,\n",
       "         -8.5000e-03, -2.0849e-02, -3.2696e-02,  4.0556e-02,  2.3771e-02,\n",
       "          3.9398e-02, -1.3882e-02,  5.9056e-03, -9.9360e-03,  1.5168e-02,\n",
       "         -3.1733e-02, -9.0265e-03, -3.6604e-02, -1.7821e-02,  1.9675e-02,\n",
       "          4.0934e-02, -1.2232e-02, -4.6826e-03,  7.0779e-02, -5.4972e-02,\n",
       "          1.6072e-02, -2.5883e-02, -1.1649e-03, -1.1291e-02,  1.5803e-02,\n",
       "          5.6192e-02, -2.3610e-02, -9.9647e-04, -3.2063e-02, -3.2199e-03,\n",
       "         -9.1232e-03, -2.0148e-02, -7.0093e-02, -1.2437e-01,  2.3373e-02,\n",
       "         -5.0414e-04, -2.1878e-02, -1.2511e-02, -3.4763e-02, -3.2012e-03,\n",
       "          1.2513e-02, -1.1341e-02, -4.5446e-02, -2.1909e-02,  1.0888e-02,\n",
       "          1.8216e-02,  6.7742e-03, -5.3086e-03, -2.8458e-02, -2.0761e-02,\n",
       "          3.3507e-03,  4.7860e-03,  8.0991e-03, -1.3183e-02,  2.3937e-03,\n",
       "         -1.5038e-03, -1.7413e-02,  1.8932e-03,  8.7939e-03,  8.2045e-03,\n",
       "         -3.2781e-04,  8.4948e-03, -1.5060e-02,  1.7544e-02,  1.6740e-02,\n",
       "         -3.9248e-03,  2.3059e-03,  1.2013e-02,  7.5180e-03, -4.2036e-02,\n",
       "          8.0001e-04,  1.8606e-02, -4.2438e-03,  8.7675e-03, -2.2118e-02,\n",
       "          1.1333e-03, -9.4967e-03,  1.8470e-02,  1.2243e-02, -7.8087e-03,\n",
       "          7.0338e-03,  1.5369e-02, -2.7767e-03,  1.7380e-02, -1.3810e-02,\n",
       "         -1.8087e-02, -2.5262e-04,  2.8160e-03, -1.0571e-02, -1.2355e-02,\n",
       "         -4.7663e-04, -1.0160e-03,  7.8082e-03,  9.1927e-02,  1.7398e-02,\n",
       "         -4.6346e-03, -7.7768e-03, -1.5489e-02,  6.7890e-03,  1.2695e-02,\n",
       "         -2.2040e-02,  1.0588e-03, -9.3757e-03,  1.0971e-02,  1.1449e-02,\n",
       "         -1.2551e-02,  1.2195e-02, -6.5241e-03,  3.3563e-03, -5.3109e-03,\n",
       "         -7.4463e-03, -2.6397e-03,  7.1599e-03, -1.4743e-02,  2.9362e-03,\n",
       "          5.0424e-03,  4.7287e-03, -4.4464e-03,  1.3988e-02, -2.1329e-04,\n",
       "          6.0693e-03,  3.1315e-02,  2.7540e-02, -1.9092e-02, -1.0151e-02,\n",
       "         -2.2125e-03, -1.5175e-02,  1.7594e-02, -1.3841e-03,  1.5670e-02,\n",
       "          4.5302e-03, -2.3162e-03, -2.3303e-03,  1.9857e-02, -7.0773e-03,\n",
       "         -8.5168e-03,  7.7492e-03, -1.1886e-02, -8.7682e-03,  6.6262e-03,\n",
       "         -7.4530e-04,  9.6382e-03, -2.1810e-02,  3.0818e-04,  1.5030e-02,\n",
       "         -1.1015e-02,  1.8579e-02, -2.0459e-02,  8.5800e-03,  1.7035e-02,\n",
       "          5.3892e-05, -3.4280e-03,  1.0407e-02, -4.1839e-03, -6.5801e-03,\n",
       "          2.4782e-02, -1.2955e-03,  5.8868e-03,  8.7253e-03,  2.5282e-03,\n",
       "         -8.6695e-03, -1.7313e-02, -6.2276e-03, -1.4733e-02,  3.7079e-02,\n",
       "         -2.1547e-04,  2.7257e-03, -5.9696e-04,  1.9285e-02, -2.0536e-02,\n",
       "          1.7143e-02,  1.4449e-02, -6.5542e-03, -2.3365e-02,  1.2347e-02,\n",
       "          8.0866e-03, -2.2223e-02,  3.1805e-03,  3.2623e-02, -1.3899e-02,\n",
       "         -4.5315e-02, -4.4333e-03, -8.2556e-04,  1.6854e-02,  5.2781e-03,\n",
       "         -2.3699e-02,  4.9956e-03,  4.7935e-03,  2.9074e-02,  8.2377e-04,\n",
       "         -1.3017e-02, -4.3768e-02,  1.0733e-02,  1.4151e-03, -1.1715e-02,\n",
       "          3.6786e-02,  3.3582e-04, -1.3081e-02, -2.4121e-03, -1.9138e-02,\n",
       "         -1.5444e-02,  4.6571e-03,  2.4032e-03,  2.0608e-02,  1.9170e-03,\n",
       "         -4.8462e-03, -4.6177e-03, -1.2237e-02, -2.1239e-02, -6.7373e-03,\n",
       "          2.1705e-03,  3.7158e-03,  6.8323e-03, -1.2950e-02,  1.8724e-02,\n",
       "          1.3274e-02,  1.7787e-02, -3.5668e-03,  2.7728e-03, -2.0766e-02,\n",
       "          2.5364e-03,  1.2686e-03, -1.5676e-02, -9.3327e-03, -5.6336e-03,\n",
       "          3.8475e-03, -1.5278e-02,  1.6453e-02], device='cuda:0'),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0236, -0.0468,  0.0435,  ...,  0.0162, -0.0310, -0.0275],\n",
       "         [ 0.0311, -0.0222,  0.0545,  ...,  0.0040, -0.0058, -0.0304],\n",
       "         [-0.0006, -0.0160,  0.0460,  ...,  0.0392,  0.0151,  0.0572],\n",
       "         ...,\n",
       "         [-0.0148, -0.0276, -0.0219,  ...,  0.0260,  0.0732, -0.0707],\n",
       "         [-0.0117, -0.0652, -0.1287,  ...,  0.0433,  0.0770,  0.0071],\n",
       "         [-0.0846,  0.0354,  0.0357,  ..., -0.0091, -0.0571,  0.0281]],\n",
       "        device='cuda:0'),\n",
       " Parameter containing:\n",
       " tensor([ 2.5661e-02,  2.8438e-03, -4.7568e-03,  2.1495e-02, -1.7552e-02,\n",
       "         -5.8286e-02, -8.2861e-03, -1.9896e-02,  4.7550e-02,  4.5496e-02,\n",
       "          7.5562e-02, -2.5829e-02,  2.9784e-02, -2.9771e-02,  7.9811e-03,\n",
       "         -1.3309e-02,  9.8513e-03, -3.7418e-02, -6.9451e-03, -8.4682e-03,\n",
       "          3.6152e-02,  2.0826e-02, -3.5835e-02,  1.2061e-03,  8.2127e-03,\n",
       "         -3.4215e-02, -5.0382e-02, -3.7946e-02,  1.3244e-02, -2.5479e-02,\n",
       "         -7.1411e-02,  4.4119e-03, -5.0616e-02,  2.1423e-03,  6.5417e-02,\n",
       "          5.8312e-02,  4.3113e-02, -3.1691e-02,  3.3276e-02,  3.9858e-02,\n",
       "          4.3144e-03, -2.1405e-02,  4.3141e-02, -3.2827e-02, -3.3253e-04,\n",
       "          4.5474e-03,  3.2558e-02,  1.1090e-02,  1.1631e-02,  9.3667e-02,\n",
       "          2.5723e-02, -3.6623e-02, -9.7763e-03, -5.6858e-02,  8.9644e-03,\n",
       "         -1.0341e-03,  4.1245e-02, -9.4715e-02, -5.0758e-02, -2.9382e-02,\n",
       "         -4.8360e-02, -7.0828e-03, -7.2977e-02, -2.2347e-02,  2.9260e-03,\n",
       "         -1.9503e-02, -5.4568e-02,  2.1875e-02, -4.5742e-02, -3.4955e-02,\n",
       "          1.5623e-02,  1.7605e-02, -4.6641e-04,  6.3658e-03,  1.3580e-02,\n",
       "          8.2934e-02, -8.5081e-02,  2.1419e-02, -2.6295e-02, -4.3614e-02,\n",
       "         -6.1995e-02,  1.3076e-02,  2.8390e-02,  1.6619e-03,  5.5403e-02,\n",
       "          8.4154e-03,  4.5806e-02, -3.9007e-02, -3.0372e-02, -4.1927e-02,\n",
       "          3.4220e-02, -1.9319e-02, -2.6155e-02, -6.3656e-02, -4.6763e-04,\n",
       "          7.6021e-02, -3.0854e-02,  2.9210e-03, -4.9407e-02,  3.4103e-02,\n",
       "          1.3549e-02,  9.0339e-03, -1.5099e-02,  2.8884e-02,  4.2694e-02,\n",
       "          1.3670e-02, -1.3515e-02,  3.4982e-02,  1.0633e-02,  8.1628e-02,\n",
       "          7.3829e-02, -3.7645e-02,  2.5425e-02, -4.3103e-02,  5.6904e-02,\n",
       "         -5.3126e-02, -3.4979e-02,  2.1494e-02,  6.6136e-03, -2.9801e-03,\n",
       "         -4.0099e-02, -9.0502e-02,  1.9767e-02, -3.6408e-02, -1.4459e-02,\n",
       "         -3.4766e-03, -6.3397e-02, -1.8621e-02, -7.1522e-04,  5.2631e-02,\n",
       "         -2.4067e-02, -6.0373e-02, -2.4351e-02,  2.7051e-02, -1.2240e-02,\n",
       "          1.6177e-02, -4.1943e-02,  3.2331e-02, -4.9130e-02,  5.1233e-02,\n",
       "          1.8584e-02,  4.3312e-02,  3.8919e-02,  9.5328e-04,  2.9318e-02,\n",
       "          4.4681e-02,  4.6009e-02,  3.2909e-02,  6.9443e-02, -4.3578e-02,\n",
       "          2.3167e-02, -3.9903e-02,  2.7478e-02,  2.1105e-02, -2.0274e-02,\n",
       "         -5.6866e-02, -7.1785e-02, -1.9990e-03,  1.5426e-02,  4.6471e-04,\n",
       "         -5.9776e-02,  4.3889e-02,  1.2913e-02,  4.5920e-02, -1.5320e-02,\n",
       "         -3.5193e-02, -5.8929e-02,  7.4193e-03, -1.8163e-04, -1.0267e-02,\n",
       "          1.5405e-02,  2.0184e-02,  4.0796e-03, -1.5596e-02,  1.8059e-02,\n",
       "          9.8132e-03, -2.5639e-02, -3.9419e-02, -3.9124e-02, -4.3264e-02,\n",
       "          8.3899e-02, -3.2598e-02,  2.8068e-02, -2.0178e-02,  1.0051e-02,\n",
       "         -2.8135e-02, -3.6558e-02,  2.2010e-02,  2.0954e-02, -1.0334e-02,\n",
       "          8.8971e-03,  2.8212e-02, -6.6715e-02,  3.7474e-02,  2.1460e-02,\n",
       "         -1.5647e-02,  3.5343e-02, -3.2962e-02, -2.1474e-02, -8.3971e-02,\n",
       "          6.4572e-02,  3.4832e-02, -8.0240e-02,  3.3343e-02,  3.0641e-02,\n",
       "         -8.0081e-02,  5.2524e-03, -9.7763e-03, -4.0240e-02, -3.0048e-02,\n",
       "          1.1611e-02,  3.0359e-02, -4.9964e-02, -9.5533e-03, -2.3882e-02,\n",
       "          1.8626e-02,  3.1076e-02, -2.8956e-02,  8.9075e-04,  1.2022e-02,\n",
       "         -1.1348e-01,  2.8774e-02, -2.4897e-03, -3.1775e-02,  5.3694e-03,\n",
       "         -5.5049e-02,  1.7559e-02, -1.5409e-02, -7.1508e-02,  4.7330e-02,\n",
       "          1.6682e-02,  1.1172e-02,  4.5701e-02,  1.5628e-02, -3.1049e-02,\n",
       "          2.5248e-02, -4.6465e-02,  7.8496e-03, -5.0930e-03, -5.0428e-02,\n",
       "         -1.1631e-01, -1.0528e-02,  7.3653e-02, -8.1540e-02,  5.3539e-02,\n",
       "          1.4288e-02, -3.9082e-02, -1.4117e-02,  5.2567e-02, -3.0875e-02,\n",
       "          3.6666e-03,  2.4879e-02,  3.9342e-02, -2.2215e-02,  5.7524e-03,\n",
       "          1.6564e-02,  2.3978e-02,  2.2593e-02, -1.6909e-02, -5.2501e-02,\n",
       "          4.0561e-02,  2.3099e-02,  1.6978e-02, -1.0553e-02, -3.0434e-03,\n",
       "          1.2279e-03,  3.3629e-03,  5.0138e-02, -5.4341e-02, -1.9108e-02,\n",
       "          9.4589e-03, -2.4958e-02, -1.0721e-03,  6.2184e-02, -2.6718e-02,\n",
       "         -3.5827e-02,  1.3325e-02,  1.9591e-02,  3.3563e-02, -3.0726e-02,\n",
       "         -3.7181e-02, -5.5001e-02, -1.6310e-02,  4.5146e-02,  2.9605e-02,\n",
       "          4.2929e-02, -7.2622e-02,  7.6866e-02,  1.6351e-02, -6.9909e-02,\n",
       "          1.3670e-02, -1.5051e-02, -7.6326e-03, -3.4489e-02, -9.6903e-03,\n",
       "          6.0245e-03,  3.5591e-02,  8.8463e-03,  5.2448e-02, -2.6586e-03,\n",
       "          1.4362e-03, -1.8378e-02,  4.4712e-02, -2.6613e-03, -1.2319e-02,\n",
       "         -1.3705e-02,  6.2081e-03,  2.8958e-02,  1.2483e-01, -2.0615e-02,\n",
       "          2.2608e-02, -1.9340e-02,  1.7909e-02, -1.5324e-02, -1.5443e-02,\n",
       "          7.1784e-02,  3.7500e-02, -2.2002e-02,  2.1981e-02, -2.8093e-02,\n",
       "         -3.9746e-03, -3.1899e-02, -7.3583e-04,  1.8395e-02,  1.5110e-02,\n",
       "          2.1177e-02, -6.4467e-02, -6.3118e-02, -2.1338e-03, -1.0738e-02,\n",
       "          6.5255e-02, -2.8122e-02,  6.8499e-02, -4.4276e-02,  9.3972e-03,\n",
       "         -3.7623e-03,  1.2524e-02,  3.1410e-03,  9.9651e-05, -1.1524e-02,\n",
       "         -2.6397e-02,  6.4055e-02,  3.4766e-02,  2.3612e-02, -7.2052e-03,\n",
       "         -3.3810e-02, -7.8489e-03, -5.2412e-03,  3.5894e-02,  1.7026e-02,\n",
       "         -1.5733e-03, -8.4395e-03, -5.4472e-02, -1.4355e-02,  3.7700e-02,\n",
       "          4.3268e-02, -7.4828e-03, -5.5729e-03,  3.4090e-02, -1.9263e-02,\n",
       "         -7.6067e-02,  3.2265e-02, -4.4947e-02,  4.3225e-02,  3.1662e-03,\n",
       "         -2.7786e-02,  5.8743e-02, -1.2443e-02, -1.7314e-02,  7.0499e-03,\n",
       "         -1.0274e-02, -1.6667e-02, -3.7694e-03,  7.1680e-03,  2.7316e-02,\n",
       "          3.3137e-02, -2.7361e-02,  5.6893e-03,  3.0824e-03, -4.4608e-03,\n",
       "         -2.1742e-02,  1.3567e-02,  3.9650e-02,  2.5170e-02, -2.0057e-02,\n",
       "          8.9578e-03, -1.9388e-02,  1.1835e-02,  5.2143e-02,  3.0606e-03,\n",
       "         -2.5052e-02, -3.6218e-02, -3.0498e-02,  1.9551e-02,  2.3248e-02,\n",
       "         -5.6564e-03,  7.4358e-03, -2.7215e-02,  3.3683e-02,  2.6620e-02,\n",
       "         -4.4610e-02,  5.7572e-02, -3.1604e-02, -2.3627e-02,  2.1448e-02,\n",
       "          7.4826e-02,  1.9757e-02, -2.5340e-02, -4.6560e-02,  7.0425e-02,\n",
       "          3.6600e-02,  6.1359e-02,  2.8134e-02,  2.0915e-02, -3.8265e-02,\n",
       "         -9.8512e-02, -4.6729e-02, -2.1719e-02, -5.3298e-02, -4.8249e-02,\n",
       "          4.7664e-02,  3.4870e-02, -6.3118e-02,  4.6895e-02,  8.7264e-03,\n",
       "          3.6550e-03,  3.8573e-02,  1.5227e-02,  2.0382e-02, -3.1078e-02,\n",
       "          3.8493e-02, -3.8904e-02,  1.9797e-02, -1.7568e-02,  3.3714e-02,\n",
       "          2.1158e-02,  1.4755e-03, -4.0277e-02, -2.2181e-02,  4.5928e-02,\n",
       "         -3.9831e-02,  1.7957e-02,  2.1303e-02,  3.6688e-02,  9.8751e-03,\n",
       "          4.4210e-02,  2.5282e-02,  8.5110e-02,  3.3132e-02,  1.3477e-02,\n",
       "         -6.1588e-02, -3.9152e-02, -6.1249e-02, -5.5067e-03,  1.6928e-02,\n",
       "         -8.8260e-03,  2.8438e-02, -2.5177e-02, -1.1409e-02,  2.4568e-02,\n",
       "         -8.9766e-03,  1.1614e-02,  5.4627e-03, -2.0370e-02, -6.5533e-02,\n",
       "          8.9341e-03, -9.5544e-03, -4.2616e-02, -3.4375e-03,  1.0111e-01,\n",
       "          2.5815e-02,  1.8628e-02, -2.1954e-02, -4.7797e-02,  6.2480e-02,\n",
       "          4.9612e-03, -6.8434e-02, -6.3310e-02, -7.3980e-03,  3.8054e-03,\n",
       "          4.2839e-04,  1.1967e-02, -3.4700e-02,  9.2769e-03,  6.2205e-02,\n",
       "          1.5765e-02, -5.4350e-02, -1.1647e-02, -3.2354e-02,  1.5515e-02,\n",
       "         -1.7500e-03, -1.9691e-03, -4.2725e-02,  4.3707e-02,  2.1509e-02,\n",
       "         -4.5497e-03,  2.8150e-02, -2.1341e-02, -3.6321e-02,  2.9687e-02,\n",
       "          4.2727e-02,  2.2152e-02,  4.2795e-02,  3.5837e-02,  9.6598e-02,\n",
       "          5.9712e-03,  5.0632e-02, -2.0811e-03,  2.6818e-02, -3.0629e-02,\n",
       "          1.3760e-01, -1.4097e-02, -3.7076e-02,  9.3921e-02, -4.3322e-02,\n",
       "         -3.4324e-02,  5.4981e-02, -4.7629e-02,  7.0833e-02,  1.7546e-02,\n",
       "          2.1167e-02, -1.3044e-02,  2.3744e-02,  2.1364e-02, -1.1893e-02,\n",
       "          1.1523e-02, -2.5295e-02,  5.4490e-02, -4.5487e-02,  2.5299e-02,\n",
       "          3.1280e-02,  5.4161e-03, -7.2364e-02, -5.7143e-02,  2.6617e-02,\n",
       "          8.0478e-02, -9.4555e-03, -6.4540e-03, -4.5154e-02,  5.7233e-02,\n",
       "         -4.3240e-02, -4.7108e-02,  2.2601e-02,  1.0235e-01,  3.2031e-02,\n",
       "         -4.0476e-03, -3.6157e-04,  4.4914e-03, -3.0613e-02, -4.4907e-02,\n",
       "         -1.7892e-02,  2.1532e-02,  5.7970e-02, -4.6251e-02, -4.3412e-02,\n",
       "          3.6807e-02, -1.4456e-02, -7.5287e-02,  1.9401e-03,  8.2555e-02,\n",
       "          3.7524e-02, -4.4795e-03, -7.3578e-02,  1.8694e-02, -9.5478e-03,\n",
       "         -1.7393e-02, -3.1529e-02, -1.7297e-02, -7.9018e-02,  4.8072e-02,\n",
       "          5.8333e-02, -5.3367e-02, -2.4755e-02, -2.4387e-02,  2.6818e-02,\n",
       "          4.0837e-02, -1.2251e-02,  1.8094e-02, -2.3803e-02,  1.4613e-02,\n",
       "          6.4903e-02, -1.3786e-02, -1.3899e-02, -3.9337e-02, -1.4666e-02,\n",
       "         -1.1109e-02, -4.4043e-02,  6.9114e-03, -2.7411e-02, -4.7538e-02,\n",
       "         -1.4882e-02, -3.1888e-02,  3.2904e-02,  8.3559e-02, -2.4537e-02,\n",
       "         -2.1875e-02,  2.4720e-02, -2.7432e-02, -3.8548e-02,  1.7113e-02,\n",
       "         -6.7927e-03, -2.0156e-02, -3.4586e-02,  2.7359e-02, -3.1860e-02,\n",
       "         -3.1545e-02,  7.5282e-02, -1.5417e-02,  2.8683e-02, -5.7584e-02,\n",
       "         -4.2532e-02,  1.3981e-02, -1.3137e-02,  5.3427e-02,  2.0525e-02,\n",
       "          4.0794e-02,  1.8850e-02,  7.0226e-02,  7.7246e-03,  3.5578e-02,\n",
       "         -3.1588e-02, -9.3457e-03,  1.9679e-02, -1.5787e-02, -2.7081e-02,\n",
       "          4.7237e-02,  3.1430e-02,  3.3851e-02,  9.3408e-03,  2.6763e-02,\n",
       "         -1.6265e-02,  1.1736e-04,  2.4960e-02, -4.3881e-02, -5.3282e-02,\n",
       "         -1.4752e-02, -1.4754e-02, -1.7534e-02, -8.6470e-02, -5.6863e-02,\n",
       "         -2.6347e-02,  5.2486e-03,  7.7843e-02,  8.2860e-02, -4.1278e-02,\n",
       "          9.1167e-02,  2.1269e-03, -6.5450e-02, -1.3494e-02, -7.1936e-04,\n",
       "         -7.3792e-03, -4.2810e-02, -3.5779e-03, -1.9401e-02, -2.1825e-02,\n",
       "         -3.1193e-02, -1.6985e-02,  1.9392e-02, -4.7002e-02,  1.8754e-02,\n",
       "          2.8218e-04,  1.0279e-02, -3.2457e-02,  3.6972e-02, -3.5447e-02,\n",
       "          1.6364e-02,  2.2258e-02,  1.1818e-02, -9.6817e-03,  8.5271e-03,\n",
       "         -5.5865e-02, -6.0954e-02,  4.2918e-03, -2.5881e-02, -3.2346e-02,\n",
       "          2.5538e-02, -1.3024e-02, -3.9184e-02, -3.0027e-02, -6.1984e-02,\n",
       "         -1.3559e-02,  3.0840e-02, -1.0358e-02,  1.0155e-02, -4.0098e-02,\n",
       "          6.9040e-03, -4.5944e-02, -3.5748e-02,  1.0294e-02,  4.3971e-05,\n",
       "          5.4092e-02, -1.9467e-02,  1.3419e-02,  1.6137e-02, -5.0566e-03,\n",
       "         -3.0796e-02,  3.4840e-02,  3.0833e-02, -2.9789e-02,  7.1727e-03,\n",
       "         -5.6943e-02, -1.8304e-02,  6.2423e-02,  2.1363e-02, -1.2050e-02,\n",
       "         -2.4211e-02, -1.2922e-02, -7.7194e-04, -6.0428e-02, -5.0137e-02,\n",
       "         -3.4469e-03, -3.0698e-03,  5.6597e-02, -1.8726e-02,  2.8393e-02,\n",
       "          2.8395e-02,  5.6358e-02, -1.5945e-02, -4.2779e-02,  3.8556e-02,\n",
       "         -8.2085e-03, -6.2440e-03, -3.7290e-02, -1.2846e-02, -4.0179e-02,\n",
       "          5.2332e-02,  5.0167e-02,  7.8664e-02, -2.7305e-03, -4.5874e-03,\n",
       "          2.8956e-03, -3.4683e-02, -4.0468e-04,  1.0754e-02, -2.0960e-02,\n",
       "          4.8482e-02,  2.1213e-02, -6.5629e-02,  4.4222e-02,  3.2211e-02,\n",
       "          9.3537e-03,  1.4696e-03,  2.8208e-02,  4.5527e-02, -5.7067e-02,\n",
       "          1.1416e-02,  1.6224e-02,  1.3811e-02,  5.3839e-02,  4.2043e-02,\n",
       "          1.6189e-02, -3.2005e-02,  8.8293e-02, -2.0637e-03,  2.7410e-02,\n",
       "          2.9377e-02,  4.1529e-02, -4.1723e-02,  4.1841e-02,  1.3697e-02,\n",
       "         -6.8449e-02,  1.1665e-02, -3.2171e-02,  1.7119e-02,  3.8358e-02,\n",
       "          1.3254e-03, -5.5786e-02, -5.3087e-02], device='cuda:0'),\n",
       " Parameter containing:\n",
       " tensor([0.8520, 0.8020, 0.8554, 0.8150, 0.8442, 0.8227, 0.8563, 0.8517, 0.8476,\n",
       "         0.8635, 0.8254, 0.8606, 0.8426, 0.8118, 0.8281, 0.9073, 0.8642, 0.8385,\n",
       "         0.8235, 0.8600, 0.8387, 0.8405, 0.8271, 0.8326, 0.8258, 0.8467, 0.8510,\n",
       "         0.8035, 0.8450, 0.8336, 0.8817, 0.8028, 0.8418, 0.8576, 0.8510, 0.8368,\n",
       "         0.8673, 0.8282, 0.8233, 0.8328, 0.8379, 0.8346, 0.8188, 0.8750, 0.8391,\n",
       "         0.8413, 1.1254, 0.8492, 0.8118, 0.8285, 0.8939, 0.8430, 0.8605, 0.8560,\n",
       "         0.8493, 0.8606, 0.8180, 0.8470, 0.8082, 0.8299, 0.8415, 0.8650, 0.8327,\n",
       "         0.8603, 0.8658, 0.8812, 0.8387, 0.8172, 0.8931, 0.8885, 0.8196, 0.8302,\n",
       "         0.8447, 0.8454, 0.8261, 0.8812, 0.8591, 0.8650, 0.8281, 0.8417, 0.8359,\n",
       "         0.8303, 0.8623, 0.8417, 0.8185, 0.8217, 0.8374, 0.8196, 0.8117, 0.9004,\n",
       "         0.8762, 0.8366, 0.8584, 0.8453, 0.8473, 0.8528, 0.8545, 0.8678, 0.8649,\n",
       "         0.8712, 0.8332, 0.8691, 0.8300, 0.8504, 0.8626, 0.8236, 0.8181, 0.8370,\n",
       "         0.8406, 0.9652, 0.8724, 0.8404, 0.8218, 0.8162, 0.8251, 0.8224, 0.8512,\n",
       "         0.8259, 0.8450, 0.8153, 0.8587, 0.8993, 0.8241, 1.2920, 0.8287, 0.8297,\n",
       "         0.8681, 0.8488, 0.8393, 0.8451, 0.8472, 0.8094, 0.8018, 0.8529, 0.8470,\n",
       "         0.8798, 0.8535, 0.8492, 0.8686, 0.8382, 0.8385, 0.8307, 1.0296, 0.9308,\n",
       "         0.8808, 0.8298, 0.8514, 0.8985, 0.8438, 0.8417, 0.8342, 0.8715, 0.8730,\n",
       "         0.8460, 0.8054, 0.8398, 0.8586, 0.8332, 0.8217, 0.7981, 0.8342, 0.8277,\n",
       "         0.8168, 0.8385, 0.8649, 0.8550, 0.8443, 0.8911, 0.8568, 0.8623, 0.8185,\n",
       "         0.8600, 0.8310, 0.8460, 0.8509, 0.8397, 0.8454, 0.8457, 0.8253, 0.8643,\n",
       "         1.1840, 0.8535, 0.8279, 0.8479, 0.8503, 0.8324, 0.8675, 0.8430, 0.8236,\n",
       "         0.8451, 0.8621, 0.8554, 0.8516, 0.8620, 0.8416, 0.8643, 0.8535, 0.8616,\n",
       "         0.8649, 0.8345, 0.8596, 0.8026, 0.8355, 0.8560, 0.8635, 1.2936, 0.8091,\n",
       "         0.8206, 0.8587, 0.8788, 0.8294, 0.8213, 0.8474, 0.8747, 0.8431, 0.8346,\n",
       "         0.8436, 0.8378, 0.8255, 0.8402, 0.8609, 0.8418, 0.8252, 0.8469, 0.8228,\n",
       "         1.0993, 0.8294, 0.8562, 0.8261, 1.0967, 0.8632, 0.8498, 0.8616, 0.8442,\n",
       "         0.8458, 0.8609, 0.8296, 0.8142, 0.8627, 0.8550, 0.8378, 0.8412, 0.8500,\n",
       "         0.9119, 0.8637, 0.8188, 0.8246, 0.8469, 0.8447, 0.8039, 0.8527, 0.8527,\n",
       "         0.8055, 0.8358, 0.8419, 0.8348, 0.8158, 0.8385, 0.8372, 0.8460, 0.8587,\n",
       "         0.8313, 0.8400, 0.8633, 0.8494, 0.8659, 0.8852, 0.8484, 0.8725, 0.8815,\n",
       "         0.8591, 0.8487, 0.8529, 0.9536, 0.8555, 0.8516, 0.8515, 0.8438, 0.8383,\n",
       "         0.8323, 0.8229, 0.8680, 0.8332, 0.8290, 0.8668, 0.8555, 0.8582, 0.9376,\n",
       "         0.8376, 0.8351, 0.8133, 0.8465, 0.8208, 0.8636, 0.8641, 0.8529, 0.8250,\n",
       "         0.8188, 0.8456, 0.8390, 0.8307, 0.8390, 0.8566, 0.8196, 0.7716, 0.8295,\n",
       "         0.8440, 0.8294, 1.6117, 0.8321, 0.8725, 0.8724, 0.8583, 0.8386, 0.8363,\n",
       "         0.7997, 0.7855, 0.8234, 0.8731, 0.8391, 0.8603, 0.8488, 0.8502, 0.8635,\n",
       "         0.8604, 0.8325, 0.8267, 0.9433, 0.8411, 0.8297, 0.8507, 0.8619, 0.8198,\n",
       "         0.8596, 0.9341, 0.8629, 0.8587, 0.8290, 0.8592, 0.8635, 0.8309, 0.7260,\n",
       "         0.8483, 0.8215, 0.8450, 0.8619, 0.8555, 0.8625, 0.8018, 0.8728, 0.8412,\n",
       "         0.8330, 0.9033, 0.8347, 0.8568, 0.8650, 0.8478, 0.8590, 0.8300, 0.8117,\n",
       "         0.9446, 0.8524, 0.8504, 0.8431, 0.8505, 0.8232, 0.8646, 0.8924, 0.8613,\n",
       "         0.8237, 0.8907, 0.8542, 0.8381, 0.8493, 0.9394, 0.8559, 0.8457, 0.8347,\n",
       "         0.8384, 0.8523, 0.8497, 0.7008, 0.8630, 0.8686, 0.8991, 0.8517, 0.8630,\n",
       "         0.8588, 0.8568, 0.8810, 0.8367, 0.8362, 0.8216, 0.8617, 0.8304, 0.7871,\n",
       "         0.8425, 0.8594, 0.8298, 0.8511, 0.8365, 0.8434, 0.8607, 0.8294, 0.8358,\n",
       "         0.8798, 0.8147, 0.8358, 0.8634, 0.8583, 0.8488, 0.8582, 0.8690, 0.8204,\n",
       "         0.8500, 0.8367, 0.8347, 0.8550, 0.8554, 0.8579, 0.8344, 0.8463, 0.8962,\n",
       "         0.8290, 0.8439, 0.8604, 0.8367, 0.7990, 0.8059, 0.8492, 0.8747, 0.8827,\n",
       "         0.8564, 0.8559, 0.8367, 0.8087, 0.8446, 0.8243, 0.8188, 0.8481, 0.8595,\n",
       "         0.8728, 0.8444, 0.8342, 0.8165, 0.9047, 0.8274, 0.8475, 0.8678, 0.8303,\n",
       "         0.8312, 0.8790, 0.8942, 0.8525, 0.8330, 0.8378, 0.8249, 0.8349, 0.8707,\n",
       "         0.8478, 0.8532, 0.8067, 0.8345, 0.8687, 0.8831, 0.8205, 0.8745, 0.8849,\n",
       "         0.8116, 0.8607, 1.0646, 0.8105, 0.8714, 0.8553, 0.8233, 0.8396, 0.8215,\n",
       "         0.8131, 0.8671, 0.8644, 0.8547, 0.8483, 0.8177, 0.8354, 0.8391, 0.8526,\n",
       "         0.8758, 0.8899, 0.8336, 0.8404, 0.8413, 0.8585, 0.8640, 0.8350, 0.8547,\n",
       "         0.8350, 0.8335, 0.8430, 0.8507, 0.8385, 0.8965, 0.8858, 0.8368, 0.8680,\n",
       "         0.8462, 0.8884, 0.8296, 0.8275, 0.8526, 0.8689, 0.8584, 0.8190, 0.8756,\n",
       "         0.8311, 0.9045, 0.8605, 0.8543, 0.8210, 0.8476, 0.8063, 0.8726, 0.8491,\n",
       "         0.8436, 0.8547, 0.8466, 0.8363, 1.0828, 0.8279, 0.8633, 0.8279, 0.8396,\n",
       "         0.8248, 0.8555, 0.8601, 0.8649, 0.9414, 0.8409, 0.8382, 0.8422, 0.7070,\n",
       "         0.8349, 0.8393, 0.8890, 0.8678, 0.8809, 0.8638, 0.8489, 0.8288, 0.8630,\n",
       "         0.8506, 0.8118, 0.8649, 0.8647, 0.8499, 0.8278, 0.8581, 0.7798, 0.8732,\n",
       "         0.8623, 0.8570, 0.8415, 0.8307, 0.8156, 0.8518, 0.8377, 0.8572, 0.8434,\n",
       "         0.8699, 0.8062, 0.8603, 0.8478, 0.8088, 0.8536, 0.8418, 0.8255, 0.8336,\n",
       "         0.8214, 0.8632, 0.8536, 0.8572, 0.8295, 0.8693, 0.8435, 0.8618, 0.8350,\n",
       "         0.8450, 0.8762, 0.8235, 0.8998, 0.8017, 0.8178, 0.8160, 0.8456, 0.8700,\n",
       "         0.8464, 0.8341, 0.8547, 0.8416, 0.8600, 0.8253, 0.8222, 0.8422, 0.8318,\n",
       "         0.8716, 0.8259, 0.8703, 0.8493, 0.8493, 0.8867, 0.8211, 0.8444, 0.8451,\n",
       "         0.8555, 0.8471, 0.8320, 0.8377, 0.8427, 0.8544, 0.8529, 0.8680, 0.8911,\n",
       "         0.8428, 0.8413, 0.8471, 0.8487, 0.8346, 0.8576, 0.8398, 0.8257, 0.8380,\n",
       "         0.8574, 0.8316, 0.8696, 0.8554, 0.8419, 0.8407, 0.8487, 0.8443, 0.8418,\n",
       "         0.8603, 0.8382, 0.8771, 0.8511, 0.8505, 0.8360, 0.8882, 0.8473, 0.9015,\n",
       "         0.8756, 0.8869, 0.8460, 0.8616, 0.8442, 0.8155, 0.8514, 0.8648, 0.8657,\n",
       "         0.8804, 0.8519, 0.8621, 0.8479, 0.8607, 0.8232, 0.8773, 0.8574, 0.8427,\n",
       "         0.8494, 0.9454, 0.8615, 0.8602, 0.9206, 0.8314, 0.8427, 0.8468, 0.8528,\n",
       "         0.8728, 0.8320, 0.8267, 0.8138, 0.8464, 0.8511, 0.8437, 0.8251, 0.8903,\n",
       "         0.8346, 0.8762, 0.8286, 0.8463, 0.8644, 0.8704, 0.8401, 0.8421, 0.8382,\n",
       "         0.8371, 0.8343, 0.8397, 0.8621, 0.8402, 0.8429, 0.8619, 0.8596, 0.8597,\n",
       "         0.8786, 0.8325, 0.8564, 0.8275, 0.8584, 0.8863, 0.8499, 0.8511, 0.8212,\n",
       "         0.8188, 0.8686, 0.8701, 0.8482, 0.8704, 0.8937, 0.8291, 0.8391, 0.8516,\n",
       "         1.5371, 0.8554, 0.8568, 0.8563, 0.8404, 0.8409, 0.8398, 0.8875, 0.8691,\n",
       "         0.8495, 0.8180, 0.8389, 0.8536, 0.6582, 0.8556, 0.8524, 0.8522, 0.8569,\n",
       "         0.8311, 0.8471, 0.8558, 0.8501, 0.8438, 0.8706, 0.8647, 0.8470, 0.8286,\n",
       "         0.8387, 0.8455, 0.8571, 0.9022, 0.8090, 1.0631, 0.8578, 0.8398, 0.8367,\n",
       "         0.9711, 0.8156, 0.8653, 0.8613, 0.7998, 0.7831, 0.8282, 0.8466, 0.8523,\n",
       "         0.8346, 0.8365, 0.8424], device='cuda:0'),\n",
       " Parameter containing:\n",
       " tensor([-3.1344e-02,  1.2080e-02, -4.6364e-02, -3.0130e-02,  7.9443e-02,\n",
       "          5.0981e-02,  1.1908e-02, -2.0652e-02, -1.0936e-01, -1.1688e-01,\n",
       "         -8.6071e-02,  2.6358e-02, -1.1645e-01,  1.5676e-02, -3.1420e-02,\n",
       "          3.6622e-02,  5.6841e-02,  4.6632e-02, -8.3829e-03, -7.8806e-02,\n",
       "         -2.4978e-02, -3.3134e-02,  6.8759e-02,  3.2692e-02,  3.3648e-02,\n",
       "         -2.8723e-02,  8.5249e-03, -5.7247e-02, -1.0357e-01, -1.4365e-02,\n",
       "          6.1772e-02, -7.3953e-02, -3.5921e-04, -3.0949e-02, -1.3592e-01,\n",
       "         -7.2458e-02, -5.0496e-02, -1.5356e-02, -5.2733e-02, -1.0026e-01,\n",
       "         -1.4037e-01, -6.2274e-02, -7.1671e-02, -1.5139e-02, -8.2626e-02,\n",
       "         -4.7771e-02, -1.4454e-01,  3.6936e-02, -7.9435e-02, -3.5952e-02,\n",
       "         -1.0434e-01,  7.1274e-02,  3.2480e-02,  6.3918e-02,  2.8611e-02,\n",
       "         -1.6015e-02, -1.3967e-01, -1.6648e-02, -6.6715e-02, -7.7961e-02,\n",
       "         -8.4401e-03,  4.0578e-02,  1.0819e-01, -4.4659e-02,  1.1845e-02,\n",
       "          5.9508e-02,  1.6610e-02,  1.8677e-02, -1.1001e-01, -1.2207e-01,\n",
       "         -1.0786e-01, -9.6549e-02, -6.1122e-02, -3.2128e-02, -7.1845e-02,\n",
       "         -1.2359e-01, -2.4683e-02, -1.9532e-02,  1.5631e-02,  9.3122e-02,\n",
       "         -7.1852e-02, -2.3720e-02, -3.4551e-02, -2.2903e-02, -4.2727e-02,\n",
       "         -8.6009e-02, -9.0888e-02,  9.3918e-02, -5.9314e-02,  7.0650e-02,\n",
       "         -1.0035e-01,  1.4553e-02,  1.3380e-02,  1.0654e-01,  7.3451e-03,\n",
       "         -1.3439e-01,  1.1810e-01, -7.5076e-02, -3.9675e-02, -1.2730e-02,\n",
       "         -9.3441e-02, -7.8124e-02,  5.0937e-03, -4.6052e-02, -1.3498e-01,\n",
       "         -9.7142e-02, -7.0380e-02, -9.0184e-02, -5.4622e-02, -1.1805e-02,\n",
       "         -9.8805e-02,  9.0518e-03, -6.6125e-02, -7.6516e-02, -1.6316e-01,\n",
       "          9.1859e-02,  6.8594e-02, -1.1347e-01, -9.3453e-02, -1.6409e-02,\n",
       "          4.9831e-02,  5.9697e-02, -2.5936e-02,  3.3220e-01, -1.5791e-02,\n",
       "         -4.6808e-02,  1.6120e-03,  5.4064e-02, -1.0811e-01, -1.0923e-01,\n",
       "         -2.3789e-02,  1.1774e-01, -2.7513e-03, -1.0053e-01, -6.7293e-02,\n",
       "         -1.4769e-01, -5.7897e-02, -1.0835e-01, -3.9113e-03, -3.5699e-02,\n",
       "         -5.1963e-02, -9.8398e-02, -8.2750e-02, -6.5031e-02, -5.8932e-02,\n",
       "         -3.8372e-02, -3.2102e-02, -1.0399e-01, -2.4155e-02, -6.1841e-02,\n",
       "         -5.7919e-02,  1.0234e-03, -1.5286e-01, -1.6116e-01, -6.3931e-02,\n",
       "          5.4084e-02,  6.7514e-02, -4.2360e-02, -9.1926e-02, -3.0753e-02,\n",
       "          3.0540e-02, -6.8048e-02, -6.3836e-02, -1.0519e-01,  3.6709e-02,\n",
       "          2.3915e-02,  8.1677e-02,  6.4061e-02, -1.0920e-01,  6.5083e-02,\n",
       "         -7.1140e-02, -9.0670e-02,  5.8689e-02, -7.2455e-02, -8.5068e-02,\n",
       "         -6.9670e-02,  6.7066e-02,  2.5771e-02,  6.9231e-02, -1.5279e-02,\n",
       "         -6.2065e-01,  4.2855e-02, -1.0108e-01,  1.4958e-02, -5.1842e-02,\n",
       "         -2.9548e-02,  1.2663e-01, -1.9485e-02, -1.4611e-02, -7.0454e-02,\n",
       "         -1.5010e-01, -1.1479e-01, -2.0274e-02, -4.6186e-02,  3.7312e-02,\n",
       "         -4.6845e-03, -1.1243e-01,  5.7662e-03, -4.8168e-02,  4.1642e-02,\n",
       "         -1.2994e-01, -1.0522e-01,  5.8554e-02, -7.5463e-02, -1.3078e-01,\n",
       "          3.4147e-02,  3.6588e-02, -4.6127e-02, -3.3408e-02,  3.4817e-03,\n",
       "         -4.2363e-02, -4.2850e-02,  6.9876e-02, -4.7035e-02,  3.6683e-02,\n",
       "          2.3717e-02, -5.6663e-02, -2.3482e-02,  2.2695e-03, -7.1891e-02,\n",
       "          3.4086e-02, -3.9050e-02, -5.4687e-02, -1.0103e-02, -4.6291e-02,\n",
       "         -1.3215e-01, -4.2017e-03, -6.4696e-02,  7.4318e-02, -3.7038e-02,\n",
       "         -7.4783e-02,  3.7466e-03, -4.4902e-02, -5.4526e-02, -3.2154e-02,\n",
       "         -6.9844e-02, -1.7045e-02, -9.5056e-02,  5.8156e-02, -1.0002e-01,\n",
       "         -2.9361e-03,  5.3961e-02, -1.5724e-02,  1.2291e-01, -7.9840e-02,\n",
       "         -6.2188e-02, -3.2282e-02,  8.2793e-03, -1.1508e-01, -1.4068e-01,\n",
       "         -5.5942e-02, -1.2841e-01, -4.0451e-02, -4.0718e-02, -6.1471e-02,\n",
       "         -8.3071e-02, -1.5063e-02, -2.3455e-02,  1.1974e-02,  7.6468e-02,\n",
       "         -6.5692e-02, -1.8200e-02, -1.4435e-02, -6.2212e-02, -1.2290e-01,\n",
       "         -1.7033e-01, -2.5049e-02, -9.7031e-02,  7.9129e-03, -3.5679e-02,\n",
       "         -6.2448e-03,  5.5563e-02, -9.0274e-02,  3.9126e-02, -1.1295e-01,\n",
       "         -5.1131e-02,  5.9279e-03, -9.5505e-03, -6.1520e-02, -2.4447e-02,\n",
       "         -1.8512e-02,  8.4790e-02, -9.8691e-02, -8.7056e-02, -1.1217e-01,\n",
       "         -1.5344e-01,  5.8255e-02, -1.4153e-01, -7.1881e-03, -1.1657e-03,\n",
       "         -7.4974e-02,  4.0591e-02, -8.0208e-02,  3.8199e-02, -5.8486e-04,\n",
       "          5.0867e-02,  3.5725e-03, -1.4645e-02, -2.7721e-04, -5.1357e-02,\n",
       "         -5.7399e-02, -8.0470e-03, -6.7928e-02,  2.5977e-02, -1.1493e-01,\n",
       "         -2.4930e-02, -1.7211e-02, -1.4713e-01, -2.2575e+00,  3.7316e-02,\n",
       "         -1.8979e-02, -1.0595e-01, -1.0567e-02, -7.6957e-02,  2.2319e-02,\n",
       "         -1.2596e-01, -5.9056e-02,  3.7224e-02,  1.6287e-02, -6.8733e-02,\n",
       "         -4.9052e-02,  4.2215e-02, -6.1539e-03, -9.2575e-02, -2.2745e-02,\n",
       "         -4.3827e-02,  1.5006e-02,  2.1218e-02, -5.9763e-02, -1.6321e-01,\n",
       "         -1.3407e-01,  7.0248e-02, -2.2305e-02,  4.9774e-02, -7.5658e-02,\n",
       "         -5.4044e-02, -1.0192e-01, -7.0567e-02, -4.1365e-02, -1.0061e-01,\n",
       "         -1.5344e-02, -2.0617e-02, -7.9850e-02, -9.0090e-02, -1.2089e-03,\n",
       "         -1.7109e-02, -1.2406e-01, -1.1144e-01, -1.0369e-01, -7.0175e-02,\n",
       "         -5.0133e-02, -1.3557e-02,  1.6262e-01, -9.9455e-02, -7.2051e-02,\n",
       "         -1.1553e-01,  3.9777e-03,  5.5598e-02, -1.8694e-02, -9.6043e-04,\n",
       "          6.9841e-02, -7.7826e-02,  5.5478e-02, -4.6886e-02, -7.8571e-03,\n",
       "         -1.7885e-02, -1.2144e-01,  3.0650e-02,  5.2436e-03, -8.6549e-02,\n",
       "         -1.2067e-01, -1.2578e-01, -3.0886e-02, -1.4648e-01, -1.3343e-01,\n",
       "         -9.9811e-02,  1.0904e-01,  5.2390e-02, -6.2593e-02,  7.3330e-02,\n",
       "         -8.2041e-02, -2.8029e-01, -1.3285e-02, -1.2791e-01,  2.9784e-02,\n",
       "         -4.9950e-02,  1.3312e-02, -2.3558e-04, -1.0833e-01, -9.1062e-02,\n",
       "         -3.6797e-02,  4.6885e-02, -8.6031e-03, -7.7853e-02, -4.7832e-02,\n",
       "          2.6897e-02, -4.3142e-02, -8.8117e-02,  1.3069e-02, -7.0432e-02,\n",
       "         -3.8987e-02, -9.9468e-02,  4.2137e-02,  2.7479e-02, -6.7665e-02,\n",
       "         -4.4414e-02,  7.5395e-03, -2.0309e-03,  4.0419e-02, -1.4872e-01,\n",
       "         -8.2230e-02, -6.8386e-02, -6.8401e-02, -4.1047e-02, -8.8755e-02,\n",
       "          5.6166e-02,  5.6017e-02, -5.5956e-02,  2.1983e-02, -2.2961e-02,\n",
       "         -7.1032e-03, -7.5360e-02,  6.5075e-02, -9.5735e-02,  4.0330e-02,\n",
       "          7.4802e-02,  8.9258e-03,  2.9245e-02, -1.5354e-02,  6.8174e-02,\n",
       "         -1.0322e-01, -2.5460e-03, -1.0041e-01, -1.2218e-02, -8.0847e-02,\n",
       "         -6.2028e-02,  6.3530e-02,  8.3978e-03, -5.7599e-02, -1.0613e-01,\n",
       "         -2.3226e-02,  5.4509e-02,  1.6102e-02, -4.9709e-02, -1.4171e-01,\n",
       "         -1.9604e-01, -9.9754e-02, -9.7179e-02, -8.0188e-02,  3.6264e-02,\n",
       "          3.3763e-02, -1.4644e-02,  1.3093e-01,  8.6540e-03,  2.5184e-02,\n",
       "          1.3682e-02, -9.1370e-02, -6.9037e-04, -2.6415e-02, -6.0097e-02,\n",
       "         -3.5762e-03, -4.9125e-02, -8.7560e-02,  5.8042e-02,  8.4096e-02,\n",
       "          3.6079e-03, -5.9528e-02, -7.0748e-02,  3.0864e-02, -1.0618e-01,\n",
       "         -8.6109e-02, -4.3467e-02, -7.9412e-03,  1.1864e-01, -8.4393e-02,\n",
       "         -6.7752e-02, -1.3682e-02,  1.3190e-01, -4.1765e-02, -9.5315e-02,\n",
       "         -8.5517e-02, -1.0381e-02,  7.4308e-03,  2.5057e-02, -1.2178e-01,\n",
       "         -1.0872e-01, -3.0953e-02, -6.2515e-03, -6.9138e-02, -1.0413e-01,\n",
       "         -9.2249e-02, -8.9106e-03,  1.4600e-01, -1.2124e-01, -8.7356e-02,\n",
       "         -1.2041e-01, -1.1749e-01, -3.5789e-02,  3.8827e-02, -8.9954e-03,\n",
       "         -2.3699e-02, -7.3208e-02, -8.5094e-02, -1.4362e-01, -5.3073e-02,\n",
       "          1.5437e-02, -8.0135e-02,  1.2968e-02, -2.8527e-02, -1.8904e-02,\n",
       "         -1.3154e-01,  3.9962e-02,  7.3063e-02, -2.0348e-01,  3.3176e-02,\n",
       "         -4.0622e-02, -7.5475e-02,  6.5075e-02, -1.1102e-01, -6.7218e-02,\n",
       "         -7.5741e-02,  2.4920e-02, -1.2435e-02, -1.9510e-02, -1.4103e-02,\n",
       "         -6.3889e-02,  1.4682e-01, -1.2999e-01, -4.1460e-02, -1.1495e-01,\n",
       "         -1.9698e-02, -1.1444e-01, -3.0427e-02,  4.8089e-02, -6.2266e-02,\n",
       "         -7.7924e-02,  2.4362e-04, -1.4317e-01, -3.6246e-02, -1.2393e-01,\n",
       "          4.1629e-02,  8.1272e-02, -8.8061e-02, -7.7921e-02, -8.0148e-02,\n",
       "         -9.8167e-02, -8.5414e-02, -1.5176e-02,  1.9671e-02,  5.1377e-02,\n",
       "         -1.5865e-02, -1.1811e-01, -7.8402e-02,  2.3705e-02,  2.9540e-02,\n",
       "         -8.7734e-02, -8.4668e-02,  9.9336e-02, -4.6445e-02, -1.6185e-01,\n",
       "         -1.3077e-01, -4.2604e-02,  5.8688e-02,  1.1358e-02, -1.2765e-01,\n",
       "         -6.1923e-02, -8.4753e-02, -3.0332e-02, -6.0014e-04, -7.5688e-02,\n",
       "         -1.0452e-01, -8.2666e-03,  4.5868e-02, -3.2654e-02, -4.4266e-02,\n",
       "         -5.2327e-02,  8.4792e-02, -7.2354e-02,  2.3774e-02, -3.6025e-02,\n",
       "         -1.1545e-01, -2.8853e-02, -2.0315e-02,  2.3356e-02, -5.5108e-02,\n",
       "         -3.2399e-02,  1.0912e-01, -1.3768e-01, -9.9473e-02, -1.2217e-02,\n",
       "         -9.2016e-02, -3.4299e-02,  1.1813e-02, -2.0268e-01, -1.2499e-02,\n",
       "          3.3174e-02, -1.0902e-01, -2.4698e-02, -1.8718e-02, -1.3656e-02,\n",
       "         -1.8760e-02, -5.9220e-02, -2.1743e-02, -5.2343e-02,  6.8899e-02,\n",
       "          3.8413e-02, -1.1812e-01,  4.0992e-02, -8.7708e-02,  6.0118e-02,\n",
       "          1.1129e-01, -2.3863e-03, -2.4479e-02,  5.5275e-02, -4.8966e-02,\n",
       "         -1.3996e-01, -1.2337e-01,  5.8795e-03,  1.5241e-02, -1.1743e-01,\n",
       "          2.0023e-02, -1.0781e-02, -8.2880e-02, -6.0243e-02,  9.4822e-03,\n",
       "         -1.2696e-01,  4.8240e-03, -6.5649e-02,  6.8928e-03, -5.0116e-02,\n",
       "          8.7673e-02,  3.1290e-02, -2.8279e-02,  1.1816e-02, -7.9418e-03,\n",
       "          3.5138e-02, -3.7828e-02,  1.0395e-01,  5.2070e-02,  1.1879e-02,\n",
       "         -3.6229e-02,  5.4177e-02, -1.7798e-01, -9.3546e-02,  3.3682e-02,\n",
       "         -1.0662e-01, -3.3063e-02,  2.0191e-02,  5.7537e-02, -1.8093e-02,\n",
       "         -9.0532e-02,  9.5468e-02, -7.4289e-02, -2.5821e-02, -2.6561e-02,\n",
       "         -4.5162e-02,  2.9504e-02, -3.1473e-02,  2.8452e-02, -2.9851e-02,\n",
       "         -2.8579e-02, -9.0557e-02,  5.2943e-03, -5.8344e-02,  4.7541e-02,\n",
       "         -2.5308e-02, -1.0288e-01, -4.9240e-02, -5.8421e-02,  1.6592e-02,\n",
       "          1.2111e-01, -5.4313e-02, -8.1466e-02, -4.9400e-02,  7.0303e-03,\n",
       "         -4.2782e-02, -4.3525e-02, -8.5807e-03,  6.2815e-02,  5.9304e-02,\n",
       "         -1.0129e-01, -6.6303e-02, -7.5956e-03, -6.3686e-02, -2.5517e-02,\n",
       "         -2.0086e-02, -6.0147e-02, -2.3865e-02, -3.3763e-02, -7.4924e-02,\n",
       "         -1.3382e-01, -1.6790e-01, -1.3245e-01, -3.7097e-02,  2.2390e-02,\n",
       "         -1.0047e-01, -8.1847e-02, -7.7880e-02,  5.0404e-02, -1.4665e-01,\n",
       "          5.9582e-02, -1.1912e-02, -1.1096e-01, -1.0069e-01,  6.5853e-02,\n",
       "          1.5271e-02,  4.4003e-02,  1.9579e-02,  4.8464e-02,  6.3348e-02,\n",
       "         -6.8227e-02,  3.1810e-02, -3.2205e-02,  8.8901e-02, -5.0658e-02,\n",
       "         -1.0127e-01, -1.6246e-01,  1.6157e-02, -4.4182e-02, -6.6988e-02,\n",
       "         -5.1723e-01,  1.4864e-02, -7.6332e-02, -7.5355e-03,  2.3505e-02,\n",
       "         -4.6339e-02, -1.4055e-02, -1.6824e-01,  7.3489e-03, -3.3287e-02,\n",
       "         -6.3824e-04,  5.9921e-03, -8.3247e-02,  9.5896e-02,  3.1140e-02,\n",
       "         -5.0645e-02, -1.2309e-01,  2.1778e-02, -8.4379e-02, -7.0738e-02,\n",
       "         -8.9048e-02, -4.2593e-02, -1.2673e-01, -1.4987e-01,  8.0047e-02,\n",
       "          7.0084e-03, -8.5599e-02, -6.0880e-02, -5.7156e-02, -8.4430e-03,\n",
       "         -6.1077e-02, -9.3670e-02, -6.0107e-02, -5.3923e-02, -5.4385e-02,\n",
       "         -1.1415e-01, -1.8597e-01, -2.0845e-02, -4.5953e-02,  1.9309e-02,\n",
       "          9.4666e-02, -1.1881e-01, -4.2700e-03, -1.2863e-01, -4.4709e-02,\n",
       "         -4.3273e-02,  5.3781e-02,  2.5261e-02], device='cuda:0'),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0587,  0.0547, -0.0215,  ...,  0.0323,  0.0472, -0.0258],\n",
       "         [-0.0112, -0.0313, -0.0500,  ...,  0.0239,  0.0018,  0.0107],\n",
       "         [ 0.0021,  0.0415, -0.0370,  ...,  0.0286,  0.0118, -0.0310],\n",
       "         ...,\n",
       "         [ 0.0422,  0.0793,  0.0202,  ...,  0.0414, -0.0607,  0.0131],\n",
       "         [-0.0060,  0.0452,  0.0393,  ..., -0.0117,  0.0522,  0.0407],\n",
       "         [-0.0437, -0.0512,  0.0376,  ...,  0.0037,  0.0824, -0.0069]],\n",
       "        device='cuda:0'),\n",
       " Parameter containing:\n",
       " tensor([-0.0976, -0.0618, -0.0515,  ..., -0.1151, -0.0466, -0.1224],\n",
       "        device='cuda:0'),\n",
       " Parameter containing:\n",
       " tensor([[-0.0224,  0.0540, -0.0089,  ..., -0.0939, -0.0108,  0.0464],\n",
       "         [ 0.0107, -0.0081, -0.0156,  ...,  0.0101, -0.0167,  0.0185],\n",
       "         [-0.0136, -0.0597, -0.0227,  ...,  0.0007,  0.0513,  0.0980],\n",
       "         ...,\n",
       "         [-0.0232, -0.0076,  0.0177,  ..., -0.0031, -0.0152,  0.0120],\n",
       "         [-0.0114,  0.0444, -0.0052,  ..., -0.0605, -0.0259,  0.0889],\n",
       "         [-0.0367, -0.0215,  0.0213,  ..., -0.0128, -0.0067,  0.0139]],\n",
       "        device='cuda:0'),\n",
       " Parameter containing:\n",
       " tensor([-6.5741e-02,  4.2078e-02,  1.2011e-02,  2.2932e-03,  5.5518e-02,\n",
       "          4.5913e-02,  2.2022e-02, -5.5168e-02, -2.1741e-02, -1.0107e-01,\n",
       "         -9.8937e-02,  5.2764e-02, -7.3483e-02,  8.6208e-02,  6.0888e-02,\n",
       "          1.2144e-01,  1.0733e-02,  4.1013e-03,  2.8674e-02, -3.3970e-02,\n",
       "          2.2772e-02, -6.7903e-03,  4.9564e-03,  1.4364e-02,  2.9709e-02,\n",
       "         -6.4162e-03,  5.6278e-02,  6.3540e-02, -1.5162e-02, -1.7254e-02,\n",
       "         -2.8550e-04,  4.7133e-02,  3.4972e-02, -7.6128e-02,  2.4526e-02,\n",
       "         -2.5571e-02, -3.0283e-02, -5.3822e-03,  2.4152e-02, -5.9109e-02,\n",
       "         -4.5285e-03, -1.7886e-02, -4.1091e-02,  1.0376e-02,  4.9845e-02,\n",
       "          4.7674e-02, -8.8754e-02,  7.5486e-02, -1.0166e-01, -7.9692e-02,\n",
       "          4.3310e-03,  5.8896e-02,  7.9482e-02,  2.7634e-03, -2.2596e-02,\n",
       "          6.1754e-02, -2.9951e-02,  6.2471e-02,  3.7189e-02,  4.2943e-02,\n",
       "         -1.5058e-02, -2.4581e-02,  6.8931e-02, -8.8080e-03, -2.2560e-02,\n",
       "         -5.4985e-02,  5.4128e-02,  7.5724e-02,  2.8788e-02,  4.6660e-02,\n",
       "         -4.4814e-03,  3.1812e-02, -3.7511e-02, -8.0002e-02,  1.0749e-01,\n",
       "         -6.5850e-02,  4.5588e-02,  2.3601e-02,  5.3216e-02,  8.7131e-02,\n",
       "          7.3938e-02, -3.2171e-02,  2.5774e-02,  9.8898e-03,  2.4024e-03,\n",
       "         -6.3372e-02, -9.0037e-02, -3.0310e-02, -1.5730e-02,  9.7426e-02,\n",
       "         -7.6012e-02, -5.0295e-03,  8.4208e-02,  5.3904e-02,  3.9876e-02,\n",
       "         -1.5031e-01,  1.3342e-02,  5.5709e-03,  8.6004e-02, -3.2644e-02,\n",
       "         -6.3840e-02, -4.0073e-02,  1.4759e-03, -2.9463e-03, -1.3734e-01,\n",
       "         -7.9349e-02, -5.9826e-02, -7.3977e-02, -7.9907e-02, -9.2668e-02,\n",
       "         -2.6027e-02,  2.4271e-02, -9.1017e-02,  1.4410e-02, -5.3724e-02,\n",
       "          1.5609e-02,  4.0855e-02,  4.5932e-02, -4.2466e-02, -3.3923e-02,\n",
       "          5.1573e-02,  1.4564e-01,  3.8452e-02, -3.4191e-03,  4.3361e-02,\n",
       "          3.6211e-02,  5.3221e-02, -2.2619e-02, -1.5577e-02, -7.8316e-02,\n",
       "         -1.2720e-02,  6.6331e-02,  7.5486e-03, -5.7261e-03, -1.3333e-02,\n",
       "         -8.1408e-02,  4.1948e-02,  1.4382e-02,  8.5527e-02, -3.4351e-02,\n",
       "         -5.0566e-02, -4.0421e-02, -1.1631e-01,  3.7037e-02,  6.9312e-02,\n",
       "          6.3102e-03, -7.9192e-02, -1.0396e-01, -9.4308e-02,  1.1846e-01,\n",
       "         -6.8122e-02,  6.9707e-02,  8.7248e-03,  7.6877e-04, -4.4632e-02,\n",
       "          8.9779e-02, -5.9622e-03,  3.3064e-02, -1.0618e-02,  2.4632e-02,\n",
       "          4.5643e-02, -7.5217e-03,  5.2801e-02, -3.7286e-02,  2.0312e-03,\n",
       "          2.8662e-02,  3.2067e-02, -1.7333e-02,  4.4542e-02,  3.4369e-02,\n",
       "         -7.0471e-02, -4.3833e-03, -2.4420e-02, -5.7783e-02, -2.8720e-02,\n",
       "         -1.1356e-02, -2.3630e-03, -2.0051e-03, -7.4737e-02, -8.6264e-03,\n",
       "         -1.6157e-01,  3.9566e-03, -3.2327e-02, -5.7044e-02, -6.0499e-02,\n",
       "          2.5157e-02,  5.5642e-02, -2.2992e-02,  7.6248e-02,  1.8392e-02,\n",
       "          4.4709e-02, -9.9869e-02,  5.4718e-02, -4.5948e-04,  6.2032e-02,\n",
       "         -7.7357e-02,  1.8792e-02,  4.1057e-02,  6.4767e-02,  4.7432e-02,\n",
       "         -9.0086e-02, -2.4933e-02,  4.9760e-02, -9.5229e-02,  2.1131e-02,\n",
       "          1.2652e-01,  2.1365e-02,  3.0726e-02,  3.9724e-02,  2.1305e-02,\n",
       "          1.0902e-02, -5.3310e-02, -5.6873e-03,  1.0282e-02,  9.9563e-02,\n",
       "         -8.4239e-02, -2.7006e-02, -1.1546e-01,  4.0102e-02, -8.9660e-02,\n",
       "          4.6134e-02,  4.1681e-02,  9.0483e-04,  4.2167e-03,  7.7366e-03,\n",
       "          5.3710e-02,  2.2877e-02, -3.4592e-02,  4.5413e-02, -9.4250e-02,\n",
       "         -5.7224e-02,  1.9963e-02,  3.3544e-02,  2.6429e-02,  4.3978e-02,\n",
       "          7.0906e-02, -1.4172e-02,  3.1707e-02,  1.1556e-01,  2.2510e-02,\n",
       "          1.3776e-01,  3.4043e-03,  1.9217e-02,  9.7301e-02, -9.2280e-02,\n",
       "         -1.0608e-02,  5.2683e-02, -1.4630e-03, -2.5501e-02,  1.8408e-02,\n",
       "         -1.0153e-02, -5.9191e-02, -6.9154e-04, -5.0810e-02, -5.9029e-03,\n",
       "         -3.4107e-02, -8.1468e-02, -4.7834e-02,  5.1254e-02,  1.1035e-01,\n",
       "         -7.3325e-02, -2.8772e-02, -2.7043e-02, -1.8885e-02,  5.3347e-02,\n",
       "         -7.9410e-03, -7.5070e-02, -4.7684e-02,  5.5057e-02,  8.8665e-02,\n",
       "          7.7637e-02,  1.4622e-02,  7.4473e-03, -1.2827e-01,  2.7860e-02,\n",
       "          9.5059e-02,  6.5501e-03, -1.1090e-02, -5.4270e-02, -2.4547e-02,\n",
       "          7.8927e-03,  2.2875e-02,  3.0793e-03, -1.2587e-02, -7.1923e-02,\n",
       "         -3.4396e-02,  7.3324e-02, -8.3631e-02, -6.4532e-02,  8.5521e-02,\n",
       "          4.4709e-02, -1.5898e-02, -2.5457e-02,  1.0278e-01,  3.4183e-02,\n",
       "          6.7814e-03,  3.5959e-03,  7.0658e-02, -4.9623e-02, -3.2984e-03,\n",
       "          2.4855e-03,  4.9145e-02, -9.1689e-03,  1.4704e-02,  2.1152e-02,\n",
       "         -8.3013e-02, -7.5539e-02, -7.5281e-02, -5.7309e-01, -6.0819e-02,\n",
       "         -8.5979e-02, -1.9025e-02,  4.4833e-02, -2.6851e-02, -1.3231e-02,\n",
       "         -1.7288e-02, -3.0165e-02,  3.5889e-02, -5.5218e-02,  4.3945e-02,\n",
       "         -4.3719e-02,  9.3420e-03,  3.9414e-02,  6.3030e-02, -8.1831e-02,\n",
       "         -3.7091e-02,  3.6320e-02,  1.1276e-01, -1.1581e-02, -5.2862e-02,\n",
       "         -1.2598e-02, -3.1647e-02,  4.8075e-03,  1.4833e-02, -6.5028e-02,\n",
       "         -7.6895e-03, -7.3512e-02, -2.5486e-02, -4.1813e-02,  1.5875e-04,\n",
       "          2.7444e-02, -7.8871e-02,  6.8268e-02, -6.3687e-03,  2.2613e-02,\n",
       "         -3.2094e-02,  4.0743e-02,  2.3803e-02, -4.9665e-02, -3.8302e-03,\n",
       "          1.7203e-03, -6.6815e-02,  2.7893e-02,  2.8895e-02,  1.1447e-02,\n",
       "         -2.9330e-02, -2.4495e-02,  7.2686e-02, -6.2831e-02, -3.8198e-02,\n",
       "          5.3170e-02, -2.2470e-03,  1.3094e-02, -2.8768e-03, -5.7709e-02,\n",
       "          1.2637e-02,  3.5921e-02,  7.7178e-02, -1.2786e-02,  6.8424e-02,\n",
       "          4.2647e-02, -3.6230e-02,  6.6335e-02,  1.8345e-02,  5.5026e-02,\n",
       "         -1.6044e-02,  9.5089e-02, -5.7743e-02, -3.9953e-03, -5.6321e-02,\n",
       "          1.7654e-02, -1.1355e-01, -2.0111e-02, -1.2025e-02, -2.2342e-02,\n",
       "         -3.8111e-02, -1.8493e-02,  4.7788e-02, -1.1700e-01,  2.9329e-02,\n",
       "          1.0777e-01,  8.1691e-02,  1.0103e-02, -3.9286e-02,  4.3974e-02,\n",
       "         -1.0193e-01,  4.6519e-04,  3.4982e-02,  1.7681e-02, -2.2531e-02,\n",
       "          5.9354e-03, -4.4545e-02,  8.4809e-02,  1.2845e-02,  7.2413e-03,\n",
       "         -8.9966e-02, -2.0195e-02, -5.5660e-02,  5.1682e-03, -2.9537e-02,\n",
       "          3.0734e-02, -6.4153e-02, -8.2416e-02, -4.1658e-02,  3.5524e-02,\n",
       "          8.4056e-02,  2.6158e-02, -7.0579e-02,  5.0375e-02, -1.5945e-02,\n",
       "         -3.6871e-02, -5.4097e-02,  7.0046e-02, -5.9001e-02, -6.6906e-02,\n",
       "          2.8931e-02,  2.1156e-02,  1.4465e-02, -4.2688e-02,  3.0843e-02,\n",
       "         -9.5841e-02,  3.1049e-02, -1.7267e-02,  5.8211e-02, -1.1215e-02,\n",
       "          2.5011e-02, -5.2271e-02, -7.2524e-03,  4.1636e-02, -1.2922e-02,\n",
       "          3.3012e-02, -1.3943e-01, -2.3983e-02, -1.0884e-01, -4.5540e-02,\n",
       "          8.2418e-03, -1.1399e-02,  1.9262e-02,  3.5149e-02,  2.1226e-02,\n",
       "          4.1662e-02,  5.5880e-02,  5.5789e-02,  7.1931e-02,  1.1613e-02,\n",
       "         -3.0148e-02, -6.4731e-03,  1.6994e-02,  4.7024e-02, -1.6237e-02,\n",
       "          7.8670e-02, -3.5580e-02, -9.2719e-03, -8.0301e-02,  7.5849e-02,\n",
       "          1.6408e-03, -1.3350e-02, -1.3303e-02, -1.0529e-02, -6.1056e-02,\n",
       "         -4.6966e-02, -2.0982e-02, -1.9628e-02,  1.8064e-02, -5.4067e-02,\n",
       "         -4.4243e-02, -1.2639e-02, -3.5794e-03,  5.3594e-02, -6.1120e-02,\n",
       "         -3.5429e-02,  2.9079e-02, -1.0818e-02, -2.0240e-02, -4.4640e-02,\n",
       "          5.4709e-02,  5.5514e-02,  4.5176e-02,  5.2774e-02,  2.0449e-03,\n",
       "          2.4148e-02,  7.5699e-02,  9.8555e-03, -1.4599e-02, -3.2677e-02,\n",
       "         -1.8330e-02, -4.1715e-02,  3.6530e-02,  7.1768e-02,  2.3452e-03,\n",
       "          1.4440e-03,  9.1412e-02, -6.8613e-02, -1.9729e-02, -5.9021e-02,\n",
       "          2.3761e-03, -4.1591e-02, -1.1875e-02, -5.7171e-02, -3.9905e-02,\n",
       "         -1.8021e-01,  1.4446e-02, -2.0283e-02, -3.4604e-02,  1.0934e-01,\n",
       "          9.3487e-02, -1.6994e-02, -2.2198e-02, -7.0739e-02, -3.3051e-02,\n",
       "          3.6369e-03,  2.6813e-02,  3.7742e-02,  5.3021e-03, -2.6763e-02,\n",
       "         -1.9912e-02,  7.5408e-02, -1.4839e-01,  5.4851e-02, -2.8359e-02,\n",
       "         -8.4575e-02, -7.1482e-02, -3.5255e-02,  4.3687e-02, -7.3170e-03,\n",
       "         -1.0373e-01,  1.8857e-02, -1.6432e-03,  4.4577e-02, -1.0060e-01,\n",
       "          4.6177e-02,  5.7075e-02,  1.3574e-01, -3.1414e-02, -1.5419e-02,\n",
       "          6.4323e-02, -1.9411e-02,  9.2282e-02,  5.1881e-02, -2.1687e-02,\n",
       "         -5.2754e-03, -1.7284e-02, -7.6404e-02, -7.7567e-03,  3.9789e-03,\n",
       "         -6.3117e-02, -7.0600e-03,  8.3449e-02,  4.4994e-02, -6.5175e-04,\n",
       "         -1.2513e-02,  2.4039e-02, -1.7452e-02,  8.6102e-02,  9.3158e-03,\n",
       "          1.5504e-02,  1.6610e-02,  3.5816e-02,  5.1665e-02, -2.8690e-03,\n",
       "         -5.9123e-02,  1.6995e-02,  2.7976e-02,  9.5845e-02, -2.8796e-02,\n",
       "         -3.9649e-02,  2.7449e-02,  4.1449e-02,  2.4183e-02, -5.3613e-02,\n",
       "         -1.2956e-02,  7.5293e-02,  2.5709e-02, -4.0720e-02,  1.0458e-03,\n",
       "         -2.5551e-02,  2.9039e-02,  1.0784e-02,  1.2268e-01,  3.7313e-02,\n",
       "         -2.2168e-02,  8.9978e-02,  4.8200e-02, -1.3578e-02,  2.6823e-02,\n",
       "          1.4709e-02, -1.7293e-02,  7.1181e-02, -2.5049e-02,  6.0888e-02,\n",
       "         -1.3517e-02, -1.1441e-02, -4.5996e-02, -8.6260e-02,  5.1932e-02,\n",
       "          1.0900e-01, -5.6022e-02, -7.5737e-03, -1.9261e-02,  3.7798e-02,\n",
       "          2.1514e-02,  4.5055e-02,  3.1432e-02, -4.1821e-02,  5.2410e-02,\n",
       "         -4.6340e-02, -5.2976e-02, -1.7671e-02,  6.4157e-02, -1.4843e-01,\n",
       "          8.2264e-03,  4.8591e-03, -5.7576e-02,  1.2325e-02,  2.7476e-02,\n",
       "         -1.5675e-02, -3.9709e-03,  1.9007e-02,  9.8442e-03, -2.1851e-02,\n",
       "          9.1209e-03, -4.7204e-02,  6.9091e-02,  4.1224e-02, -5.7185e-03,\n",
       "         -2.4363e-02,  4.1596e-02,  6.0440e-02,  6.6958e-02, -1.5571e-02,\n",
       "         -6.2780e-02,  2.3440e-02, -6.9045e-02, -9.7530e-02,  8.4682e-04,\n",
       "         -1.2317e-01, -2.8132e-02,  1.5301e-02,  6.0423e-03,  4.2853e-02,\n",
       "          3.2683e-02,  3.7758e-03,  1.2861e-02,  3.9315e-02, -5.9100e-02,\n",
       "         -2.7530e-02, -1.8280e-02,  1.0072e-02,  3.5076e-02, -6.3968e-02,\n",
       "         -5.9066e-02, -1.2943e-01, -8.5690e-03, -3.7181e-02,  4.0224e-02,\n",
       "         -2.3463e-02,  2.0499e-02, -7.8721e-02,  1.6681e-02,  9.0797e-02,\n",
       "          9.1372e-02, -1.5501e-04, -2.4964e-02, -2.6653e-02,  7.8210e-02,\n",
       "          5.0348e-02,  6.5885e-03,  7.9732e-02, -5.8013e-02,  4.9909e-02,\n",
       "          5.5382e-02, -2.6498e-02,  5.4312e-02,  2.9646e-02, -1.8378e-02,\n",
       "         -1.3659e-02,  3.9050e-02,  3.0359e-02,  6.9679e-02, -2.7579e-02,\n",
       "         -5.6372e-02, -9.0979e-02, -8.0618e-03, -7.0255e-02,  1.0123e-02,\n",
       "          7.2885e-02, -5.6820e-02, -6.6698e-02,  9.6192e-02, -6.9482e-02,\n",
       "          6.3897e-02,  5.1526e-02, -6.7743e-02,  3.1714e-02,  5.6532e-02,\n",
       "          7.8241e-02, -1.0519e-02,  7.6105e-02,  1.2452e-01,  8.5340e-02,\n",
       "         -3.3119e-02, -1.7505e-02, -6.9945e-02,  6.2715e-02, -2.0161e-02,\n",
       "          8.1423e-03, -2.7961e-03,  6.6570e-02,  1.3823e-01, -4.0674e-02,\n",
       "         -3.4993e-02,  2.4117e-02,  1.5082e-02,  6.2326e-02,  7.5106e-02,\n",
       "          4.3886e-03, -6.8087e-02, -4.5146e-02, -9.0466e-04, -4.9335e-02,\n",
       "          1.3295e-03, -6.4656e-03,  2.4747e-02,  3.9950e-02,  6.8273e-02,\n",
       "         -3.3840e-02,  3.3392e-02,  9.5674e-02,  2.1115e-02, -8.5006e-03,\n",
       "          1.8209e-02, -4.2123e-02, -5.6538e-02, -2.2504e-02,  1.8652e-02,\n",
       "         -2.2379e-02, -3.0439e-02, -9.4725e-02, -8.1087e-02, -2.8831e-02,\n",
       "          2.8818e-02,  5.2690e-02, -1.8964e-01, -5.8527e-02, -9.2450e-02,\n",
       "          9.2485e-03, -4.2063e-02,  8.4913e-02,  2.1387e-02, -4.8710e-02,\n",
       "         -4.6535e-02,  1.5636e-02,  3.8541e-02, -5.6517e-05, -6.6900e-02,\n",
       "          5.6160e-02,  3.0651e-02,  4.5752e-02], device='cuda:0'),\n",
       " Parameter containing:\n",
       " tensor([0.6384, 0.6300, 0.6657, 0.6127, 0.6376, 0.6371, 0.6570, 0.6331, 0.6264,\n",
       "         0.6274, 0.6279, 0.6530, 0.6055, 0.6508, 0.6297, 0.6216, 0.6509, 0.6025,\n",
       "         0.6437, 0.6300, 0.6269, 0.6288, 0.6218, 0.6324, 0.6286, 0.6211, 0.6202,\n",
       "         0.6334, 0.6525, 0.6213, 0.6393, 0.6333, 0.6316, 0.6377, 0.6364, 0.6289,\n",
       "         0.6342, 0.6112, 0.6209, 0.6371, 0.6302, 0.6346, 0.6272, 0.6218, 0.6665,\n",
       "         0.6390, 0.6836, 0.6314, 0.6834, 0.6381, 0.6403, 0.6628, 0.6905, 0.6079,\n",
       "         0.6166, 0.6367, 0.6341, 0.6005, 0.6061, 0.6538, 0.6669, 0.6125, 0.6143,\n",
       "         0.6157, 0.6322, 0.6397, 0.6320, 0.6501, 0.6304, 0.6317, 0.6289, 0.6107,\n",
       "         0.6380, 0.6405, 0.6143, 0.6522, 0.6047, 0.6076, 0.6453, 0.6347, 0.6345,\n",
       "         0.6315, 0.6294, 0.6553, 0.6169, 0.6251, 0.6510, 0.6221, 0.6401, 0.6853,\n",
       "         0.6214, 0.6307, 0.6428, 0.6185, 0.6171, 0.6550, 0.6437, 0.6276, 0.6232,\n",
       "         0.6212, 0.6536, 0.6362, 0.6159, 0.6311, 0.6680, 0.6267, 0.6330, 0.6212,\n",
       "         0.6301, 0.5318, 0.6436, 0.6386, 0.6399, 0.6273, 0.6604, 0.6951, 0.6258,\n",
       "         0.6358, 0.6286, 0.6141, 0.6586, 0.6308, 0.6431, 0.6485, 0.6303, 0.6178,\n",
       "         0.6410, 0.6356, 0.6644, 0.6729, 0.6194, 0.6183, 0.6338, 0.6260, 0.6120,\n",
       "         0.6670, 0.6350, 0.6337, 0.6329, 0.6186, 0.6261, 0.6323, 0.5352, 0.6262,\n",
       "         0.6402, 0.6480, 0.6291, 0.6221, 0.6568, 0.6198, 0.6312, 0.6352, 0.6188,\n",
       "         0.6464, 0.6209, 0.6688, 0.6608, 0.6192, 0.6645, 0.6005, 0.6289, 0.6210,\n",
       "         0.6187, 0.6294, 0.6319, 0.6159, 0.6647, 0.6350, 0.6330, 0.6304, 0.6271,\n",
       "         0.6352, 0.6252, 0.6280, 0.6691, 0.6531, 0.7134, 0.6341, 0.6153, 0.6190,\n",
       "         0.4560, 0.6633, 0.6192, 0.6314, 0.6253, 0.6157, 0.6236, 0.6331, 0.6351,\n",
       "         0.6228, 0.6346, 0.6620, 0.6803, 0.6246, 0.6366, 0.6230, 0.6173, 0.7476,\n",
       "         0.6356, 0.6460, 0.6094, 0.6203, 0.6758, 0.6076, 0.6733, 0.5315, 0.6364,\n",
       "         0.6397, 0.6674, 0.6287, 0.6353, 0.6318, 0.6663, 0.6376, 0.6200, 0.6303,\n",
       "         0.6224, 0.6362, 0.6235, 0.6183, 0.6421, 0.5906, 0.6244, 0.6254, 0.6406,\n",
       "         0.6402, 0.6670, 0.6475, 0.6141, 0.6419, 0.6289, 0.6276, 0.6012, 0.6322,\n",
       "         0.6502, 0.6098, 0.6163, 0.6439, 0.6441, 0.6198, 0.6096, 0.6438, 0.6417,\n",
       "         0.6207, 0.6221, 0.6314, 0.6346, 0.6424, 0.6505, 0.6197, 0.6626, 0.6280,\n",
       "         0.6274, 0.6147, 0.6576, 0.6521, 0.6229, 0.6307, 0.6542, 0.5951, 0.6398,\n",
       "         0.6278, 0.6261, 0.6218, 0.6329, 0.6784, 0.6857, 0.6377, 0.6338, 0.6250,\n",
       "         0.6533, 0.6379, 0.6497, 0.4800, 0.6291, 0.6451, 0.6843, 0.6271, 0.6006,\n",
       "         0.6481, 0.6145, 0.6409, 0.6265, 0.6621, 0.6242, 0.6424, 0.6176, 0.6414,\n",
       "         0.6487, 0.6233, 0.6266, 0.6764, 0.6153, 0.6282, 0.6213, 0.6218, 0.6077,\n",
       "         0.6760, 0.6448, 0.6194, 0.6133, 0.6627, 0.6620, 0.6123, 0.6447, 0.6276,\n",
       "         0.6074, 0.6355, 0.2733, 0.6234, 0.6448, 0.6281, 0.6408, 0.6209, 0.6348,\n",
       "         0.5948, 0.6265, 0.6288, 0.6523, 0.6324, 0.6185, 0.6138, 0.6167, 0.6319,\n",
       "         0.6588, 0.6490, 0.6170, 0.6269, 0.6300, 0.6341, 0.6205, 0.6115, 0.6197,\n",
       "         0.6779, 0.6323, 0.6168, 0.6396, 0.6188, 0.6363, 0.6000, 0.6344, 0.6188,\n",
       "         0.6309, 0.6176, 0.6522, 0.6386, 0.6353, 0.6329, 0.6167, 0.6441, 0.6369,\n",
       "         0.6345, 0.6275, 0.6363, 0.6353, 0.6486, 0.6287, 0.6268, 0.6261, 0.6385,\n",
       "         0.5896, 0.6288, 0.6216, 0.6141, 0.6393, 0.6453, 0.6181, 0.6181, 0.6344,\n",
       "         0.6263, 0.6077, 0.6241, 0.6399, 0.6224, 0.6326, 0.6354, 0.6133, 0.6352,\n",
       "         0.6321, 0.6392, 0.6411, 0.5365, 0.6179, 0.6134, 0.6166, 0.6293, 0.6133,\n",
       "         0.6500, 0.6387, 0.6225, 0.6784, 0.6560, 0.6243, 0.6406, 0.6288, 0.6380,\n",
       "         0.6308, 0.6276, 0.6754, 0.6355, 0.6390, 0.6337, 0.6624, 0.6925, 0.6277,\n",
       "         0.6035, 0.6296, 0.6262, 0.7089, 0.6599, 0.6311, 0.6430, 0.6382, 0.6258,\n",
       "         0.6512, 0.6338, 0.6338, 0.6218, 0.6266, 0.6353, 0.6361, 0.7031, 0.6023,\n",
       "         0.6456, 0.6385, 0.6299, 0.6390, 0.6515, 0.6246, 0.6396, 0.6430, 0.6355,\n",
       "         0.7782, 0.6255, 0.6211, 0.6263, 0.6885, 0.6084, 0.6313, 0.6296, 0.6192,\n",
       "         0.5809, 0.6468, 0.6258, 0.6383, 0.6193, 0.6281, 0.6406, 0.6237, 0.6258,\n",
       "         0.6268, 0.5954, 0.6637, 0.6102, 0.6271, 0.6270, 0.6300, 0.6493, 0.6270,\n",
       "         0.6391, 0.6346, 0.6552, 0.6556, 0.6211, 0.6292, 0.6225, 0.6489, 0.6339,\n",
       "         0.6224, 0.6588, 0.6057, 0.6201, 0.6219, 0.7086, 0.6325, 0.6385, 0.6228,\n",
       "         0.6193, 0.6111, 0.6333, 0.6416, 0.6281, 0.6128, 0.6520, 0.6238, 0.6236,\n",
       "         0.6303, 0.6492, 0.6139, 0.6293, 0.6339, 0.6194, 0.6279, 0.6479, 0.6316,\n",
       "         0.6276, 0.6436, 0.6213, 0.6427, 0.6065, 0.6271, 0.6237, 0.6408, 0.6397,\n",
       "         0.6151, 0.7299, 0.6305, 0.6353, 0.6369, 0.6307, 0.6365, 0.6394, 0.6501,\n",
       "         0.6402, 0.6174, 0.6086, 0.6521, 0.6170, 0.6230, 0.6358, 0.6497, 0.6148,\n",
       "         0.6285, 0.6630, 0.6186, 0.6260, 0.6180, 0.6201, 0.6336, 0.6081, 0.6540,\n",
       "         0.6130, 0.6145, 0.6939, 0.6317, 0.6149, 0.7039, 0.6160, 0.6272, 0.6087,\n",
       "         0.6282, 0.6575, 0.6640, 0.6269, 0.6428, 0.6156, 0.6506, 0.6301, 0.6189,\n",
       "         0.6474, 0.6188, 0.6614, 0.6475, 0.6764, 0.6282, 0.6363, 0.6255, 0.6093,\n",
       "         0.6269, 0.6226, 0.6362, 0.6387, 0.6603, 0.6926, 0.6305, 0.6326, 0.5934,\n",
       "         0.6091, 0.5922, 0.6331, 0.6759, 0.6226, 0.6200, 0.6283, 0.6323, 0.6834,\n",
       "         0.6318, 0.6485, 0.6731, 0.6307, 0.6407, 0.6428, 0.6478, 0.6101, 0.6382,\n",
       "         0.6248, 0.6563, 0.6270, 0.5176, 0.6143, 0.6567, 0.6277, 0.6234, 0.6273,\n",
       "         0.6230, 0.6163, 0.6253, 0.6246, 0.6219, 0.6437, 0.6163, 0.6394, 0.6223,\n",
       "         0.6381, 0.6711, 0.6193, 0.6341, 0.6202, 0.6306, 0.6855, 0.6169, 0.6608,\n",
       "         0.6413, 0.6073, 0.6099, 0.6886, 0.6545, 0.6303, 0.6289, 0.6170, 0.6146,\n",
       "         0.6222, 0.6569, 0.6391, 0.6498, 0.6231, 0.6311, 0.6149, 0.6380, 0.6652,\n",
       "         0.6179, 0.6393, 0.6344, 0.6307, 0.6375, 0.6570, 0.6364, 0.6536, 0.6366,\n",
       "         0.6939, 0.6344, 0.6406, 0.6145, 0.6104, 0.6540, 0.6684, 0.6228, 0.6354,\n",
       "         0.6247, 0.6329, 0.6069, 0.6222, 0.6401, 0.6184, 0.6320, 0.6253, 0.6561,\n",
       "         0.6311, 0.6098, 0.6296, 0.6345, 0.6655, 0.6368, 0.6417, 0.6204, 0.6167,\n",
       "         0.6496, 0.5092, 0.6322, 0.6467, 0.6442, 0.6169, 0.6217, 0.6171, 0.6479,\n",
       "         0.6395, 0.6242, 0.6299, 0.6305, 0.6158, 0.6503, 0.6288, 0.6161, 0.6538,\n",
       "         0.6713, 0.6220, 0.6196, 0.6965, 0.6285, 0.6369, 0.6248, 0.6180, 0.6288,\n",
       "         0.6421, 0.6326, 0.6461, 0.6799, 0.6320, 0.6065, 0.6553, 0.6395, 0.6489,\n",
       "         0.6506, 0.6355, 0.6255, 0.6291, 0.6445, 0.6394, 0.6475, 0.6315, 0.6116,\n",
       "         0.6204, 0.6243, 0.6288, 0.6222, 0.6340, 0.6689, 0.6636, 0.6397, 0.6240,\n",
       "         0.6251, 0.6379, 0.6349, 0.6577, 0.6313, 0.6256, 0.6191, 0.6826, 0.6339,\n",
       "         0.6188, 0.6248, 0.6788, 0.6242, 0.6443, 0.6252, 0.6199, 0.6232, 0.6255,\n",
       "         0.6361, 0.6255, 0.6459, 0.6494, 0.6014, 0.7048, 0.6307, 0.6436, 0.6340,\n",
       "         0.6423, 0.6538, 0.6481, 0.6516, 0.6327, 0.5313, 0.6531, 0.6167, 0.6196,\n",
       "         0.6541, 0.6369, 0.6306, 0.6473, 0.6102, 0.6082, 0.6307, 0.6290, 0.6272,\n",
       "         0.6281, 0.6218, 0.6398], device='cuda:0'),\n",
       " Parameter containing:\n",
       " tensor([-6.3461e-03, -1.9894e-02,  4.6285e-02,  1.5851e-02, -4.2569e-02,\n",
       "         -1.3283e-02, -9.1369e-03, -3.3534e-02,  2.5846e-02, -3.6451e-02,\n",
       "         -6.1155e-02,  3.0418e-02,  1.7200e-02,  1.2180e-02, -3.3043e-03,\n",
       "         -4.8502e-02,  5.4115e-04, -9.2394e-02,  1.0380e-02,  1.1926e-02,\n",
       "         -5.8754e-02, -6.8212e-03,  4.3755e-03, -4.5268e-02,  5.5810e-03,\n",
       "         -4.1087e-02, -4.8185e-02, -5.1641e-02, -9.1535e-02, -4.0457e-02,\n",
       "         -2.7150e-02, -4.6665e-02, -1.1590e-02, -7.6277e-02, -4.2056e-02,\n",
       "         -5.7515e-02, -2.7733e-02, -7.7364e-02,  6.9931e-03, -5.9466e-02,\n",
       "          5.3589e-02, -2.0961e-02, -4.5835e-02, -6.9220e-02, -1.0842e-01,\n",
       "         -1.9135e-02, -1.3539e-01, -1.7333e-03, -1.0963e-01, -1.1798e-01,\n",
       "         -2.1245e-02,  4.4968e-02,  3.4347e-02, -2.5082e-02, -8.8453e-02,\n",
       "         -8.9132e-03, -2.2133e-03, -6.2073e-02, -5.2548e-04, -6.9640e-02,\n",
       "          4.3890e-02, -8.3953e-02,  2.0464e-02,  5.0537e-02, -4.3895e-02,\n",
       "         -4.7470e-02, -9.3629e-04, -5.9144e-02,  3.3087e-02,  9.0344e-04,\n",
       "          2.1959e-02, -2.3260e-02, -6.6600e-02, -5.4183e-02, -1.2760e-02,\n",
       "         -8.5299e-02,  4.8837e-02, -4.9116e-02,  4.0889e-02,  3.1605e-02,\n",
       "          3.4653e-02, -3.4898e-02, -5.1033e-02,  1.9560e-02, -5.7742e-02,\n",
       "         -3.7249e-02,  2.8692e-02, -1.3316e-02, -4.7234e-02,  2.9975e-02,\n",
       "          1.4817e-02, -7.7454e-02,  5.3532e-02, -1.5101e-02, -4.1975e-02,\n",
       "         -1.1653e-01, -4.0040e-02, -1.4511e-02,  2.9632e-02, -9.7741e-03,\n",
       "         -8.1774e-02, -7.5635e-02, -9.0471e-02, -2.8863e-02, -1.0929e-01,\n",
       "         -1.9059e-02, -2.8311e-02,  4.8349e-02, -5.1201e-02, -2.3034e-02,\n",
       "         -3.5799e-02, -3.8083e-02,  1.2375e-02,  2.0018e-02, -2.0495e-02,\n",
       "          3.5751e-02, -2.9506e-03,  3.5649e-02, -2.2652e-03, -7.3649e-03,\n",
       "          3.4992e-02,  1.1655e-01, -3.2204e-03, -3.2992e-02,  1.1133e-02,\n",
       "          5.0059e-02,  1.0109e-02, -3.6496e-03, -3.4484e-02, -1.2807e-01,\n",
       "         -6.3418e-03, -1.3855e-03, -6.3614e-03, -3.4011e-02,  6.4414e-02,\n",
       "         -1.1152e-01, -4.5309e-02, -2.4458e-03,  3.8958e-02, -3.5377e-02,\n",
       "         -1.1892e-02, -4.7690e-03,  3.9924e-02, -2.8776e-02, -1.5479e-02,\n",
       "         -8.6420e-02, -3.7144e-02,  2.9788e-02, -6.6552e-02,  3.1951e-02,\n",
       "          5.6070e-03,  5.7415e-02,  7.2993e-02, -7.0931e-02,  1.5957e-02,\n",
       "          8.3283e-02, -1.4200e-02, -7.0544e-02, -1.1789e-01, -1.1684e-02,\n",
       "          2.3768e-02,  5.1856e-02, -1.8541e-02,  5.1044e-02, -6.6014e-02,\n",
       "         -7.0924e-02,  3.6743e-03, -9.4640e-02, -5.0436e-02, -5.2546e-02,\n",
       "         -2.4143e-02, -1.7134e-02,  1.8747e-02, -5.1428e-03, -7.9001e-02,\n",
       "          5.7671e-02,  9.3698e-02, -3.3379e-02, -5.0376e-02, -7.9511e-02,\n",
       "          1.3052e-01,  1.4976e-02, -1.7266e-02, -3.8186e-02, -1.4686e-02,\n",
       "         -6.0080e-02, -6.1968e-02,  6.4466e-03,  3.3904e-02,  9.1657e-03,\n",
       "         -3.3732e-02, -1.1071e-01, -1.0519e-02, -6.0141e-04,  4.4024e-02,\n",
       "         -5.1106e-02,  1.0253e-02,  1.2218e-01, -7.0053e-03,  2.8158e-02,\n",
       "         -4.4821e-02, -2.7442e-02,  3.9209e-02,  5.1164e-02, -8.5783e-02,\n",
       "          4.3804e-02,  4.2881e-02, -6.2721e-02,  4.4046e-02, -6.1240e-02,\n",
       "         -1.3413e-02, -6.9396e-02,  4.2974e-02, -8.8764e-02, -4.5121e-03,\n",
       "         -2.9662e-02,  5.5994e-02, -6.8597e-02, -4.9121e-02,  4.4877e-03,\n",
       "          3.2838e-02, -6.2602e-02, -1.9148e-02,  5.5441e-03, -7.5632e-02,\n",
       "         -1.1891e-01,  3.6709e-02, -1.0153e-01, -3.3007e-02,  1.2996e-03,\n",
       "         -5.4093e-02, -4.5864e-02, -1.0291e-01,  1.7535e-03,  6.1727e-02,\n",
       "         -2.1266e-02, -8.1250e-03, -3.6417e-02,  4.2391e-02,  3.1332e-02,\n",
       "         -4.2671e-02, -1.9608e-03, -4.9377e-02,  1.9157e-02, -3.1282e-03,\n",
       "         -3.2789e-02,  4.4702e-02, -7.5857e-02, -7.6272e-02, -4.3770e-02,\n",
       "         -5.8095e-02, -9.6655e-02, -2.2400e-02, -2.8436e-02,  7.7737e-02,\n",
       "         -8.0331e-02, -4.7472e-02,  7.3887e-03,  2.6763e-02, -5.7786e-02,\n",
       "         -6.0987e-02, -3.2469e-02, -3.1629e-02, -1.2727e-01, -5.1392e-02,\n",
       "         -6.6688e-02, -1.9219e-01, -5.1794e-02,  2.4553e-03,  2.9904e-02,\n",
       "          8.0934e-02, -3.0436e-02,  1.5577e-02, -7.2660e-02, -3.3336e-02,\n",
       "         -5.3065e-02,  1.1671e-01,  1.0026e-04,  1.9790e-03, -6.5624e-02,\n",
       "         -4.3014e-02, -1.0482e-02,  4.8351e-02, -8.9676e-02,  6.5467e-02,\n",
       "         -2.4850e-02,  1.4410e-02, -9.7899e-02, -1.1841e-01,  1.1648e-02,\n",
       "          1.5642e-02,  4.3354e-02, -2.9421e-02, -5.1757e-02, -1.1092e-02,\n",
       "         -3.3445e-02, -2.2702e-02,  8.9328e-02, -6.5749e-02,  1.3442e-02,\n",
       "          3.7518e-02,  3.1988e-02, -9.5302e-02, -4.4028e-02, -1.5638e-02,\n",
       "         -3.1500e-02, -1.3504e-02,  2.7924e-03,  2.2506e-01, -5.3851e-02,\n",
       "         -2.0857e-02,  4.1186e-03, -2.5374e-02, -2.3853e-02,  6.7087e-02,\n",
       "         -1.5815e-02, -3.7528e-03,  3.4893e-02, -2.6880e-02,  4.4248e-02,\n",
       "         -3.1470e-02, -3.4574e-02, -9.8893e-03,  6.6473e-02, -7.7952e-02,\n",
       "         -3.0651e-02, -4.4723e-02,  1.6922e-02, -8.1135e-02, -3.1212e-02,\n",
       "         -2.7336e-02, -4.2488e-02, -9.1422e-02,  6.7794e-02, -3.8465e-02,\n",
       "         -8.5077e-04, -4.9080e-02, -1.6648e-02, -5.0307e-02,  7.3965e-02,\n",
       "         -2.5469e-02, -1.5340e-02, -1.3841e-02, -5.3395e-02,  3.1235e-02,\n",
       "         -4.4601e-02, -3.0874e-02,  1.3887e-02, -1.5264e-02, -4.4242e-02,\n",
       "         -8.1788e-02,  3.6415e-02, -1.0788e-01, -6.9351e-02, -2.4133e-02,\n",
       "         -1.9062e-02,  3.4992e-02, -6.9762e-02, -5.6817e-02, -1.0014e-02,\n",
       "         -5.2109e-02,  6.1807e-02, -9.6213e-02, -3.5930e-02, -9.8491e-02,\n",
       "         -8.2680e-03,  4.1227e-03, -9.1548e-02, -3.5262e-02, -7.2005e-03,\n",
       "         -3.6008e-02, -3.1894e-02, -9.3911e-02,  1.2452e-02,  3.7334e-02,\n",
       "         -3.6833e-02,  1.2549e-02,  6.3336e-02, -2.8293e-02, -6.2196e-02,\n",
       "         -5.1179e-02, -2.1169e-02, -2.2330e-02,  1.7721e-02, -5.7899e-02,\n",
       "         -1.0315e-01, -8.9405e-03, -5.2078e-03, -5.5555e-02,  1.6670e-02,\n",
       "          1.2780e-01,  5.6130e-02,  6.1353e-03, -5.0181e-02,  5.3761e-02,\n",
       "         -1.5701e-03, -2.8827e-02,  5.8172e-02,  2.3896e-02, -7.4967e-02,\n",
       "         -5.9917e-02, -1.2450e-02,  2.4290e-02,  1.0785e-01, -1.6169e-02,\n",
       "         -6.5232e-02,  2.7714e-02, -6.4520e-02,  8.3731e-02, -4.5130e-02,\n",
       "         -1.9344e-03, -4.9180e-02, -2.6595e-02, -2.3764e-02, -5.1821e-02,\n",
       "         -5.2828e-02,  5.9051e-03, -3.1387e-02,  1.5917e-02, -6.3710e-03,\n",
       "         -9.6739e-03, -1.7171e-01, -1.5133e-02, -3.4919e-02, -6.6834e-02,\n",
       "         -2.9504e-02,  7.5407e-03, -5.1922e-03, -7.4846e-02, -1.5151e-02,\n",
       "         -7.9631e-02, -2.6341e-02, -1.5012e-01,  2.2460e-02,  1.0107e-02,\n",
       "         -2.5778e-02, -2.6850e-05, -2.0127e-02, -3.9769e-02, -1.5528e-02,\n",
       "         -2.5348e-02, -6.8829e-02,  3.4717e-02, -2.0053e-03, -5.0096e-02,\n",
       "          5.8816e-02,  4.7402e-02, -1.0247e-01, -2.6462e-02, -4.4910e-02,\n",
       "         -2.1627e-02, -4.0818e-02, -1.4128e-02, -9.3792e-04, -6.7596e-02,\n",
       "         -6.2011e-02, -4.6467e-02, -6.8064e-03,  7.3054e-02, -4.7341e-02,\n",
       "         -1.4687e-02, -3.7106e-02, -1.4129e-01, -9.2913e-02, -1.1632e-02,\n",
       "         -5.4049e-02, -5.8600e-02,  5.4550e-02, -3.9708e-02, -9.0867e-02,\n",
       "         -5.5624e-02, -2.0226e-02, -1.3310e-02,  4.2342e-02, -2.7498e-02,\n",
       "         -1.5261e-02, -5.2250e-02, -7.4362e-02,  5.7417e-02, -6.4251e-02,\n",
       "         -5.5464e-02,  5.9414e-03,  1.3054e-02,  3.2055e-02,  9.2786e-03,\n",
       "          9.6044e-04,  9.5211e-02,  3.8422e-02, -1.6226e-03, -2.5338e-02,\n",
       "         -6.2656e-03, -8.6946e-03, -4.5840e-02, -5.9405e-02, -2.9124e-02,\n",
       "          1.0008e-02, -4.2456e-02,  6.5832e-02,  4.8613e-02, -4.0501e-02,\n",
       "         -4.6531e-02, -1.5976e-02, -4.6823e-02, -1.7073e-02, -1.7184e-04,\n",
       "          9.7579e-02,  1.2385e-02, -1.8580e-02, -2.3771e-02,  5.3708e-03,\n",
       "         -6.2866e-02, -6.6282e-02,  3.2546e-02, -5.4184e-02,  9.1790e-02,\n",
       "         -1.0412e-01, -9.0681e-02,  5.6900e-03, -3.8840e-02, -7.4245e-02,\n",
       "         -4.6258e-02, -1.0141e-02,  2.7948e-03, -9.8402e-02, -5.7454e-02,\n",
       "          8.9371e-03, -4.7653e-02, -3.5983e-05,  3.6370e-02, -6.8060e-03,\n",
       "         -1.2731e-01, -2.8675e-02,  2.9252e-02,  7.4892e-02, -1.2447e-02,\n",
       "         -2.8557e-02,  1.0087e-01,  1.0119e-02,  3.5378e-02,  4.1354e-02,\n",
       "         -3.4999e-02,  6.8073e-02, -8.3200e-02,  1.2770e-03, -5.2315e-02,\n",
       "         -7.4785e-03, -6.4946e-02,  1.3443e-02, -3.5037e-02,  2.9521e-03,\n",
       "         -3.4377e-02, -6.3584e-02, -5.1472e-02,  2.8725e-02, -7.8538e-03,\n",
       "         -1.0991e-03,  1.7426e-02, -6.5711e-02, -1.6871e-02,  2.3383e-02,\n",
       "          2.2133e-02, -4.0960e-02,  1.2284e-02, -8.1919e-02, -2.2731e-02,\n",
       "         -4.7884e-02, -4.9796e-02, -4.8432e-02, -3.7191e-02,  8.0479e-03,\n",
       "         -4.7701e-02,  1.8650e-02, -1.0814e-01,  7.4144e-02, -3.5186e-02,\n",
       "         -1.3576e-01, -6.6604e-02,  3.8105e-02,  1.3714e-02,  7.7252e-03,\n",
       "          5.0489e-02,  4.1594e-02,  5.2363e-02, -5.8934e-02, -2.5082e-02,\n",
       "          1.9394e-02, -6.3943e-03, -3.7295e-03,  3.8858e-02,  1.0877e-02,\n",
       "         -7.3624e-02, -4.5368e-02, -4.7722e-02, -1.0537e-02,  1.5779e-02,\n",
       "         -9.4885e-02,  1.1391e-02,  4.5534e-02, -2.0946e-02,  2.2194e-02,\n",
       "         -2.3659e-02, -2.0593e-02, -9.2396e-02, -5.1322e-02,  3.7030e-02,\n",
       "         -2.6163e-02, -5.7035e-03, -1.9519e-02, -7.4590e-03,  5.5139e-02,\n",
       "         -5.2928e-02, -9.3105e-02, -9.0368e-02, -5.1391e-02,  4.1980e-03,\n",
       "         -1.1540e-01, -2.4661e-02, -1.2784e-02,  3.7718e-02,  9.8530e-03,\n",
       "         -5.9503e-02, -1.6615e-02, -1.0033e-01, -1.2805e-01,  3.3951e-02,\n",
       "         -6.6903e-02, -1.0696e-01, -2.2511e-02,  1.0136e-02, -8.9295e-02,\n",
       "         -5.8106e-02, -1.8917e-02,  3.0799e-02, -8.4209e-02, -4.7490e-02,\n",
       "          3.4659e-02, -2.3828e-02,  1.0109e-02, -2.2014e-02,  1.3713e-01,\n",
       "         -8.5138e-02,  4.2718e-03,  3.1069e-03,  2.6038e-02, -1.7890e-02,\n",
       "         -3.1987e-02, -7.9456e-02, -2.1475e-02, -9.4370e-02,  8.7895e-03,\n",
       "         -2.8219e-02, -4.0465e-02, -1.3446e-02, -5.4235e-02,  3.6914e-02,\n",
       "         -5.9405e-02, -4.1999e-02,  7.6936e-03, -2.3912e-02,  2.7201e-02,\n",
       "         -8.1045e-02, -6.2674e-02,  6.0316e-03, -8.3956e-03, -9.7366e-02,\n",
       "         -2.8905e-02, -6.4466e-02, -4.1084e-02, -1.5809e-02, -3.3628e-02,\n",
       "         -7.5521e-04,  1.1060e-02, -3.0861e-02, -2.7453e-02, -2.2560e-02,\n",
       "         -4.0385e-02,  1.1128e-02,  3.2616e-02,  1.9923e-02, -5.2315e-02,\n",
       "         -6.3481e-02,  2.8915e-02, -4.1004e-02,  2.9585e-02,  7.3443e-02,\n",
       "         -3.7008e-02, -1.3618e-02,  7.0967e-02,  6.3854e-02, -3.6539e-02,\n",
       "          5.6841e-03, -2.3552e-02, -2.3075e-02, -1.0181e-01, -3.1658e-02,\n",
       "          7.5154e-02, -6.8950e-02,  1.7453e-02, -4.2651e-02, -8.9301e-02,\n",
       "         -1.1395e-01, -5.5166e-02, -6.5374e-02, -1.0342e-01, -1.0614e-01,\n",
       "         -2.7443e-03, -2.7233e-02, -7.5034e-03,  1.6079e-02, -3.1732e-02,\n",
       "          2.1911e-02, -3.8058e-02,  1.2087e-02,  1.9566e-02, -2.1590e-02,\n",
       "         -1.8401e-02, -5.6781e-02,  6.9947e-02,  3.7185e-02, -6.0096e-03,\n",
       "         -5.5889e-02, -7.1291e-04, -2.1571e-02, -1.1587e-01, -1.2091e-02,\n",
       "          1.6871e-02,  2.5919e-02, -1.1262e-01, -9.1336e-02, -4.0920e-02,\n",
       "         -8.1354e-03,  6.3578e-02, -1.0269e-02,  6.2887e-02, -7.6906e-02,\n",
       "         -2.2190e-02, -3.0619e-04, -1.7264e-02,  1.4001e-02, -6.2175e-02,\n",
       "         -3.0614e-02, -7.7598e-03,  7.8681e-02, -1.1139e-01, -9.4248e-02,\n",
       "         -5.5925e-02,  1.9979e-02, -4.3891e-02, -5.6174e-02, -7.6827e-02,\n",
       "          5.8949e-02,  2.0870e-02, -6.9111e-02, -7.9080e-02, -2.0177e-03,\n",
       "         -2.2291e-02, -1.0346e-01,  2.1730e-02,  7.4742e-02, -3.9678e-02,\n",
       "         -7.2509e-02, -1.7938e-02,  8.0589e-03, -4.5611e-02, -8.8949e-03,\n",
       "          1.6488e-02,  3.7573e-02, -1.7079e-02], device='cuda:0')]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(singletaskbert.bert.encoder.layer[-1].parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3092b4e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "singletaskbert.load_state_dict(torch.load('../../result/baselines/ShortHumor_freezed/checkpoint-5910/pytorch_model.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d912bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model=singletaskbert, args=training_args, compute_metrics=compute_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfdcf8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 37801\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1182' max='1182' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1182/1182 03:32]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.5187092423439026,\n",
       " 'eval_accuracy': 0.7524139573027169,\n",
       " 'eval_f1': 0.7676456714416942,\n",
       " 'eval_precision': 0.7234779353268754,\n",
       " 'eval_recall': 0.8175568482284505,\n",
       " 'eval_runtime': 212.9343,\n",
       " 'eval_samples_per_second': 177.524,\n",
       " 'eval_steps_per_second': 5.551}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66c469e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
