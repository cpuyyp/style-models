{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6870b53",
   "metadata": {},
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#unfreeze,-with-pooler,-limit-30000\" data-toc-modified-id=\"unfreeze,-with-pooler,-limit-30000-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>unfreeze, with pooler, limit 30000</a></span></li><li><span><a href=\"#unfreeze,-without-pooler,-limit-30000\" data-toc-modified-id=\"unfreeze,-without-pooler,-limit-30000-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>unfreeze, without pooler, limit 30000</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470ae8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./multitask_style_learning_utils.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd416e26",
   "metadata": {},
   "source": [
    "# unfreeze, with pooler, limit 30000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b3d094",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2166d0ec8174a9bb7ffef3f29314e78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9380 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1876 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d025ed7d8d5540348954d9e10116bcd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "selected_tasks = [\n",
    "#                   'PASTEL_country', # 33224\n",
    "                  'SARC', # 205645\n",
    "#                   'SarcasmGhosh', # 39780\n",
    "                  'ShortHumor', # 37801\n",
    "#                   'ShortJokeKaggle', # 406682\n",
    "#                   'ShortRomance', # 1902\n",
    "#                   'TroFi', # 3335\n",
    "#                   'VUA', # 15157\n",
    "                 ] \n",
    "training_args = TrainingArgs(selected_tasks=selected_tasks,\n",
    "                             base_model_name='bert-base-uncased',\n",
    "                             freeze_bert=False,\n",
    "                             use_pooler=True,\n",
    "                             num_epoch=5,\n",
    "                             data_limit=30000,\n",
    "                            )\n",
    "\n",
    "model = init_model(training_args)\n",
    "freeze_model(model, training_args.freeze_bert)\n",
    "df_evaluation, df_loss_per_step, model = train_model(model, training_args)\n",
    "\n",
    "eval_dataloader = MultiTaskTestDataLoader(training_args, split='dev')\n",
    "attn_entropy, head_importance, preds, labels = compute_heads_importance(model, eval_dataloader, training_args)\n",
    "\n",
    "imshow(attn_entropy)\n",
    "imshow(head_importance)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11064500",
   "metadata": {},
   "source": [
    "# unfreeze, without pooler, limit 30000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e04bdf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_tasks = [\n",
    "#                   'PASTEL_country', # 33224\n",
    "                  'SARC', # 205645\n",
    "#                   'SarcasmGhosh', # 39780\n",
    "                  'ShortHumor', # 37801\n",
    "#                   'ShortJokeKaggle', # 406682\n",
    "#                   'ShortRomance', # 1902\n",
    "#                   'TroFi', # 3335\n",
    "#                   'VUA', # 15157\n",
    "                 ] \n",
    "training_args = TrainingArgs(selected_tasks=selected_tasks,\n",
    "                             base_model_name='bert-base-uncased',\n",
    "                             freeze_bert=False,\n",
    "                             use_pooler=False,\n",
    "                             num_epoch=5,\n",
    "                             data_limit=30000,\n",
    "                            )\n",
    "\n",
    "model = init_model(training_args)\n",
    "freeze_model(model, training_args.freeze_bert)\n",
    "df_evaluation, df_loss_per_step, model = train_model(model, training_args)\n",
    "\n",
    "eval_dataloader = MultiTaskTestDataLoader(training_args, split='dev')\n",
    "attn_entropy, head_importance, preds, labels = compute_heads_importance(model, eval_dataloader, training_args)\n",
    "\n",
    "imshow(attn_entropy)\n",
    "imshow(head_importance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5893f76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
